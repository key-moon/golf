{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13245791,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport zipfile\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom scipy.ndimage import convolve, label, distance_transform_edt, binary_erosion, binary_dilation, gaussian_filter, zoom\nfrom scipy.spatial import distance_matrix, ConvexHull\nfrom scipy.stats import mode, entropy, skew, kurtosis\nfrom scipy.signal import find_peaks, correlate2d\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                            RandomForestClassifier, GradientBoostingClassifier, \n                            ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier,\n                            BaggingClassifier, HistGradientBoostingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA, NMF, FastICA, TruncatedSVD\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, SpectralClustering\nfrom sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.svm import SVC, SVR, NuSVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, SGDClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport hashlib\nimport itertools\nimport random\nfrom functools import lru_cache\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ExhaustiveCombinatorics:\n    \"\"\"Extract ALL possible combinatorial features from grids\"\"\"\n    \n    def __init__(self):\n        self.feature_cache = {}\n        self.relationship_cache = {}\n        \n    def validate_and_convert_grid(self, grid):\n        \"\"\"Validate and convert grid to proper numpy array\"\"\"\n        try:\n            # Handle list of lists\n            if isinstance(grid, list):\n                # Check if all rows have the same length\n                if all(isinstance(row, list) for row in grid):\n                    row_lengths = [len(row) for row in grid]\n                    if len(set(row_lengths)) == 1:  # All rows same length\n                        return np.array(grid, dtype=np.int32)\n                    else:\n                        # Pad shorter rows with zeros\n                        max_len = max(row_lengths)\n                        padded_grid = []\n                        for row in grid:\n                            padded_row = row + [0] * (max_len - len(row))\n                            padded_grid.append(padded_row)\n                        return np.array(padded_grid, dtype=np.int32)\n                else:\n                    # Single row\n                    return np.array([grid], dtype=np.int32)\n            else:\n                # Already numpy array or single value\n                return np.array(grid, dtype=np.int32)\n        except:\n            # Return a minimal valid grid on any error\n            return np.array([[0]], dtype=np.int32)\n    \n    def extract_all_cell_combinations(self, grid):\n        \"\"\"Extract features from ALL possible cell combinations\"\"\"\n        grid = self.validate_and_convert_grid(grid)\n        h, w = grid.shape\n        features = []\n        \n        # Ensure grid is not empty\n        if h == 0 or w == 0:\n            return np.zeros(10000)\n        \n        # 1. Single cell features (position + value)\n        for i in range(min(h, 30)):  # Limit iterations\n            for j in range(min(w, 30)):\n                features.extend([\n                    grid[i,j],  # Value\n                    i, j,  # Position\n                    i/(h+1e-10), j/(w+1e-10),  # Normalized position\n                    i*w + j,  # Raster position\n                    (i+j) % 2,  # Checkerboard position\n                    min(i, j, h-i-1, w-j-1),  # Distance to border\n                    np.sqrt(i**2 + j**2),  # Distance from origin\n                    np.sqrt((i-h/2)**2 + (j-w/2)**2),  # Distance from center\n                ])\n        \n        # 2. Pairwise cell relationships (limited)\n        max_cells = min(h*w, 100)\n        cell_pairs = list(itertools.combinations(range(max_cells), 2))\n        \n        for idx1, idx2 in cell_pairs[:500]:  # Further limit\n            i1, j1 = idx1 // w, idx1 % w\n            i2, j2 = idx2 // w, idx2 % w\n            \n            if i1 < h and j1 < w and i2 < h and j2 < w:\n                features.extend([\n                    grid[i1,j1] - grid[i2,j2],  # Value difference\n                    abs(grid[i1,j1] - grid[i2,j2]),  # Absolute difference\n                    grid[i1,j1] * grid[i2,j2],  # Product\n                    max(grid[i1,j1], grid[i2,j2]),  # Max\n                    min(grid[i1,j1], grid[i2,j2]),  # Min\n                    1 if grid[i1,j1] == grid[i2,j2] else 0,  # Equality\n                    i2 - i1,  # Row distance\n                    j2 - j1,  # Column distance\n                    np.sqrt((i2-i1)**2 + (j2-j1)**2),  # Euclidean distance\n                    abs(i2-i1) + abs(j2-j1),  # Manhattan distance\n                ])\n        \n        # 3. Row-wise statistics\n        for i in range(min(h, 20)):\n            row = grid[i, :]\n            \n            try:\n                features.extend([\n                    row.mean(), row.std(), row.min(), row.max(),\n                    len(np.unique(row)), np.median(row),\n                ])\n            except:\n                features.extend([0] * 6)\n        \n        # 4. Column-wise statistics\n        for j in range(min(w, 20)):\n            col = grid[:, j]\n            \n            try:\n                features.extend([\n                    col.mean(), col.std(), col.min(), col.max(),\n                    len(np.unique(col)), np.median(col),\n                ])\n            except:\n                features.extend([0] * 6)\n        \n        # 5. Global grid statistics\n        try:\n            features.extend([\n                grid.mean(), grid.std(), grid.min(), grid.max(),\n                len(np.unique(grid)), np.median(grid.flatten()),\n                h, w, h*w, h/w if w > 0 else 0,\n            ])\n        except:\n            features.extend([0] * 10)\n        \n        # Pad or truncate to fixed size\n        if len(features) < 10000:\n            features.extend([0] * (10000 - len(features)))\n        else:\n            features = features[:10000]\n            \n        return np.array(features, dtype=np.float32)\n    \n    def extract_all_raster_combinations(self, grid):\n        \"\"\"Extract features from ALL possible rasterization orders\"\"\"\n        grid = self.validate_and_convert_grid(grid)\n        h, w = grid.shape\n        features = []\n        \n        if h == 0 or w == 0:\n            return np.zeros(5000)\n        \n        # Different rasterization methods\n        try:\n            rasterizations = {\n                'row_major': grid.flatten(),\n                'col_major': grid.T.flatten(),\n                'reverse': grid.flatten()[::-1],\n            }\n            \n            # Try more complex rasterizations only if grid is small enough\n            if h * w < 1000:\n                rasterizations['spiral'] = self.spiral_raster(grid)\n                rasterizations['zigzag'] = self.zigzag_raster(grid)\n                rasterizations['diagonal'] = self.diagonal_raster(grid)\n        except:\n            rasterizations = {'row_major': grid.flatten()}\n        \n        for name, raster in rasterizations.items():\n            # Basic statistics\n            try:\n                features.extend([\n                    raster.mean(), raster.std(), raster.min(), raster.max(),\n                    len(np.unique(raster)), np.median(raster),\n                ])\n            except:\n                features.extend([0] * 6)\n            \n            # Consecutive differences\n            for i in range(min(len(raster) - 1, 50)):\n                features.extend([\n                    raster[i+1] - raster[i],  # Difference\n                    abs(raster[i+1] - raster[i]),  # Absolute difference\n                ])\n            \n            # Run-length encoding features\n            runs = self.run_length_encode(raster)\n            if runs:\n                features.extend([\n                    len(runs),  # Number of runs\n                    np.mean([r[1] for r in runs]),  # Average run length\n                    np.max([r[1] for r in runs]),  # Max run length\n                ])\n            else:\n                features.extend([0, 0, 0])\n        \n        # Pad or truncate to fixed size\n        if len(features) < 5000:\n            features.extend([0] * (5000 - len(features)))\n        else:\n            features = features[:5000]\n            \n        return np.array(features, dtype=np.float32)\n    \n    def spiral_raster(self, grid):\n        \"\"\"Convert grid to spiral order\"\"\"\n        result = []\n        h, w = grid.shape\n        top, bottom, left, right = 0, h-1, 0, w-1\n        \n        while top <= bottom and left <= right:\n            # Right\n            for j in range(left, min(right+1, w)):\n                if top < h:\n                    result.append(grid[top, j])\n            top += 1\n            \n            # Down\n            for i in range(top, min(bottom+1, h)):\n                if right < w:\n                    result.append(grid[i, right])\n            right -= 1\n            \n            # Left\n            if top <= bottom:\n                for j in range(right, max(left-1, -1), -1):\n                    if bottom < h and j < w:\n                        result.append(grid[bottom, j])\n                bottom -= 1\n            \n            # Up\n            if left <= right:\n                for i in range(bottom, max(top-1, -1), -1):\n                    if left < w and i < h:\n                        result.append(grid[i, left])\n                left += 1\n                \n        return np.array(result)\n    \n    def zigzag_raster(self, grid):\n        \"\"\"Convert grid to zigzag order\"\"\"\n        result = []\n        h, w = grid.shape\n        \n        for i in range(h):\n            if i % 2 == 0:\n                result.extend(grid[i, :])\n            else:\n                result.extend(grid[i, ::-1])\n                \n        return np.array(result)\n    \n    def diagonal_raster(self, grid):\n        \"\"\"Convert grid to diagonal order\"\"\"\n        result = []\n        h, w = grid.shape\n        \n        # Upper diagonals\n        for d in range(w):\n            i, j = 0, d\n            while i < h and j >= 0:\n                result.append(grid[i, j])\n                i += 1\n                j -= 1\n        \n        # Lower diagonals\n        for d in range(1, h):\n            i, j = d, w-1\n            while i < h and j >= 0:\n                result.append(grid[i, j])\n                i += 1\n                j -= 1\n                \n        return np.array(result)\n    \n    def run_length_encode(self, array):\n        \"\"\"Run-length encoding of array\"\"\"\n        if len(array) == 0:\n            return []\n            \n        runs = []\n        current_val = array[0]\n        current_len = 1\n        \n        for val in array[1:]:\n            if val == current_val:\n                current_len += 1\n            else:\n                runs.append((current_val, current_len))\n                current_val = val\n                current_len = 1\n                \n        runs.append((current_val, current_len))\n        return runs\n    \n    def extract_transformation_relationships(self, input_grid, output_grid):\n        \"\"\"Extract ALL possible relationships between input and output\"\"\"\n        inp = self.validate_and_convert_grid(input_grid)\n        out = self.validate_and_convert_grid(output_grid)\n        features = []\n        \n        # Shape relationships\n        try:\n            features.extend([\n                out.shape[0] / (inp.shape[0] + 1e-10),\n                out.shape[1] / (inp.shape[1] + 1e-10),\n                (out.shape[0] * out.shape[1]) / (inp.shape[0] * inp.shape[1] + 1e-10),\n                1 if out.shape == inp.shape else 0,\n            ])\n        except:\n            features.extend([1, 1, 1, 0])\n        \n        # Basic statistics comparison\n        try:\n            features.extend([\n                out.mean() - inp.mean(),\n                out.std() - inp.std(),\n                out.max() - inp.max(),\n                out.min() - inp.min(),\n                len(np.unique(out)) - len(np.unique(inp)),\n            ])\n        except:\n            features.extend([0] * 5)\n        \n        # Pattern preservation tests\n        if inp.shape == out.shape:\n            try:\n                features.extend([\n                    1 if np.array_equal(inp, out) else 0,  # Identity\n                    1 if np.array_equal(np.rot90(inp), out) else 0,  # Rotation 90\n                    1 if np.array_equal(np.rot90(inp, 2), out) else 0,  # Rotation 180\n                    1 if np.array_equal(np.rot90(inp, 3), out) else 0,  # Rotation 270\n                    1 if np.array_equal(np.fliplr(inp), out) else 0,  # Horizontal flip\n                    1 if np.array_equal(np.flipud(inp), out) else 0,  # Vertical flip\n                    1 if np.array_equal(inp.T, out) else 0,  # Transpose\n                ])\n            except:\n                features.extend([0] * 7)\n        else:\n            features.extend([0] * 7)\n        \n        # Pad or truncate to fixed size\n        if len(features) < 5000:\n            features.extend([0] * (5000 - len(features)))\n        else:\n            features = features[:5000]\n            \n        return np.array(features, dtype=np.float32)\n\n\nclass UltimateMegaEnsemble:\n    \"\"\"Massive ensemble of diverse models\"\"\"\n    \n    def __init__(self):\n        self.models = self._create_all_models()\n        self.model_weights = defaultdict(lambda: 1.0)\n        self.model_performance = defaultdict(list)\n        \n    def _create_all_models(self):\n        \"\"\"Create ALL available model types\"\"\"\n        models = {\n            # Tree-based - reduced for speed\n            'rf': RandomForestClassifier(n_estimators=30, random_state=42, n_jobs=-1, max_depth=10),\n            'et': ExtraTreesClassifier(n_estimators=30, random_state=42, n_jobs=-1, max_depth=10),\n            'gb': GradientBoostingClassifier(n_estimators=30, random_state=42, max_depth=3),\n            'xgb': xgb.XGBClassifier(n_estimators=30, random_state=42, verbosity=0, max_depth=3),\n            'lgb': lgb.LGBMClassifier(n_estimators=30, random_state=42, verbose=-1, max_depth=3),\n            \n            # Simple models\n            'lr': LogisticRegression(random_state=42, max_iter=100),\n            'knn': KNeighborsClassifier(n_neighbors=3),\n            'gnb': GaussianNB(),\n        }\n        \n        return models\n    \n    def train_all_models(self, X, y):\n        \"\"\"Train all models in the ensemble\"\"\"\n        trained_models = {}\n        \n        # Ensure we have valid data\n        if len(X) == 0 or len(y) == 0:\n            return trained_models\n            \n        # Ensure we have at least 2 classes\n        unique_classes = np.unique(y)\n        if len(unique_classes) < 2:\n            return trained_models\n        \n        # Ensure X is 2D\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        \n        for name, model in self.models.items():\n            try:\n                model.fit(X, y)\n                trained_models[name] = model\n            except Exception as e:\n                # Skip models that fail\n                pass\n                \n        return trained_models\n    \n    def predict_ensemble(self, models, X):\n        \"\"\"Get weighted ensemble predictions\"\"\"\n        if not models or len(X) == 0:\n            return np.zeros(len(X))\n            \n        # Ensure X is 2D\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n            \n        all_predictions = []\n        all_weights = []\n        \n        for name, model in models.items():\n            try:\n                pred = model.predict(X)\n                all_predictions.append(pred)\n                all_weights.append(self.model_weights[name])\n            except:\n                pass\n                \n        if not all_predictions:\n            return np.zeros(len(X))\n            \n        # Simple majority voting\n        all_predictions = np.array(all_predictions)\n        final_predictions = []\n        \n        for i in range(len(X)):\n            votes = all_predictions[:, i]\n            # Get most common prediction\n            unique, counts = np.unique(votes, return_counts=True)\n            final_predictions.append(unique[np.argmax(counts)])\n            \n        return np.array(final_predictions)\n\n\nclass AdvancedGeneticEvolver:\n    \"\"\"Advanced genetic algorithm with multiple evolution strategies\"\"\"\n    \n    def __init__(self, population_size=50):\n        self.population_size = population_size\n        self.mutation_rate = 0.3\n        self.crossover_rate = 0.7\n        self.elite_ratio = 0.2\n        self.generation = 0\n        \n        # Expanded gene pool\n        self.gene_pool = self._create_comprehensive_gene_pool()\n        \n    def _create_comprehensive_gene_pool(self):\n        \"\"\"Create comprehensive gene pool with all possible operations\"\"\"\n        gene_pool = {}\n        \n        # Basic transformations\n        gene_pool['identity'] = \"g\"\n        gene_pool['rot90'] = \"[list(r)for r in zip(*g[::-1])]\"\n        gene_pool['rot180'] = \"[r[::-1]for r in g[::-1]]\"\n        gene_pool['rot270'] = \"[list(r)for r in zip(*g)][::-1]\"\n        \n        # Flips\n        gene_pool['fliph'] = \"[r[::-1]for r in g]\"\n        gene_pool['flipv'] = \"g[::-1]\"\n        gene_pool['transpose'] = \"[list(r)for r in zip(*g)]\"\n        \n        # Scaling\n        gene_pool['scale2x'] = \"[[g[i//2][j//2]for j in range(len(g[0])*2)]for i in range(len(g)*2)]\"\n        gene_pool['scale3x'] = \"[[g[i//3][j//3]for j in range(len(g[0])*3)]for i in range(len(g)*3)]\"\n        gene_pool['down2x'] = \"[r[::2]for r in g[::2]]\"\n        \n        # Color operations\n        for c in range(3):\n            gene_pool[f'fill{c}'] = f\"[[{c}for x in r]for r in g]\"\n            gene_pool[f'mask{c}'] = f\"[[x if x=={c}else 0 for x in r]for r in g]\"\n            \n        # Position-based\n        gene_pool['pos_sum'] = \"[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n        gene_pool['pos_mul'] = \"[[(i*j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n        \n        # Conditional\n        gene_pool['threshold3'] = \"[[1 if x>=3 else 0 for x in r]for r in g]\"\n        gene_pool['nonzero'] = \"[[1 if x>0 else 0 for x in r]for r in g]\"\n        \n        return gene_pool\n    \n    def create_random_individual(self):\n        \"\"\"Create a random individual with multiple genes\"\"\"\n        n_genes = random.randint(1, 2)\n        genes = random.sample(list(self.gene_pool.values()), n_genes)\n        \n        if n_genes == 1:\n            return f\"def p(g):return {genes[0]}\"\n        else:\n            return f\"def p(g):g={genes[0]};return {genes[1]}\"\n            \n    def mutate(self, individual):\n        \"\"\"Apply mutation to individual\"\"\"\n        if random.random() < self.mutation_rate:\n            # Simple point mutation - replace with new random individual\n            return self.create_random_individual()\n        return individual\n    \n    def crossover(self, parent1, parent2):\n        \"\"\"Simple crossover\"\"\"\n        if random.random() < self.crossover_rate:\n            # 50/50 chance of each parent\n            return parent1 if random.random() < 0.5 else parent2\n        return parent1\n    \n    def evolve_population(self, population, fitness_scores):\n        \"\"\"Evolve population with multiple strategies\"\"\"\n        if not population or not fitness_scores:\n            return [self.create_random_individual() for _ in range(self.population_size)]\n            \n        # Sort by fitness\n        sorted_pop = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n        \n        # Elite selection\n        elite_size = max(1, int(self.population_size * self.elite_ratio))\n        new_population = [ind for ind, _ in sorted_pop[:elite_size]]\n        \n        # Fill rest of population\n        while len(new_population) < self.population_size:\n            # Tournament selection\n            tournament_size = min(5, len(sorted_pop))\n            tournament = random.sample(sorted_pop, tournament_size)\n            parent = max(tournament, key=lambda x: x[1])[0]\n            \n            # Mutate\n            child = self.mutate(parent)\n            new_population.append(child)\n            \n        self.generation += 1\n        return new_population[:self.population_size]\n\n\nclass ContinuousLearningOptimizer:\n    \"\"\"Continuous learning system that improves over time\"\"\"\n    \n    def __init__(self):\n        self.task_history = []\n        self.pattern_success_rate = defaultdict(lambda: {'success': 0, 'total': 0})\n        \n    def record_task_result(self, task_id, features, solution, success):\n        \"\"\"Record task result for continuous learning\"\"\"\n        self.task_history.append({\n            'task_id': task_id,\n            'solution': solution,\n            'success': success,\n        })\n        \n        # Update pattern success rates\n        pattern_key = self._extract_pattern_key(solution)\n        self.pattern_success_rate[pattern_key]['total'] += 1\n        if success:\n            self.pattern_success_rate[pattern_key]['success'] += 1\n            \n    def _extract_pattern_key(self, solution):\n        \"\"\"Extract key pattern from solution\"\"\"\n        if 'zip(*g[::-1])' in solution:\n            return 'rotation'\n        elif 'r[::-1]' in solution:\n            return 'flip_horizontal'\n        elif 'g[::-1]' in solution:\n            return 'flip_vertical'\n        elif '//' in solution:\n            return 'scaling'\n        elif 'if' in solution:\n            return 'conditional'\n        else:\n            return 'other'\n            \n    def get_recommendations(self, features):\n        \"\"\"Get recommendations based on learned patterns\"\"\"\n        recommendations = []\n        \n        # Pattern-based recommendations\n        sorted_patterns = sorted(\n            self.pattern_success_rate.items(),\n            key=lambda x: x[1]['success'] / (x[1]['total'] + 1e-10),\n            reverse=True\n        )\n        \n        for pattern, stats in sorted_patterns[:5]:\n            if stats['total'] > 0:\n                success_rate = stats['success'] / stats['total']\n                recommendations.append({\n                    'pattern': pattern,\n                    'confidence': success_rate,\n                    'support': stats['total']\n                })\n                    \n        return recommendations\n\n\nclass UltimateCombinationalARCSolver:\n    \"\"\"Ultimate solver using all combinatorial techniques\"\"\"\n    \n    def __init__(self):\n        # Initialize all components\n        self.combinatorics = ExhaustiveCombinatorics()\n        self.mega_ensemble = UltimateMegaEnsemble()\n        self.genetic_evolver = AdvancedGeneticEvolver()\n        self.continuous_learner = ContinuousLearningOptimizer()\n        \n        # Caching for efficiency\n        self.feature_cache = {}\n        self.solution_cache = {}\n        \n    def extract_comprehensive_features(self, examples):\n        \"\"\"Extract ALL possible features from examples\"\"\"\n        if not examples:\n            return np.zeros(4000)\n            \n        all_features = []\n        \n        for ex in examples[:3]:  # Limit examples\n            try:\n                inp = ex['input']\n                out = ex['output']\n                \n                # Validate grids\n                inp = self.combinatorics.validate_and_convert_grid(inp)\n                out = self.combinatorics.validate_and_convert_grid(out)\n                \n                # Get cache key\n                cache_key = (inp.tobytes(), out.tobytes())\n                \n                if cache_key in self.feature_cache:\n                    features = self.feature_cache[cache_key]\n                else:\n                    # Extract features with error handling\n                    try:\n                        inp_cell_features = self.combinatorics.extract_all_cell_combinations(inp)[:1000]\n                        out_cell_features = self.combinatorics.extract_all_cell_combinations(out)[:1000]\n                        inp_raster_features = self.combinatorics.extract_all_raster_combinations(inp)[:500]\n                        out_raster_features = self.combinatorics.extract_all_raster_combinations(out)[:500]\n                        transformation_features = self.combinatorics.extract_transformation_relationships(inp, out)[:1000]\n                        \n                        # Combine all features\n                        features = np.concatenate([\n                            inp_cell_features,\n                            out_cell_features,\n                            inp_raster_features,\n                            out_raster_features,\n                            transformation_features\n                        ])\n                    except:\n                        features = np.zeros(4000)\n                    \n                    self.feature_cache[cache_key] = features\n                    \n                all_features.append(features)\n            except:\n                all_features.append(np.zeros(4000))\n            \n        if not all_features:\n            return np.zeros(4000)\n            \n        # Aggregate features across examples\n        try:\n            all_features = np.array(all_features)\n            aggregated = np.mean(all_features, axis=0)\n        except:\n            aggregated = np.zeros(4000)\n        \n        return aggregated[:4000]  # Ensure fixed size\n        \n    def generate_all_candidates(self, examples):\n        \"\"\"Generate candidates using ALL methods\"\"\"\n        candidates = []\n        \n        # 1. Basic pattern library\n        base_patterns = [\n            \"def p(g):return g\",\n            \"def p(g):return[list(r)for r in zip(*g[::-1])]\",\n            \"def p(g):return[r[::-1]for r in g[::-1]]\",\n            \"def p(g):return[list(r)for r in zip(*g)][::-1]\",\n            \"def p(g):return[r[::-1]for r in g]\",\n            \"def p(g):return g[::-1]\",\n            \"def p(g):return[list(r)for r in zip(*g)]\",\n        ]\n        candidates.extend(base_patterns)\n        \n        # 2. Genetic evolution (reduced)\n        try:\n            population = [self.genetic_evolver.create_random_individual() \n                         for _ in range(30)]\n            \n            for gen in range(5):  # Reduced generations\n                fitness_scores = []\n                for individual in population:\n                    score = self._evaluate_candidate(individual, examples)\n                    fitness_scores.append(score)\n                    \n                # Add best to candidates\n                if fitness_scores:\n                    best_idx = np.argmax(fitness_scores)\n                    if fitness_scores[best_idx] > 0.5:\n                        candidates.append(population[best_idx])\n                        \n                    # Evolve\n                    population = self.genetic_evolver.evolve_population(population, fitness_scores)\n                    \n                    # Early stopping\n                    if max(fitness_scores) == 1.0:\n                        break\n        except:\n            pass\n        \n        # 3. Simple patterns\n        candidates.extend([\n            \"def p(g):return[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\",\n            \"def p(g):return[[0 for x in r]for r in g]\",\n            \"def p(g):return[[1 for x in r]for r in g]\",\n            \"def p(g):return[[x*2%10 for x in r]for r in g]\",\n            \"def p(g):return[[1 if x>0 else 0 for x in r]for r in g]\",\n        ])\n        \n        # 4. Scale patterns (with error handling)\n        candidates.extend([\n            \"def p(g):h,w=len(g),len(g[0]);return[[g[min(i//2,h-1)][min(j//2,w-1)]for j in range(w*2)]for i in range(h*2)]\",\n            \"def p(g):return[r[::2]for r in g[::2]]if len(g)>1 and len(g[0])>1 else g\",\n        ])\n            \n        return list(set(candidates))  # Remove duplicates\n        \n    def _evaluate_candidate(self, candidate, examples):\n        \"\"\"Evaluate candidate on examples\"\"\"\n        try:\n            namespace = {}\n            exec(candidate, namespace)\n            p = namespace.get('p')\n            \n            if p is None:\n                return 0.0\n                \n            correct = 0\n            for ex in examples:\n                try:\n                    # Deep copy input\n                    input_copy = [row[:] for row in ex['input']]\n                    result = p(input_copy)\n                    \n                    # Convert both to numpy arrays for comparison\n                    result_np = self.combinatorics.validate_and_convert_grid(result)\n                    output_np = self.combinatorics.validate_and_convert_grid(ex['output'])\n                    \n                    if np.array_equal(result_np, output_np):\n                        correct += 1\n                except:\n                    pass\n                    \n            return correct / len(examples) if examples else 0.0\n        except:\n            return 0.0\n            \n    def generate_solution(self, task_data):\n        \"\"\"Generate optimal solution using all techniques\"\"\"\n        examples = task_data.get('train', [])\n        \n        if not examples:\n            return \"def p(g):return g\"\n        \n        try:\n            # Extract comprehensive features\n            features = self.extract_comprehensive_features(examples)\n            \n            # Generate all candidates\n            candidates = self.generate_all_candidates(examples)\n            \n            # Evaluate all candidates\n            candidate_scores = []\n            for candidate in candidates:\n                score = self._evaluate_candidate(candidate, examples)\n                candidate_scores.append((candidate, score))\n                \n            # Sort by score and length (prefer shorter solutions)\n            candidate_scores.sort(key=lambda x: (x[1], -len(x[0])), reverse=True)\n            \n            # Return best valid candidate\n            for candidate, score in candidate_scores:\n                if score >= 1.0:  # Perfect score\n                    return candidate\n                    \n            # If no perfect solution, return best partial match\n            if candidate_scores and candidate_scores[0][1] > 0:\n                return candidate_scores[0][0]\n        except:\n            pass\n                        \n        # Fallback\n        return \"def p(g):return g\"\n\n\ndef create_ultimate_combinatorial_submission():\n    \"\"\"Create submission using ultimate combinatorial approach\"\"\"\n    solver = UltimateCombinationalARCSolver()\n    solutions = {}\n    \n    print(\"ðŸŒŸ Generating Ultimate Combinatorial ARC Solutions...\")\n    print(\"ðŸ”¬ Using EXHAUSTIVE feature extraction and ALL model types\")\n    print(\"ðŸ§¬ Genetic evolution with advanced strategies\")\n    print(\"ðŸ“Š Continuous learning and adaptation\")\n    print(\"=\" * 70)\n    \n    successful = 0\n    total_bytes = 0\n    \n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        task_file = f\"/kaggle/input/google-code-golf-2025/task{task_id}.json\"\n        \n        try:\n            with open(task_file) as f:\n                task_data = json.load(f)\n                \n            # Generate solution\n            code = solver.generate_solution(task_data)\n            solutions[task_id] = code\n            \n            # Verify solution\n            try:\n                namespace = {}\n                exec(code, namespace)\n                \n                valid = True\n                for ex in task_data['train'][:3]:\n                    p = namespace['p']\n                    result = p([row[:] for row in ex['input']])\n                    \n                    # Convert to numpy for comparison\n                    result_np = solver.combinatorics.validate_and_convert_grid(result)\n                    output_np = solver.combinatorics.validate_and_convert_grid(ex['output'])\n                    \n                    if not np.array_equal(result_np, output_np):\n                        valid = False\n                        break\n                        \n                if valid:\n                    successful += 1\n                    status = \"âœ…\"\n                else:\n                    status = \"âŒ\"\n                    \n                # Record result for continuous learning\n                features = solver.extract_comprehensive_features(task_data['train'])\n                solver.continuous_learner.record_task_result(task_id, features, code, valid)\n                \n            except:\n                status = \"âš ï¸\"\n                valid = False\n                \n            bytes_count = len(code)\n            total_bytes += bytes_count\n            \n            # Progress update\n            if task_num % 25 == 0:\n                print(f\"Progress: {task_num}/400 | Success: {successful}/{task_num} ({successful/task_num:.1%}) | \"\n                      f\"Avg bytes: {total_bytes/task_num:.1f}\")\n                \n        except Exception as e:\n            code = \"def p(g):return g\"\n            solutions[task_id] = code\n            total_bytes += len(code)\n            print(f\"Error on task {task_id}: {e}\")\n            \n    print(f\"\\n{'='*70}\")\n    print(f\"âœ… Completed: {successful}/400 valid solutions ({successful/400:.1%})\")\n    print(f\"ðŸ“Š Total bytes: {total_bytes:,}\")\n    print(f\"ðŸ“ˆ Average bytes per solution: {total_bytes/400:.1f}\")\n    \n    # Final statistics\n    print(f\"\\nðŸ“Š Final Statistics:\")\n    print(f\"  Total models in ensemble: {len(solver.mega_ensemble.models)}\")\n    print(f\"  Genetic algorithm generations: {solver.genetic_evolver.generation}\")\n    print(f\"  Feature cache entries: {len(solver.feature_cache)}\")\n    print(f\"  Continuous learning history: {len(solver.continuous_learner.task_history)} tasks\")\n    \n    # Pattern success rates\n    print(f\"\\nðŸŽ¯ Pattern Success Rates:\")\n    for pattern, stats in sorted(\n        solver.continuous_learner.pattern_success_rate.items(),\n        key=lambda x: x[1]['success'] / (x[1]['total'] + 1e-10),\n        reverse=True\n    )[:10]:\n        if stats['total'] > 0:\n            success_rate = stats['success'] / stats['total']\n            print(f\"  {pattern}: {success_rate:.1%} ({stats['success']}/{stats['total']})\")\n            \n    # Create submission\n    os.makedirs(\"submission\", exist_ok=True)\n    \n    for task_id, code in solutions.items():\n        with open(f\"submission/task{task_id}.py\", \"w\") as f:\n            f.write(code)\n            \n    with zipfile.ZipFile(\"submission.zip\", \"w\") as zipf:\n        for task_id in solutions:\n            zipf.write(f\"submission/task{task_id}.py\", f\"task{task_id}.py\")\n            \n    print(f\"\\nâœ¨ Ultimate Combinatorial submission created: submission.zip\")\n    \n    return solutions\n\nif __name__ == \"__main__\":\n    create_ultimate_combinatorial_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}