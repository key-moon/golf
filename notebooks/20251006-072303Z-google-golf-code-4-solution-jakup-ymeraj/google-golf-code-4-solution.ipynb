{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":""},"jupytext":{"formats":"ipynb,py:percent"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13472782,"sourceType":"competition"},{"sourceId":13099530,"sourceType":"datasetVersion","datasetId":8295919},{"sourceId":13273276,"sourceType":"datasetVersion","datasetId":8252851},{"sourceId":264972870,"sourceType":"kernelVersion"},{"sourceId":265158055,"sourceType":"kernelVersion"},{"sourceId":265374026,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"ðŸ™¶ Smart Solution Search & Verification ðŸ™¸\n\n## Overview\nThis script intelligently searches multiple sources for the best (smallest + working) \nsolutions for each task (001-400) and creates an optimized submission.\n\n## Key Features\n1. **Multi-Source Support**: Handles both submission.zip archives AND direct task datasets\n2. **Smart Selection**: Only chooses alternatives that are SMALLER than default AND working\n3. **Solution Verification**: Tests each candidate solution before using it\n4. **Binary File Handling**: Direct binary copy preserves exact file sizes and handles all file types\n5. **Kaggle & Local**: Works in Kaggle and local environments \n\n## Process Flow\n1. **Discovery**: Search all sources for submission.zip files OR task datasets\n2. **Setup**: Extract/copy default source (index 0) to /kaggle/working/submission/\n3. **Extraction**: Process extra sources to /kaggle/working/source/{source_name}/\n4. **Comparison**: For each task, find all candidates and sort by size (smallest first)\n5. **Verification**: Test smaller alternatives using task examples until one works\n6. **Selection**: Use smallest working solution OR default if no smaller solution works\n7. **Output**: Create final submission.zip + others.zip + metadata CSV","metadata":{}},{"cell_type":"code","source":"# Auto-detect environment: local vs Kaggle\nimport os\nimport zipfile\nimport shutil\nimport json\nimport importlib.util\nimport sys\nimport copy\nimport traceback\nimport numpy as np\nimport re\nfrom collections import defaultdict\nimport csv\nimport glob\nfrom pathlib import Path\n\nif os.path.exists(\"./tasks\"):\n    # Local environment\n    print(\"Detected LOCAL environment - using local tasks folder\")\n    ENVIRONMENT = \"local\"\n    sources = [\n        \"./tasks\"\n    ]\nelse:\n    # Kaggle environment - treat input paths as direct dataset sources\n    print(\"Detected KAGGLE environment - using dataset sources\")\n    ENVIRONMENT = \"kaggle\"\n    sources = [\n        \"/kaggle/input/google-code-golf-2025-submit\",\n        \"/kaggle/input/arc-code-golf-starter-solutions\",\n        \"/kaggle/input/source-code-for-full-400-solutions\",\n        \"/kaggle/input/code-compression-900k-club\",\n        \"/kaggle/input/neuroips-4-of-some-lessons-learned\",\n    ]\n\nprint(f\"Environment: {ENVIRONMENT}\")\nprint(f\"Sources: {sources}\")","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_submission_zips():\n    \"\"\"Find all submission.zip files in source directories and their subdirectories\"\"\"\n    submission_zips = {}\n    \n    for source in sources:\n        if not os.path.exists(source):\n            print(f\"Warning: Source {source} does not exist\")\n            continue\n            \n        # Use glob to find all submission.zip files recursively\n        pattern = os.path.join(source, \"**/submission.zip\")\n        found_zips = glob.glob(pattern, recursive=True)\n        \n        if found_zips:\n            # Use the first submission.zip found in this source\n            submission_zips[source] = found_zips[0]\n            print(f\"Found submission.zip in {source}: {found_zips[0]}\")\n        else:\n            print(f\"No submission.zip found in {source}\")\n    \n    return submission_zips\n\nsubmission_zips = find_submission_zips()\nprint(f\"\\nFound {len(submission_zips)} submission.zip files\")","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_working_directories():\n    \"\"\"Create working directory structure\"\"\"\n    if ENVIRONMENT == \"kaggle\":\n        working_dir = \"/kaggle/working\"\n        submission_dir = \"/kaggle/working/submission\"\n        source_dir = \"/kaggle/working/source\"\n    else:\n        working_dir = \"./working\"\n        submission_dir = \"./working/submission\"\n        source_dir = \"./working/source\"\n    \n    # Clean specific subdirectories and leftover files\n    cleanup_items = [\n        submission_dir,\n        source_dir,\n        os.path.join(working_dir, \"submission_final\"),\n        os.path.join(working_dir, \"submission_final.zip\"),\n        os.path.join(working_dir, \"submission.zip\"),\n        os.path.join(working_dir, \"others.zip\"),\n        os.path.join(working_dir, \"others_metadata.csv\")\n    ]\n    \n    for item_path in cleanup_items:\n        if os.path.exists(item_path):\n            try:\n                if os.path.isdir(item_path):\n                    shutil.rmtree(item_path)\n                    print(f\"Cleaned existing directory: {item_path}\")\n                else:\n                    os.remove(item_path)\n                    print(f\"Cleaned existing file: {item_path}\")\n            except OSError as e:\n                print(f\"Warning: Could not clean {item_path}: {e}\")\n                # Try to clean contents if it's a directory\n                if os.path.isdir(item_path):\n                    try:\n                        for item in os.listdir(item_path):\n                            sub_item_path = os.path.join(item_path, item)\n                            if os.path.isdir(sub_item_path):\n                                shutil.rmtree(sub_item_path)\n                            else:\n                                os.remove(sub_item_path)\n                        print(f\"Cleaned contents of: {item_path}\")\n                    except OSError as e2:\n                        print(f\"Warning: Could not clean contents of {item_path}: {e2}\")\n    \n    # Create directories\n    os.makedirs(submission_dir, exist_ok=True)\n    os.makedirs(source_dir, exist_ok=True)\n    \n    return working_dir, submission_dir, source_dir\n\ndef read_file_as_text(file_path):\n    \"\"\"Read file content as text for verification purposes only\"\"\"\n    try:\n        # Try UTF-8 first (most common)\n        with open(file_path, 'r') as f:\n            return f.read()\n    except UnicodeDecodeError:\n        # Fallback to latin-1 which can handle any byte sequence\n        with open(file_path, 'r', encoding='latin-1') as f:\n            return f.read()\n    except Exception as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return None\n\ndef copy_task_files_from_dataset(source_path, dest_dir):\n    \"\"\"Copy task files directly from dataset folder as binary to preserve exact bytes\"\"\"\n    print(f\"Copying task files from dataset: {source_path}\")\n    \n    # Look for task files with pattern task001.py to task400.py\n    task_files_copied = 0\n    for task_num in range(1, 401):\n        task_filename = f\"task{task_num:03d}.py\"\n        source_file = os.path.join(source_path, task_filename)\n        \n        if os.path.exists(source_file):\n            dest_file = os.path.join(dest_dir, task_filename)\n            try:\n                # Simple binary copy to preserve exact bytes\n                shutil.copy2(source_file, dest_file)\n                task_files_copied += 1\n            except Exception as e:\n                print(f\"Error copying {task_filename}: {e}\")\n    \n    print(f\"Copied {task_files_copied} task files from {source_path}\")\n    return task_files_copied\n\ndef extract_default_submission(submission_zips, submission_dir):\n    \"\"\"Extract default source (first in sources list) to submission directory\"\"\"\n    if not sources:\n        raise ValueError(\"No sources defined\")\n    \n    default_source = sources[0]\n    \n    # ALWAYS prioritize direct task files over submission.zip for default source\n    if os.path.exists(default_source):\n        print(f\"Using direct task files from default source: {default_source}\")\n        \n        # Check how many task files are actually available in the source\n        available_tasks = 0\n        for task_num in range(1, 401):\n            task_file = os.path.join(default_source, f\"task{task_num:03d}.py\")\n            if os.path.exists(task_file):\n                available_tasks += 1\n        \n        print(f\"Found {available_tasks} task files in default source\")\n        \n        task_files_copied = copy_task_files_from_dataset(default_source, submission_dir)\n        \n        if task_files_copied > 0:\n            print(f\"Copied {task_files_copied} task files directly from default source\")\n            \n            # Verify what was actually copied\n            copied_tasks = 0\n            for task_num in range(1, 401):\n                dest_file = os.path.join(submission_dir, f\"task{task_num:03d}.py\")\n                if os.path.exists(dest_file):\n                    copied_tasks += 1\n            \n            print(f\"Verified {copied_tasks} task files in submission directory\")\n            \n            if copied_tasks < 400:\n                print(f\"WARNING: Only {copied_tasks}/400 tasks copied from default source!\")\n            \n            return None\n    \n    # Fallback to submission.zip if direct files not available\n    if default_source in submission_zips:\n        default_zip = submission_zips[default_source]\n        print(f\"Fallback: Extracting default submission from: {default_zip}\")\n        \n        with zipfile.ZipFile(default_zip, 'r') as zip_ref:\n            zip_ref.extractall(submission_dir)\n        \n        print(f\"Default submission extracted to: {submission_dir}\")\n        return default_zip\n    \n    # No files found at all\n    raise ValueError(f\"No task files or submission.zip found in default source {default_source}\")\n\ndef extract_extra_submissions(submission_zips, source_dir):\n    \"\"\"Extract extra sources to their own directories\"\"\"\n    extra_sources = {}\n    \n    for i, source in enumerate(sources[1:], 1):  # Skip first source (default)\n        # Get top-level folder name from the source path\n        source_name = Path(source).parts[-1] if Path(source).parts else f\"source_{i}\"\n        source_path = os.path.join(source_dir, source_name)\n        \n        os.makedirs(source_path, exist_ok=True)\n        \n        if source in submission_zips:\n            # Found submission.zip, extract it\n            print(f\"Processing extra source {source} with submission.zip\")\n            \n            # Copy submission.zip to the source directory\n            zip_dest = os.path.join(source_path, \"submission.zip\")\n            shutil.copy2(submission_zips[source], zip_dest)\n            \n            # Extract submission.zip in the source directory\n            with zipfile.ZipFile(submission_zips[source], 'r') as zip_ref:\n                zip_ref.extractall(source_path)\n            \n            extra_sources[source] = {\n                'path': source_path,\n                'name': source_name,\n                'zip_path': zip_dest\n            }\n            \n            print(f\"Extra source {source} extracted to: {source_path}\")\n            \n        elif os.path.exists(source):\n            # No submission.zip found, copy task files directly from dataset\n            print(f\"Processing extra source {source} as task dataset\")\n            \n            task_files_copied = copy_task_files_from_dataset(source, source_path)\n            \n            if task_files_copied > 0:\n                extra_sources[source] = {\n                    'path': source_path,\n                    'name': source_name,\n                    'zip_path': None  # No zip file for direct copy\n                }\n                print(f\"Extra source {source} copied {task_files_copied} files to: {source_path}\")\n            else:\n                print(f\"Warning: No task files found in extra source {source}\")\n                # Remove empty directory\n                if os.path.exists(source_path):\n                    shutil.rmtree(source_path)\n        else:\n            print(f\"Warning: Extra source {source} does not exist\")\n    \n    return extra_sources\n\nworking_dir, submission_dir, source_dir = setup_working_directories()\ndefault_zip = extract_default_submission(submission_zips, submission_dir)\nextra_sources = extract_extra_submissions(submission_zips, source_dir)\n\nprint(f\"\\nSetup complete:\")\nprint(f\"  Working directory: {working_dir}\")\nprint(f\"  Default submission: {submission_dir}\")\nprint(f\"  Extra sources: {len(extra_sources)}\")","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_task_examples(task_num):\n    \"\"\"Load examples for a task (simplified version of verify1.py logic)\"\"\"\n    # Try to find task data file\n    task_data_paths = [\n        f\"/kaggle/input/google-code-golf-2025/task{task_num:03d}.json\",\n        f\"google-code-golf-2025/task{task_num:03d}.json\",\n        f\"./task{task_num:03d}.json\"\n    ]\n    \n    for path in task_data_paths:\n        if os.path.exists(path):\n            with open(path, 'r') as f:\n                return json.load(f)\n    \n    print(f\"Warning: No test data found for task {task_num}\")\n    return None\n\ndef verify_solution_code(solution_code, task_num, examples=None):\n    \"\"\"Verify if a solution works correctly\"\"\"\n    if examples is None:\n        examples = load_task_examples(task_num)\n        if examples is None:\n            return False, \"No test data available\"\n    \n    try:\n        # Execute the solution code\n        namespace = {}\n        exec(solution_code, namespace)\n        \n        if 'p' not in namespace:\n            return False, \"No function 'p' found\"\n        \n        solution_func = namespace['p']\n        if not callable(solution_func):\n            return False, \"Function 'p' is not callable\"\n        \n        # Test on all available test cases\n        test_cases = []\n        for category in ['test', 'train', 'arc-gen']:\n            if category in examples:\n                test_cases.extend(examples[category])\n        \n        if not test_cases:\n            return False, \"No test cases available\"\n        \n        correct_count = 0\n        total_count = len(test_cases)\n        \n        for i, test_case in enumerate(test_cases):\n            try:\n                result = solution_func(copy.deepcopy(test_case['input']))\n                \n                # Convert result to comparable format\n                result_json = json.dumps(result)\n                result_json = result_json.replace(\"true\", \"1\").replace(\"false\", \"0\")\n                \n                # Check for unsafe characters\n                unsafe_chars = re.compile(r\"[^0-9,\\[\\]\\s\\.]\")\n                if unsafe_chars.search(result_json):\n                    return False, f\"Invalid output format: {result_json[:100]}\"\n                \n                result_array = np.array(json.loads(result_json))\n                expected_array = np.array(test_case['output'])\n                \n                if np.array_equal(result_array, expected_array):\n                    correct_count += 1\n                else:\n                    return False, f\"Wrong output on test case {i+1}/{total_count}\"\n                    \n            except Exception as e:\n                return False, f\"Exception on test case {i+1}: {str(e)[:100]}\"\n        \n        return True, f\"Passed all {correct_count}/{total_count} test cases\"\n        \n    except Exception as e:\n        return False, f\"Execution error: {str(e)[:100]}\"","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_task_candidates(task_num, submission_dir, extra_sources):\n    \"\"\"Get all candidate solutions for a task, sorted by size\"\"\"\n    candidates = []\n    \n    # Default solution\n    default_path = os.path.join(submission_dir, f\"task{task_num:03d}.py\")\n    if os.path.exists(default_path):\n        # Use actual file size on disk\n        size = os.path.getsize(default_path)\n        \n        # Read content for verification only\n        content = read_file_as_text(default_path)\n        if content is not None:\n            candidates.append({\n                'path': default_path,\n                'content': content,\n                'size': size,\n                'source': 'default',\n                'source_name': 'default'\n            })\n        else:\n            print(f\"Could not read default task {task_num} from {default_path}\")\n    \n    # Extra source solutions\n    for source, info in extra_sources.items():\n        extra_path = os.path.join(info['path'], f\"task{task_num:03d}.py\")\n        if os.path.exists(extra_path):\n            # Use actual file size on disk\n            size = os.path.getsize(extra_path)\n            \n            # Read content for verification only\n            content = read_file_as_text(extra_path)\n            if content is not None:\n                candidates.append({\n                    'path': extra_path,\n                    'content': content,\n                    'size': size,\n                    'source': source,\n                    'source_name': info['name']\n                })\n    \n    # Sort by size (smallest first)\n    candidates.sort(key=lambda x: x['size'])\n    return candidates\n\ndef select_best_solution(task_num, submission_dir, extra_sources):\n    \"\"\"Select the best (smallest working) solution for a task\"\"\"\n    candidates = get_task_candidates(task_num, submission_dir, extra_sources)\n    \n    if not candidates:\n        print(f\"No candidates found for task {task_num}\")\n        return None\n    \n    # Load examples once for this task\n    examples = load_task_examples(task_num)\n    \n    default_candidate = next((c for c in candidates if c['source'] == 'default'), None)\n    if not default_candidate:\n        print(f\"No default solution found for task {task_num}\")\n        return None\n    \n    # Find smaller candidates than default\n    smaller_candidates = [c for c in candidates if c['size'] < default_candidate['size']]\n    \n    if not smaller_candidates:\n        # No smaller solutions, use default\n        return {\n            'candidate': default_candidate,\n            'used_default': True,\n            'verification_result': None,\n            'alternatives_tested': 0\n        }\n    \n    print(f\"Task {task_num}: Default size {default_candidate['size']}, testing {len(smaller_candidates)} smaller alternatives\")\n    \n    # Test smaller candidates in order (smallest first)\n    for i, candidate in enumerate(smaller_candidates):\n        print(f\"  Testing candidate {i+1}/{len(smaller_candidates)}: {candidate['source_name']} ({candidate['size']} bytes)\")\n        \n        is_working, message = verify_solution_code(candidate['content'], task_num, examples)\n        \n        if is_working:\n            print(f\"  âœ“ Working! Using {candidate['source_name']} solution ({candidate['size']} bytes, saved {default_candidate['size'] - candidate['size']} bytes)\")\n            return {\n                'candidate': candidate,\n                'used_default': False,\n                'verification_result': message,\n                'alternatives_tested': i + 1,\n                'bytes_saved': default_candidate['size'] - candidate['size']\n            }\n        else:\n            print(f\"  âœ— Failed: {message}\")\n    \n    # No smaller solution worked, use default\n    print(f\"  Using default solution ({default_candidate['size']} bytes)\")\n    return {\n        'candidate': default_candidate,\n        'used_default': True,\n        'verification_result': \"No smaller working solution found\",\n        'alternatives_tested': len(smaller_candidates),\n        'bytes_saved': 0\n    }","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nProcessing tasks 1-400...\")\n\nresults = {}\ntotal_bytes_saved = 0\ntasks_improved = 0\ntasks_with_alternatives = 0\nverification_stats = defaultdict(int)\n\nfor task_num in range(1, 401):\n    if task_num % 50 == 0:\n        print(f\"Progress: {task_num}/400 tasks processed\")\n    \n    result = select_best_solution(task_num, submission_dir, extra_sources)\n    \n    if result is None:\n        verification_stats['no_candidates'] += 1\n        continue\n    \n    results[task_num] = result\n    \n    if not result['used_default']:\n        tasks_improved += 1\n        total_bytes_saved += result.get('bytes_saved', 0)\n    \n    if result['alternatives_tested'] > 0:\n        tasks_with_alternatives += 1\n    \n    verification_stats['processed'] += 1\n\nprint(f\"\\nProcessing complete!\")\nprint(f\"Tasks processed: {verification_stats['processed']}\")\nprint(f\"Tasks with smaller alternatives: {tasks_with_alternatives}\")\nprint(f\"Tasks improved: {tasks_improved}\")\nprint(f\"Total bytes saved: {total_bytes_saved}\")","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_final_submission(results, working_dir):\n    \"\"\"Create the final submission with best solutions\"\"\"\n    # Create submission.zip directly in working directory\n    submission_zip_path = os.path.join(working_dir, \"submission.zip\")\n    \n    tasks_written = 0\n    total_size = 0\n    \n    # Create submission.zip directly without intermediate directory\n    missing_tasks = []\n    with zipfile.ZipFile(submission_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for task_num in range(1, 401):\n            if task_num not in results:\n                missing_tasks.append(task_num)\n                continue\n            \n            result = results[task_num]\n            candidate = result['candidate']\n            \n            # Read the original binary file and add it to zip\n            task_filename = f\"task{task_num:03d}.py\"\n            try:\n                with open(candidate['path'], 'rb') as f:\n                    file_data = f.read()\n                zipf.writestr(task_filename, file_data)\n                tasks_written += 1\n                total_size += len(file_data)\n            except Exception as e:\n                print(f\"Error reading {candidate['path']}: {e}\")\n                missing_tasks.append(task_num)\n    \n    # Report missing tasks\n    if missing_tasks:\n        print(f\"WARNING: {len(missing_tasks)} tasks missing from results!\")\n        print(f\"Missing tasks: {missing_tasks[:10]}...\" if len(missing_tasks) > 10 else f\"Missing tasks: {missing_tasks}\")\n    else:\n        print(\"All 400 tasks included in submission\")\n    \n    zip_size = os.path.getsize(submission_zip_path)\n    \n    print(f\"\\nFinal submission created:\")\n    print(f\"  Zip file: {submission_zip_path}\")\n    print(f\"  Tasks: {tasks_written}\")\n    print(f\"  Total uncompressed size: {total_size:,} bytes\")\n    print(f\"  Zip size: {zip_size:,} bytes ({zip_size/1024:.1f} KB)\")\n    print(f\"  Compression ratio: {zip_size/total_size:.3f}\")\n    \n    return submission_zip_path\n\ndef create_others_zip_and_metadata(results, working_dir):\n    \"\"\"Create others.zip with improved solutions and metadata CSV\"\"\"\n    improved_tasks = {task_num: result for task_num, result in results.items() \n                     if not result['used_default']}\n    \n    if not improved_tasks:\n        print(\"No improved tasks found - skipping others.zip creation\")\n        return None, None\n    \n    others_zip_path = os.path.join(working_dir, \"others.zip\")\n    others_csv_path = os.path.join(working_dir, \"others_metadata.csv\")\n    \n    # Create CSV metadata\n    with open(others_csv_path, 'w', newline='') as csvfile:\n        fieldnames = [\n            'task_number', 'source', 'source_name',\n            'default_size', 'chosen_size', 'bytes_saved',\n            'alternatives_tested', 'verification_result'\n        ]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        \n        for task_num, result in sorted(improved_tasks.items()):\n            # Find default size for comparison\n            default_size = None\n            candidates = get_task_candidates(task_num, submission_dir, extra_sources)\n            default_candidate = next((c for c in candidates if c['source'] == 'default'), None)\n            if default_candidate:\n                default_size = default_candidate['size']\n            \n            candidate = result['candidate']\n            writer.writerow({\n                'task_number': f\"task{task_num:03d}\",\n                'source': candidate['source'],\n                'source_name': candidate['source_name'],\n                'default_size': default_size or '',\n                'chosen_size': candidate['size'],\n                'bytes_saved': result.get('bytes_saved', 0),\n                'alternatives_tested': result['alternatives_tested'],\n                'verification_result': result['verification_result'] or ''\n            })\n    \n    # Create others.zip with improved solutions\n    with zipfile.ZipFile(others_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for task_num, result in sorted(improved_tasks.items()):\n            task_filename = f\"task{task_num:03d}.py\"\n            try:\n                with open(result['candidate']['path'], 'rb') as f:\n                    file_data = f.read()\n                zipf.writestr(task_filename, file_data)\n            except Exception as e:\n                print(f\"Error reading {result['candidate']['path']} for others.zip: {e}\")\n    \n    others_zip_size = os.path.getsize(others_zip_path)\n    others_csv_size = os.path.getsize(others_csv_path)\n    \n    print(f\"\\nOthers files created:\")\n    print(f\"  Others.zip: {others_zip_path} ({others_zip_size:,} bytes)\")\n    print(f\"  Metadata CSV: {others_csv_path} ({others_csv_size:,} bytes)\")\n    print(f\"  Improved tasks: {len(improved_tasks)}\")\n    print(f\"  Total bytes saved: {sum(r.get('bytes_saved', 0) for r in improved_tasks.values())}\")\n    \n    return others_zip_path, others_csv_path","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create final submission\nsubmission_zip_path = create_final_submission(results, working_dir)\n\n# Create others.zip and metadata  \nothers_zip_path, others_csv_path = create_others_zip_and_metadata(results, working_dir)\n\n# Calculate final score estimate\ntotal_score = 0\nfor task_num, result in results.items():\n    task_size = result['candidate']['size']\n    task_score = max(1, 2500 - task_size)\n    total_score += task_score\n\nprint(f\"\\nFinal Summary:\")\nprint(\"=\" * 60)\nprint(f\"Environment: {ENVIRONMENT}\")\nprint(f\"Sources processed: {len(sources)}\")\nprint(f\"Submission zips found: {len(submission_zips)}\")\nprint(\"\")\nprint(f\"Tasks processed: {len(results)}\")\nprint(f\"Tasks with smaller alternatives: {tasks_with_alternatives}\")\nprint(f\"Tasks improved: {tasks_improved}\")\nprint(f\"Total bytes saved: {total_bytes_saved:,}\")\nprint(\"\")\nprint(f\"Final submission: {submission_zip_path}\")\nif others_zip_path:\n    print(f\"Others archive: {others_zip_path}\")\n    print(f\"Metadata file: {others_csv_path}\")\nprint(\"\")\nprint(f\"Estimated score: {total_score:,}\")\nprint(\"=\" * 60)","metadata":{},"outputs":[],"execution_count":null}]}