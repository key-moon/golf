{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":95282,"databundleVersionId":13472782,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# 0) CÀI ĐẶT & IMPORT\n# =========================================================\nimport os, json, random, textwrap, gc, math, sys\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import Dataset, DatasetDict\n# import evaluate\n\nSEED = 42\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Nếu chạy trên Kaggle:\nINPUT_ROOT = Path(\"/kaggle/input/google-code-golf-2025\")\nOUTPUT_ROOT = Path(\"/kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T08:37:15.177274Z","iopub.execute_input":"2025-09-17T08:37:15.177575Z","iopub.status.idle":"2025-09-17T08:37:15.187105Z","shell.execute_reply.started":"2025-09-17T08:37:15.177552Z","shell.execute_reply":"2025-09-17T08:37:15.18635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 1) TÌM & ĐỌC DỮ LIỆU JSON (task001.json ... task400.json)\n#    - Mặc định sẽ rà tất cả thư mục trong /kaggle/input\n#      để tìm các file \"task*.json\".\n# =========================================================\n\njson_files = sorted(INPUT_ROOT.rglob(\"task*.json\"))\nassert len(json_files) > 0, \"Không tìm thấy file task*.json trong /kaggle/input\"\n\nprint(f\"Found {len(json_files)} JSON files (e.g., {json_files[:3]})\")\n\ndef grid_to_json_str(grid: List[List[int]]) -> str:\n    # dùng JSON chuẩn để model (ByT5: char-level) xử lý ổn định\n    return json.dumps(grid, separators=(\",\", \":\"))\n\ndef should_use_extras(input_grid, extras):\n    \"\"\"\n    ước lượng độ dài prompt nếu thêm extras; nếu vượt MAX_INPUT_LEN thì bỏ.\n    Không gọi make_single_example để tránh đệ quy.\n    \"\"\"\n    # prompt tối thiểu (không extras)\n    base = (\n        \"Write ONLY this function (no prints, no globals):\\n\"\n        \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\"\n        \"\\nInput:\\n\" + json.dumps(input_grid, separators=(',', ':'))\n    )\n    base_len = len(tokenizer(base, add_special_tokens=False).input_ids)\n\n    # phần extras (tối đa 2 ví dụ)\n    extra_text = \"\"\n    for i, (iin, oout) in enumerate(extras[:2], 1):\n        extra_text += (\n            f\"\\nExample {i}\\nInput:\\n{json.dumps(iin, separators=(',',':'))}\"\n            f\"\\nOutput:\\n{json.dumps(oout, separators=(',',':'))}\"\n        )\n\n    total_text = base + (\"\\nExamples to infer the rule:\" + extra_text if extra_text else \"\")\n    total_len = len(tokenizer(total_text, add_special_tokens=False).input_ids)\n\n    # để dư buffer ~5% phòng token hóa khác biệt\n    return total_len < int(MAX_INPUT_LEN * 0.95)\n\n\ndef make_single_example(input_grid, output_grid, extra_examples=None):\n    parts = []\n    parts.append(\"Write ONLY this function (no prints, no globals):\")\n    parts.append(\"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\")\n    if extra_examples and should_use_extras(input_grid, extra_examples):\n        parts.append(\"Examples to infer the rule:\")\n        for i, (iin, oout) in enumerate(extra_examples[:2], 1):\n            parts.append(\n                f\"\\nExample {i}\\nInput:\\n{json.dumps(iin, separators=(',',':'))}\"\n                f\"\\nOutput:\\n{json.dumps(oout, separators=(',',':'))}\"\n            )\n    parts.append(\"\\nInput:\\n\" + json.dumps(input_grid, separators=(',', ':')))\n    prompt = \"\\n\".join(parts).strip()\n\n    # target \"return constant\" để giữ định dạng code cho training\n    target_code = (\n        \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\"\n        f\"    return {json.dumps(output_grid, separators=(',',':'))}\\n\"\n    ).strip()\n\n    return {\"prompt\": prompt, \"code\": target_code}\n\ndef load_arc_style_pairs(json_path: Path):\n    \"\"\"\n    Đọc file ARC-style JSON: kỳ vọng các khóa 'train' (list các {input, output}),\n    'test' (thường có) ... Trả về list[(in, out)] từ 'train'.\n    \"\"\"\n    obj = json.loads(json_path.read_text())\n    pairs = []\n    if \"train\" in obj:\n        for it in obj[\"train\"]:\n            if \"input\" in it and \"output\" in it:\n                pairs.append((it[\"input\"], it[\"output\"]))\n    return pairs\n\n# Gom toàn bộ dữ liệu thành các (prompt, code)\nsamples = []\nfor jf in json_files:\n    pairs = load_arc_style_pairs(jf)\n    if not pairs:\n        continue\n\n    # Tạo mẫu kiểu \"single-pair\" (baseline), đồng thời thêm vài ví dụ phụ từ cùng file\n    # để tăng tính nhất quán định dạng prompt.\n    extras = []\n    if len(pairs) >= 3:\n        # Lấy 2 ví dụ phụ\n        extras = [pairs[0], pairs[1]]\n\n    for i, (iin, oout) in enumerate(pairs):\n        # Với mỗi cặp trong file, tạo 1 sample huấn luyện\n        # (có thể kèm extras từ cùng nhiệm vụ)\n        ex = make_single_example(iin, oout, extra_examples=extras if i == 0 else None)\n        samples.append(ex)\n\nprint(f\"Prepared {len(samples)} trainable (prompt->code) samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:33:26.392457Z","iopub.execute_input":"2025-09-17T09:33:26.393524Z","iopub.status.idle":"2025-09-17T09:33:32.345914Z","shell.execute_reply.started":"2025-09-17T09:33:26.393489Z","shell.execute_reply":"2025-09-17T09:33:32.345286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 2) CHIA TẬP TRAIN/VAL\n# =========================================================\n\nrandom.shuffle(samples)\nsplit = int(0.95 * len(samples)) if len(samples) > 20 else max(1, int(0.8 * len(samples)))\ntrain_data = samples[:split]\nval_data   = samples[split:]\n\nds_train = Dataset.from_list(train_data)\nds_val   = Dataset.from_list(val_data)\nraw_ds = DatasetDict({\"train\": ds_train, \"validation\": ds_val})\nraw_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:33:37.115991Z","iopub.execute_input":"2025-09-17T09:33:37.116278Z","iopub.status.idle":"2025-09-17T09:33:37.139885Z","shell.execute_reply.started":"2025-09-17T09:33:37.116255Z","shell.execute_reply":"2025-09-17T09:33:37.139313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 3) TOKENIZER & TIỀN XỬ LÝ\n#    - ByT5 (google/byt5-base) là char-level → hợp với JSON/code.\n# =========================================================\n\nMODEL_NAME = \"google/byt5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nMAX_INPUT_LEN  = 6144   \nMAX_TARGET_LEN = 1024 \n\ndef preprocess_fn(batch):\n    model_inputs = tokenizer(batch[\"prompt\"], max_length=MAX_INPUT_LEN, truncation=True)\n    labels = tokenizer(text_target=batch[\"code\"], max_length=MAX_TARGET_LEN, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized = raw_ds.map(preprocess_fn, batched=True, remove_columns=raw_ds[\"train\"].column_names)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, padding=\"longest\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:33:40.064097Z","iopub.execute_input":"2025-09-17T09:33:40.064677Z","iopub.status.idle":"2025-09-17T09:33:41.5196Z","shell.execute_reply.started":"2025-09-17T09:33:40.064653Z","shell.execute_reply":"2025-09-17T09:33:41.518913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 4) MODEL & THAM SỐ HUẤN LUYỆN — Seq2SeqTrainer cho ByT5\n# =========================================================\nimport torch, numpy as np\nfrom transformers import (\n    T5ForConditionalGeneration,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n\nVOCAB_SIZE = int(model.config.vocab_size)\nPAD_ID     = int(tokenizer.pad_token_id)\nSPECIALS   = set(int(i) for i in tokenizer.all_special_ids)\nOFFSET     = int(getattr(tokenizer, \"offset\", 3))\n\ndef _to_int_seq(seq):\n    \"\"\"Ép về list[int], flatten nông nếu gặp list/ndarray lồng nông.\"\"\"\n    out = []\n    for x in seq:\n        if isinstance(x, (list, tuple, np.ndarray)):\n            for y in x:\n                try: out.append(int(y))\n                except: out.append(PAD_ID)\n        else:\n            try: out.append(int(x))\n            except: out.append(PAD_ID)\n    return out\n\ndef _sanitize_ids_batch(batch_ids):\n    \"\"\"Clamp id theo vocab, xử lý id < OFFSET (không phải special) -> PAD.\"\"\"\n    clean = []\n    for seq in batch_ids:\n        seq = _to_int_seq(seq)\n        fixed = []\n        for tid in seq:\n            if tid not in SPECIALS and tid < OFFSET:\n                tid = PAD_ID\n            if tid < 0 or tid >= VOCAB_SIZE:\n                tid = PAD_ID\n            fixed.append(tid)\n        clean.append(fixed)\n    return clean\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    # Seq2SeqTrainer + predict_with_generate -> preds là token ids; nhưng vẫn robust:\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    # chuyển về list để dễ xử lý ragged\n    if isinstance(preds, np.ndarray):\n        preds = preds.tolist()\n    if isinstance(labels, np.ndarray):\n        labels = labels.tolist()\n\n    # làm sạch pred ids\n    pred_ids = _sanitize_ids_batch(preds)\n\n    # thay -100 trong labels -> PAD, rồi làm sạch\n    labels = [[(int(t) if t != -100 else PAD_ID) for t in seq] for seq in labels]\n    label_ids = _sanitize_ids_batch(labels)\n\n    # decode\n    pred_str  = tokenizer.batch_decode(pred_ids,  skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    exacts = [1 if p.strip() == y.strip() else 0 for p, y in zip(pred_str, label_str)]\n    return {\"exact_match\": float(sum(exacts))/len(exacts) if exacts else 0.0}\n\nBATCH_SIZE = 1          # trước là 2\nGRAD_ACC   = 32         # trước là 8 (giữ effective batch ≈ 32)\nLR         = 2e-4\nNUM_EPOCHS = 5 \nMAX_GEN    = MAX_TARGET_LEN\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=str(OUTPUT_ROOT / \"byt5-codegen\"),\n    num_train_epochs=NUM_EPOCHS,\n    learning_rate=LR,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACC,\n\n    eval_strategy=\"steps\",\n    eval_steps=100,\n\n    save_steps=200,\n    logging_steps=50,\n    save_total_limit=2,\n\n    fp16=torch.cuda.is_available(),\n    bf16=False,\n    gradient_checkpointing=True,\n\n    # Quan trọng: bắt Seq2SeqTrainer dùng generate() khi eval\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LEN,\n    generation_num_beams=1,   # tùy chọn\n\n    report_to=[],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_exact_match\",\n    greater_is_better=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:33:46.130505Z","iopub.execute_input":"2025-09-17T09:33:46.131046Z","iopub.status.idle":"2025-09-17T09:33:48.940204Z","shell.execute_reply.started":"2025-09-17T09:33:46.131023Z","shell.execute_reply":"2025-09-17T09:33:48.93948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 5) HUẤN LUYỆN\n# =========================================================\n\ntrain_result = trainer.train()\ntrainer.save_model(OUTPUT_ROOT / \"byt5-codegen-best\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:33:52.520445Z","iopub.execute_input":"2025-09-17T09:33:52.521104Z","iopub.status.idle":"2025-09-17T10:11:11.531921Z","shell.execute_reply.started":"2025-09-17T09:33:52.521078Z","shell.execute_reply":"2025-09-17T10:11:11.531053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 6) SUY LUẬN: SINH CODE TỪ 1 NHIỆM VỤ CỤ THỂ & THỰC THI THỬ\n# =========================================================\n\ndef build_inference_prompt(task_json_path: Path, max_examples: int = 3) -> Dict[str, Any]:\n    obj = json.loads(task_json_path.read_text())\n    train_pairs = [(x[\"input\"], x[\"output\"]) for x in obj.get(\"train\", []) if \"input\" in x and \"output\" in x]\n    test_items  = obj.get(\"test\", [])\n    assert train_pairs, \"No train pairs found.\"\n    assert test_items, \"No test items found.\"\n\n    # Lấy tối đa vài ví dụ train để làm ngữ cảnh\n    k = min(max_examples, len(train_pairs))\n    extras = train_pairs[:k]\n\n    # Chọn test[0] để demo\n    test_input = test_items[0][\"input\"]\n    prompt = make_single_example(test_input, test_items[0].get(\"output\", test_input), extra_examples=extras)[\"prompt\"]\n    # Lưu ý: ở inference thực tế, bạn thường KHÔNG biết output của test.\n    # Ở đây chỉ để tạo prompt dạng \"few-shot\" nhất quán.\n    return {\"prompt\": prompt, \"test_input\": test_input, \"gt_output\": test_items[0].get(\"output\")}\n\ndef generate_code(prompt: str, max_new_tokens: int = 384) -> str:\n    prefix = \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n    \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    dec_prefix = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n    \n    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    gen_ids = model.generate(\n        **enc,\n        decoder_input_ids=dec_prefix,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        early_stopping=True,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n        length_penalty=1.0,\n    )\n    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n    # Đảm bảo có prefix\n    if not text.startswith(prefix):\n        text = prefix + text\n    return text\n\n# Chọn ngẫu nhiên một file để demo suy luận\ndemo_path = random.choice(json_files)\nprint(\"Demo with:\", demo_path)\ndemo = build_inference_prompt(demo_path, max_examples=3)\nprint(\"\\n=== PROMPT (truncated) ===\\n\", demo[\"prompt\"][:800], \"...\\n\")\n\ngen_code = generate_code(demo[\"prompt\"])\nprint(\"=== GENERATED CODE ===\\n\", gen_code[:1000], \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:05:12.221347Z","iopub.execute_input":"2025-09-17T11:05:12.221892Z","iopub.status.idle":"2025-09-17T11:05:13.386885Z","shell.execute_reply.started":"2025-09-17T11:05:12.221869Z","shell.execute_reply":"2025-09-17T11:05:13.386156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 7) THỰC THI CODE (CẨN TRỌNG) & ĐÁNH GIÁ\n#    - Sandbox cơ bản (KHÔNG đảm bảo an toàn tuyệt đối; chỉ dùng offline/Kaggle).\n# =========================================================\n\nSAFE_BUILTINS = {\n    \"range\": range, \"len\": len, \"enumerate\": enumerate, \"list\": list, \"max\": max, \"min\": min, \"sum\": sum,\n    \"abs\": abs, \"zip\": zip, \"map\": map, \"filter\": filter, \"all\": all, \"any\": any\n}\n\ndef run_transform(code_str: str, grid: List[List[int]]) -> List[List[int]]:\n    local_env = {}\n    try:\n        exec(compile(code_str, \"<generated>\", \"exec\"), {\"__builtins__\": SAFE_BUILTINS}, local_env)\n        if \"transform\" not in local_env:\n            raise RuntimeError(\"No function 'transform' defined.\")\n        result = local_env[\"transform\"](grid)\n        return result\n    except Exception as e:\n        print(\"Execution error:\", e)\n        return None\n\npred_grid = run_transform(gen_code, demo[\"test_input\"])\nprint(\"Pred grid:\", pred_grid)\nif demo[\"gt_output\"] is not None:\n    print(\"GT grid   :\", demo[\"gt_output\"])\n    print(\"Exact match:\", pred_grid == demo[\"gt_output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:38:41.033032Z","iopub.execute_input":"2025-09-17T10:38:41.03331Z","iopub.status.idle":"2025-09-17T10:38:41.04021Z","shell.execute_reply.started":"2025-09-17T10:38:41.03329Z","shell.execute_reply":"2025-09-17T10:38:41.039538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 8) VIẾT task.py TỪ CODE SINH RA & IMPORT EVALUATOR\n# =========================================================\nimport re, ast\nfrom pathlib import Path\n\n_HEADER = \"def transform(grid: list[list[int]]) -> list[list[int]]:\"\n\ndef _clean_text(s: str) -> str:\n    # bỏ markdown ```...``` và control chars\n    s = re.sub(r\"```(?:python)?\\s*([\\s\\S]*?)```\", r\"\\1\", s, flags=re.I)\n    s = \"\".join(ch for ch in s if ch == \"\\n\" or 32 <= ord(ch) <= 0x10FFFF)\n    # cắt mọi thứ trước header (nếu model còn in thêm lời)\n    i = s.find(_HEADER)\n    if i >= 0: s = s[i:]\n    # giữ lại chỉ thân hàm + wrapper\n    return s.strip()\n\ndef _ensure_indent(code: str) -> str:\n    # nếu sau header không có indent, thêm 4 spaces vào mỗi dòng thân\n    lines = code.splitlines()\n    if not lines or not lines[0].startswith(\"def transform\"):\n        return code\n    body = lines[1:]\n    if not body:\n        body = [\"    return grid\"]\n    # nếu tất cả body đã indent đúng thì giữ nguyên, nếu không thì ép indent\n    if not all((not ln) or ln.startswith(\"    \") for ln in body):\n        body = [(\"    \" + ln if ln and not ln.startswith(\"    \") else ln) for ln in body]\n    return \"\\n\".join([lines[0]] + body)\n\ndef _compile_ok(src: str) -> bool:\n    try:\n        ast.parse(src)\n        return True\n    except SyntaxError:\n        return False\n\ndef write_task_py_from_generated(gen_code: str, path=OUTPUT_ROOT / \"task.py\"):\n    src = _clean_text(gen_code or \"\")\n    # nếu thiếu header (khó xảy ra sau patch A) thì thêm\n    if not src.startswith(_HEADER):\n        src = _HEADER + \"\\n    \" + src\n\n    # đảm bảo có ít nhất 1 return trong thân; nếu không, thêm 'return grid'\n    if \"return \" not in src.split(_HEADER, 1)[-1]:\n        src = src.rstrip() + \"\\n    return grid\\n\"\n\n    src = _ensure_indent(src)\n\n    # nếu vẫn lỗi cú pháp -> fallback sang thân rỗng nhưng hợp lệ\n    if not _compile_ok(src):\n        src = _HEADER + \"\\n    return grid\\n\"\n\n    wrapper = \"\\n\\ndef p(grid):\\n    return transform(grid)\\n\"\n    Path(path).write_text(src.rstrip() + wrapper, encoding=\"utf-8\")\n    print(f\"Wrote {path} (forced header + sanitized)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:38:44.843479Z","iopub.execute_input":"2025-09-17T10:38:44.844149Z","iopub.status.idle":"2025-09-17T10:38:44.852447Z","shell.execute_reply.started":"2025-09-17T10:38:44.844123Z","shell.execute_reply":"2025-09-17T10:38:44.85169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 9) SINH CODE, TẠO task.py, VÀ CHẤM\n#    (Dùng model/tokenizer/trainer ở các cell trước)\n# =========================================================\nimport json, random\n\n# ---- chọn 1 task để demo (VD: task400.json) ----\ntask_glob = list(INPUT_ROOT.rglob(\"task400.json\")) or list(INPUT_ROOT.rglob(\"task*.json\"))\nassert task_glob, \"Không tìm thấy task*.json trong /kaggle/input\"\ntask_path = task_glob[0]\ntask_num_match = re.search(r\"task(\\d+)\\.json$\", str(task_path))\ntask_num = int(task_num_match.group(1)) if task_num_match else 0\nprint(\"Using task:\", task_path, \"| task_num:\", task_num)\n\n# ---- tạo prompt few-shot từ file task ----\nobj = json.loads(task_path.read_text())\npairs = [(x[\"input\"], x[\"output\"]) for x in obj.get(\"train\", []) if \"input\" in x and \"output\" in x]\nassert pairs, \"task.json thiếu cặp train input/output\"\nextras = pairs[: min(3, len(pairs))]\n\ntest_items = obj.get(\"test\", [])\nif not test_items:\n    # Nếu không có test, dùng tạm train[0] để huấn luyện format prompt (không ảnh hưởng verify)\n    test_items = [{\"input\": pairs[0][0]}]\n\ntest_input = test_items[0][\"input\"]\nprompt = make_single_example(test_input, pairs[0][1], extra_examples=extras)[\"prompt\"]  # tái dùng hàm ở cell trước\n\n# ---- generate code bằng model đã fine-tuned ----\ngen_code = generate_code(prompt, max_new_tokens=384)\nprint(\"=== GENERATED (truncated) ===\\n\", gen_code[:600], \"...\\n\")\n\n# ---- ghi task.py (p() gọi transform()) ----\nwrite_task_py_from_generated(gen_code, OUTPUT_ROOT/\"task.py\")\n\n# ---- chuẩn bị examples cho evaluator ----\nexamples = {\n    \"train\": obj.get(\"train\", []),\n    \"test\":  obj.get(\"test\", []),\n    \"arc-gen\": obj.get(\"arc-gen\", []),  # nếu thiếu sẽ thành []\n}\n\n# ---- chạy verify_program từ evaluator ----\n# Lưu ý: nếu muốn chỉ định đường dẫn task.py khác, truyền task_path=...\nevaluator.verify_program(task_num=task_num, examples=examples)\n\ndef _lengths(ds, field):\n    ids = tokenizer(ds[field][:50], truncation=False)  # sample 50 để nhanh\n    return [len(x) for x in ids[\"input_ids\"]]\n\nin_lens  = _lengths(raw_ds[\"train\"], \"prompt\")\ntgt_lens = _lengths(raw_ds[\"train\"], \"code\")\nprint(\"max input len:\", max(in_lens), \" | MAX_INPUT_LEN =\", MAX_INPUT_LEN)\nprint(\"max target len:\", max(tgt_lens), \" | MAX_TARGET_LEN =\", MAX_TARGET_LEN)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T10:38:48.430401Z","iopub.execute_input":"2025-09-17T10:38:48.431107Z","iopub.status.idle":"2025-09-17T10:38:49.745165Z","shell.execute_reply.started":"2025-09-17T10:38:48.431081Z","shell.execute_reply":"2025-09-17T10:38:49.744609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 9b) HEURISTIC SYNTHESIS (nếu model sinh rác)\n#      - Tìm phép biến đổi không gian (rotate/flip/transpose)\n#        + ánh xạ màu (color map) sao cho khớp toàn bộ train.\n#      - Nếu tìm thấy, ghi thẳng task.py với code \"thật\".\n# =========================================================\nfrom copy import deepcopy\n\ndef transpose(g): return [list(r) for r in zip(*g)]\ndef rot90(g):    return transpose(g[::-1])\ndef rot180(g):   return [r[::-1] for r in g[::-1]]\ndef rot270(g):   return rot90(rot180(g))\ndef flip_h(g):   return [r[::-1] for r in g]\ndef flip_v(g):   return g[::-1]\ndef identity(g): return [row[:] for row in g]\n\nSPATIAL_OPS = [\n    (\"identity\", identity),\n    (\"rot90\",   rot90),\n    (\"rot180\",  rot180),\n    (\"rot270\",  rot270),\n    (\"flip_h\",  flip_h),\n    (\"flip_v\",  flip_v),\n    (\"transpose\", transpose),\n]\n\ndef same_shape(a, b):\n    return len(a)==len(b) and all(len(ra)==len(rb) for ra, rb in zip(a,b))\n\ndef learn_color_map(src, tgt):\n    \"\"\"Học ánh xạ màu toàn cục: mỗi màu a -> đúng 1 màu b (không phụ thuộc vị trí).\"\"\"\n    m = {}\n    for ra, rb in zip(src, tgt):\n        for a, b in zip(ra, rb):\n            if a in m and m[a] != b: \n                return None\n            m[a] = b\n    return m\n\ndef apply_color_map(g, m, default=None):\n    out = []\n    for r in g:\n        nr = []\n        for v in r:\n            if v in m:\n                nr.append(m[v])\n            else:\n                nr.append(v if default is None else default)\n        out.append(nr)\n    return out\n\ndef compose_ops(ops):\n    def comp(g):\n        cur = g\n        for _, f in ops:\n            cur = f(cur)\n        return cur\n    name = \"|\".join(n for n,_ in ops) if ops else \"identity\"\n    return name, comp\n\ndef find_rule(train_pairs):\n    \"\"\"\n    Tìm chuỗi op không gian (0..2 bước) + màu (toàn cục) khớp 100% train.\n    Trả về: (ops_list, color_map_dict) hoặc None\n    \"\"\"\n    candidates = [()]  # độ dài 0\n    # độ dài 1\n    for op in SPATIAL_OPS:\n        candidates.append((op,))\n    # độ dài 2 (op1 -> op2)\n    for i, op1 in enumerate(SPATIAL_OPS):\n        for op2 in SPATIAL_OPS:\n            candidates.append((op1, op2))\n\n    for ops in candidates:\n        name, f = compose_ops(list(ops))\n        # học color-map từ cặp đầu\n        src0, tgt0 = train_pairs[0]\n        mid0 = f(src0)\n        if not same_shape(mid0, tgt0):\n            continue\n        cmap = learn_color_map(mid0, tgt0)\n        if cmap is None:\n            continue\n        # kiểm tra tất cả cặp\n        ok = True\n        for src, tgt in train_pairs:\n            mid = f(src)\n            if not same_shape(mid, tgt):\n                ok = False; break\n            pred = apply_color_map(mid, cmap)\n            if pred != tgt:\n                ok = False; break\n        if ok:\n            return list(ops), cmap\n    return None\n\ndef synthesize_code_from_rule(ops, cmap):\n    \"\"\"Sinh mã Python tối giản cho transform() tương ứng với (ops + color map).\"\"\"\n    op_code = {\n        \"identity\":  \"def _id(g): return [row[:] for row in g]\",\n        \"transpose\": \"def _transpose(g): return [list(r) for r in zip(*g)]\",\n        \"rot90\":     \"def _rot90(g): return _transpose(g[::-1])\",\n        \"rot180\":    \"def _rot180(g): return [r[::-1] for r in g[::-1]]\",\n        \"rot270\":    \"def _rot270(g): return _rot90(_rot180(g))\",\n        \"flip_h\":    \"def _flip_h(g): return [r[::-1] for r in g]\",\n        \"flip_v\":    \"def _flip_v(g): return g[::-1]\",\n    }\n    seq = []\n    for name, _ in ops:\n        seq.append(name)\n    seq_code = \"\\n\".join(op_code[n] for n in sorted(set(seq)) if n in op_code)\n\n    pipe_lines = []\n    pipe_lines.append(\"mid = [row[:] for row in grid]\")\n    for name in seq:\n        if name==\"identity\": \n            continue\n        pipe_lines.append({\n            \"transpose\": \"mid = _transpose(mid)\",\n            \"rot90\":     \"mid = _rot90(mid)\",\n            \"rot180\":    \"mid = _rot180(mid)\",\n            \"rot270\":    \"mid = _rot270(mid)\",\n            \"flip_h\":    \"mid = _flip_h(mid)\",\n            \"flip_v\":    \"mid = _flip_v(mid)\",\n        }[name])\n\n    # color map code\n    items = \", \".join(f\"{k}:{v}\" for k,v in sorted(cmap.items()))\n    cmap_code = (\n        \"cmap = {\"+items+\"}\\n\"\n        \"out = []\\n\"\n        \"for r in mid:\\n\"\n        \"    out.append([cmap.get(v, v) for v in r])\\n\"\n        \"return out\"\n    )\n\n    body = \"\\n    \".join(pipe_lines + [cmap_code])\n\n    code = f\"\"\"\ndef transform(grid: list[list[int]]) -> list[list[int]]:\n    {body}\n\ndef p(grid):\n    return transform(grid)\n\"\"\".strip()\n\n    # Thêm helpers nếu cần\n    if seq:\n        code = seq_code + \"\\n\\n\" + code\n    return code\n\n# ===== Chạy synthesis khi cần =====\ntrain_pairs = pairs  # từ Cell 9 (đã đọc obj)\nrule = find_rule(train_pairs)\nif rule is not None:\n    ops, cmap = rule\n    synth_code = synthesize_code_from_rule(ops, cmap)\n    (OUTPUT_ROOT/\"task.py\").write_text(synth_code, encoding=\"utf-8\")\n    print(\"Wrote task.py via heuristic synthesis ✅\")\nelse:\n    print(\"No simple rule found (ops<=2 + color-map). Keeping previous task.py.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:14:00.275338Z","iopub.execute_input":"2025-09-17T09:14:00.275648Z","iopub.status.idle":"2025-09-17T09:14:00.293324Z","shell.execute_reply.started":"2025-09-17T09:14:00.275626Z","shell.execute_reply":"2025-09-17T09:14:00.292557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Inference sạch cho 1 task =====\nimport json, re, ast\nfrom pathlib import Path\nimport torch\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nDEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_DIR  = OUTPUT_ROOT / \"byt5-codegen-best\"   # <- chỗ bạn Trainer.save_model()\nTOKENIZER  = \"google/byt5-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\nmodel     = T5ForConditionalGeneration.from_pretrained(str(MODEL_DIR)).to(DEVICE).eval()\n\nHEADER = \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n    \"\n\ndef generate_code(prompt: str, max_new_tokens: int = 384) -> str:\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    dec_prefix = tokenizer(HEADER, return_tensors=\"pt\").input_ids.to(DEVICE)\n    with torch.no_grad():\n        ids = model.generate(\n            **enc,\n            decoder_input_ids=dec_prefix,     # ép mở đầu đúng header\n            max_new_tokens=max_new_tokens,\n            do_sample=False,                   # beam search → sạch hơn\n            num_beams=4,\n            early_stopping=True,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            length_penalty=1.0,\n        )\n    text = tokenizer.decode(ids[0], skip_special_tokens=True)\n    if not text.startswith(HEADER):\n        text = HEADER + text\n    return text\n\ndef _clean_code(s: str) -> str:\n    # bỏ ```...```, control chars, cắt mọi thứ trước HEADER\n    s = re.sub(r\"```(?:python)?\\s*([\\s\\S]*?)```\", r\"\\1\", s, flags=re.I)\n    s = \"\".join(ch for ch in s if ch == \"\\n\" or 32 <= ord(ch) <= 0x10FFFF)\n    i = s.find(HEADER.strip().splitlines()[0])\n    if i >= 0:\n        s = s[i:]\n    # đảm bảo có header\n    if not s.startswith(HEADER.splitlines()[0]):\n        s = HEADER + \"return grid\\n\"\n    # bảo đảm indent thân hàm\n    lines = s.splitlines()\n    head = lines[0].strip()\n    body = lines[1:] if len(lines) > 1 else []\n    if not body or not any(\"return\" in ln for ln in body):\n        body = [\"    return grid\"]\n    if not all((not ln) or ln.startswith(\"    \") for ln in body):\n        body = [(\"    \"+ln if ln and not ln.startswith(\"    \") else ln) for ln in body]\n    final = [HEADER.rstrip()] + body\n    code = \"\\n\".join(final) + \"\\n\\n\" + \"def p(grid):\\n    return transform(grid)\\n\"\n    # compile-check\n    try:\n        ast.parse(code)\n    except SyntaxError:\n        code = HEADER + \"    return grid\\n\\n\" + \"def p(grid):\\n    return transform(grid)\\n\"\n    return code\n\n# ---- chọn task ----\ntask_glob = list(INPUT_ROOT.rglob(\"task400.json\")) or list(INPUT_ROOT.rglob(\"task*.json\"))\nassert task_glob, \"Không tìm thấy task*.json\"\ntask_path = task_glob[0]\ntask_num  = int(Path(task_path).stem.replace(\"task\",\"\"))\nprint(\"Using task:\", task_path, \"| task_num:\", task_num)\n\n# ---- build prompt NGẮN để tránh truncation (không nhồi extras nếu quá dài) ----\nobj = json.loads(Path(task_path).read_text())\npairs = [(x[\"input\"], x[\"output\"]) for x in obj.get(\"train\", []) if \"input\" in x and \"output\" in x]\nassert pairs, \"task.json thiếu cặp train input/output\"\ntest_items = obj.get(\"test\", []) or [{\"input\": pairs[0][0]}]\n\ndef prompt_minimal(iin):\n    return (\n        \"Write ONLY this function (no prints, no globals):\\n\"\n        \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\\n\"\n        \"Input:\\n\" + json.dumps(iin, separators=(',',':'))\n    )\n\nprompt   = prompt_minimal(test_items[0][\"input\"])\ngen_code = generate_code(prompt, max_new_tokens=512)\nprint(\"=== GEN PREVIEW ===\\n\", gen_code[:300], \"...\\n\")\n\ncode = _clean_code(gen_code)\n(OUTPUT_ROOT/\"task.py\").write_text(code, encoding=\"utf-8\")\nprint(\"Wrote\", OUTPUT_ROOT/\"task.py\")\n\n# ---- verifier (API của bạn không có task_path) ----\nexamples = {\"train\": obj.get(\"train\", []), \"test\": obj.get(\"test\", []), \"arc-gen\": obj.get(\"arc-gen\", [])}\nevaluator.verify_program(task_num=task_num, examples=examples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:13:44.40833Z","iopub.execute_input":"2025-09-17T11:13:44.409043Z","iopub.status.idle":"2025-09-17T11:13:46.463187Z","shell.execute_reply.started":"2025-09-17T11:13:44.409017Z","shell.execute_reply":"2025-09-17T11:13:46.462602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport json, torch\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_DIR = OUTPUT_ROOT / \"byt5-codegen-best\"   # chỗ bạn save_model\ntokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\nmodel = T5ForConditionalGeneration.from_pretrained(str(MODEL_DIR)).to(DEVICE).eval()\n\nsample_task = next(iter(INPUT_ROOT.rglob(\"task*.json\")))\nobj = json.loads(Path(sample_task).read_text())\niin, oout = obj[\"train\"][0][\"input\"], obj[\"train\"][0][\"output\"]\n\nprompt = (\n    \"Write ONLY this function (no prints, no globals):\\n\"\n    \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\\n\"\n    \"Input:\\n\" + json.dumps(iin, separators=(',',':'))\n)\n\nHEADER = \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n    \"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\ndec_prefix = tokenizer(HEADER, return_tensors=\"pt\").input_ids.to(DEVICE)\nwith torch.no_grad():\n    ids = model.generate(**inputs, decoder_input_ids=dec_prefix, max_new_tokens=256,\n                         do_sample=False, num_beams=4, eos_token_id=tokenizer.eos_token_id,\n                         pad_token_id=tokenizer.pad_token_id)\ntext = tokenizer.decode(ids[0], skip_special_tokens=True)\nprint(text[:300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:10:09.466117Z","iopub.execute_input":"2025-09-17T11:10:09.46639Z","iopub.status.idle":"2025-09-17T11:10:11.228217Z","shell.execute_reply.started":"2025-09-17T11:10:09.46637Z","shell.execute_reply":"2025-09-17T11:10:11.22743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport json, random\n\ndef build_supervised_set_with_heuristics():\n    items = []\n    all_tasks = sorted(INPUT_ROOT.rglob(\"task*.json\"))\n    print(\"Scanning\", len(all_tasks), \"tasks...\")\n    ok = 0\n    for p in all_tasks:\n        obj = json.loads(Path(p).read_text())\n        pairs = [(x[\"input\"], x[\"output\"]) for x in obj.get(\"train\", []) if \"input\" in x and \"output\" in x]\n        if not pairs:\n            continue\n        rule = find_rule(pairs)  # dùng từ Cell 9b bạn đã có: (ops, cmap) hoặc None\n        if rule is None:\n            continue\n        ops, cmap = rule\n        code = synthesize_code_from_rule(ops, cmap)  # code “thật” giải task\n\n        # prompt ngắn: chỉ đưa Input của test[0] để giữ thống nhất\n        test_items = obj.get(\"test\", []) or [{\"input\": pairs[0][0]}]\n        prompt = (\n            \"Write ONLY this function (no prints, no globals):\\n\"\n            \"def transform(grid: list[list[int]]) -> list[list[int]]:\\n\\n\"\n            \"Input:\\n\" + json.dumps(test_items[0][\"input\"], separators=(',',':'))\n        )\n        items.append({\"prompt\": prompt, \"code\": code})\n        ok += 1\n    print(f\"Prepared {ok} heuristic-labeled samples.\")\n    return items\n\nheuristic_samples = build_supervised_set_with_heuristics()\nassert len(heuristic_samples) > 0, \"Heuristic couldn't solve any task; mở rộng phép biến đổi hoặc giữ cả label hằng số.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:13:10.138086Z","iopub.execute_input":"2025-09-17T11:13:10.138845Z","iopub.status.idle":"2025-09-17T11:13:16.704872Z","shell.execute_reply.started":"2025-09-17T11:13:10.138819Z","shell.execute_reply":"2025-09-17T11:13:16.704101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# samples_hard = heuristic_samples\n# samples_easy = samples (tập cũ prompt->'return hằng số')\nmix_ratio = 0.8  # 80% code thật, 20% hằng số\nn_true = int(mix_ratio * len(heuristic_samples))\nrandom.shuffle(heuristic_samples)\nrandom.shuffle(samples)  # 'samples' là tập cũ bạn tạo\nmixed = heuristic_samples[:n_true] + samples[: max(0, len(heuristic_samples)-n_true)]\nrandom.shuffle(mixed)\n\nds_train = Dataset.from_list(mixed[: int(0.95*len(mixed))])\nds_val   = Dataset.from_list(mixed[int(0.95*len(mixed)):])\ntokenized = DatasetDict({\"train\": ds_train, \"validation\": ds_val}).map(\n    preprocess_fn, batched=True, remove_columns=[\"prompt\",\"code\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T11:13:29.809825Z","iopub.execute_input":"2025-09-17T11:13:29.81006Z","iopub.status.idle":"2025-09-17T11:13:29.87653Z","shell.execute_reply.started":"2025-09-17T11:13:29.810044Z","shell.execute_reply":"2025-09-17T11:13:29.875917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 10) (TÙY CHỌN) ĐÁNH GIÁ HÀNG LOẠT TASK\n# =========================================================\nfrom collections import defaultdict\n\nresults = defaultdict(list)\nall_tasks = sorted(INPUT_ROOT.rglob(\"task*.json\"))\nprint(f\"Found {len(all_tasks)} tasks.\")\n\nfor tpath in all_tasks:\n    m = re.search(r\"task(\\d+)\\.json$\", str(tpath))\n    tnum = int(m.group(1)) if m else 0\n    obj = json.loads(tpath.read_text())\n    pairs = [(x[\"input\"], x[\"output\"]) for x in obj.get(\"train\", []) if \"input\" in x and \"output\" in x]\n    if not pairs:\n        continue\n    extras = pairs[: min(3, len(pairs))]\n    test_items = obj.get(\"test\", []) or [{\"input\": pairs[0][0]}]\n    test_input = test_items[0][\"input\"]\n\n    prompt = make_single_example(test_input, pairs[0][1], extra_examples=extras)[\"prompt\"]\n    code = generate_code(prompt, max_new_tokens=384)\n    write_task_py_from_generated(code, WORKING/\"task.py\")\n\n    examples = {\"train\": obj.get(\"train\", []), \"test\": obj.get(\"test\", []), \"arc-gen\": obj.get(\"arc-gen\", [])}\n    print(f\"\\n=== VERIFY task{tnum:03d} ===\")\n    evaluator.verify_program(task_num=tnum, examples=examples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T09:14:09.976496Z","iopub.execute_input":"2025-09-17T09:14:09.976763Z","iopub.status.idle":"2025-09-17T09:14:10.00138Z","shell.execute_reply.started":"2025-09-17T09:14:09.976741Z","shell.execute_reply":"2025-09-17T09:14:10.000358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install transformers==4.44.2 datasets==2.21.0 accelerate==0.34.2 evaluate==0.4.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T03:31:56.674334Z","iopub.execute_input":"2025-09-17T03:31:56.674855Z","iopub.status.idle":"2025-09-17T03:33:54.555455Z","shell.execute_reply.started":"2025-09-17T03:31:56.674831Z","shell.execute_reply":"2025-09-17T03:33:54.554595Z"}},"outputs":[],"execution_count":null}]}