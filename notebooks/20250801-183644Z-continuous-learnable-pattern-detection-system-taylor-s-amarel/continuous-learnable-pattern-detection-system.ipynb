{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13245791,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport zipfile\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom scipy.ndimage import convolve, label, distance_transform_edt, binary_erosion, binary_dilation, gaussian_filter\nfrom scipy.spatial import distance_matrix, ConvexHull\nfrom scipy.stats import mode, entropy, skew, kurtosis\nfrom scipy.signal import find_peaks, correlate2d\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.decomposition import PCA, NMF, FastICA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.manifold import TSNE\nimport hashlib\nimport itertools\nfrom functools import lru_cache\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass UltraAdvancedARCSolver:\n    \"\"\"\n    Ultra-advanced solver with:\n    - Hierarchical pattern detection\n    - Shared learning across tasks\n    - Advanced feature engineering\n    - Ensemble modeling\n    - Code optimization\n    \"\"\"\n    \n    def __init__(self):\n        # Extended pattern library with verified solutions\n        self.verified_patterns = {\n            # Basic transformations\n            'rot90': (\"def p(g):return[list(r)for r in zip(*g[::-1])]\", 46),\n            'rot180': (\"def p(g):return[r[::-1]for r in g[::-1]]\", 40),\n            'rot270': (\"def p(g):return[list(r)for r in zip(*g)][::-1]\", 46),\n            'fliph': (\"def p(g):return[r[::-1]for r in g]\", 34),\n            'flipv': (\"def p(g):return g[::-1]\", 23),\n            'trans': (\"def p(g):return[list(r)for r in zip(*g)]\", 40),\n            \n            # Scaling patterns\n            'scale2x': (\"def p(g):return[[g[i//2][j//2]for j in range(len(g[0])*2)]for i in range(len(g)*2)]\", 88),\n            'scale3x': (\"def p(g):return[[g[i//3][j//3]for j in range(len(g[0])*3)]for i in range(len(g)*3)]\", 88),\n            'half': (\"def p(g):return[r[::2]for r in g[::2]]\", 39),\n            \n            # Color operations\n            'fill': (\"def p(g):return[[{c}]*len(g[0])for _ in g]\", 42),\n            'swap': (\"def p(g):return[[{b}if x=={a}else{a}if x=={b}else x for x in r]for r in g]\", 70),\n            'map': (\"def p(g):m={m};return[[m.get(x,x)for x in r]for r in g]\", 54),\n            \n            # Pattern fills\n            'checker': (\"def p(g):return[[{a}if(i+j)%2else{b}for j in range(len(g[0]))]for i in range(len(g))]\", 82),\n            'border': (\"def p(g):h,w=len(g),len(g[0]);return[[{c}if i in[0,h-1]or j in[0,w-1]else g[i][j]for j in range(w)]for i in range(h)]\", 119),\n            \n            # Advanced patterns\n            'crop': (\"def p(g):return[r[{x1}:{x2}]for r in g[{y1}:{y2}]]\", 48),\n            'pad': (\"def p(g):return[[0]*{w}for _ in range({h})]+[r+[0]*({w}-len(r))for r in g]\", 69),\n            'extract': (\"def p(g):return[[x if x=={c}else 0for x in r]for r in g]\", 56),\n            'count': (\"def p(g):n=sum(r.count({c})for r in g);return[[n]]\", 48),\n            \n            # Complex operations\n            'outline': (\"def p(g):h,w=len(g),len(g[0]);return[[g[i][j]if g[i][j]and any(i+di<0or i+di>=h or j+dj<0or j+dj>=w or not g[i+di][j+dj]for di,dj in[(0,1),(0,-1),(1,0),(-1,0)])else 0for j in range(w)]for i in range(h)]\", 200),\n            'fill_line': (\"def p(g):return[[max(r)]*len(r)for r in g]\", 41),\n            'propagate': (\"def p(g):c=[max(set(r),key=r.count)for r in g];return[[c[i]]*len(r)for i,r in enumerate(g)]\", 93),\n            \n            # Ultra-compact patterns\n            'id': (\"def p(g):return g\", 18),\n            'const': (\"def p(g):return[[{v}]]\", 23),\n            'size': (\"def p(g):return[[len(g)]]\", 26),\n        }\n        \n        # Pattern detection hierarchy\n        self.pattern_hierarchy = {\n            'level1': ['id', 'rot90', 'rot180', 'rot270', 'fliph', 'flipv', 'trans'],\n            'level2': ['scale2x', 'scale3x', 'half', 'fill', 'crop'],\n            'level3': ['swap', 'map', 'checker', 'border', 'extract'],\n            'level4': ['outline', 'fill_line', 'propagate', 'count']\n        }\n        \n        # Shared learning repository\n        self.pattern_cache = {}\n        self.feature_cache = {}\n        self.success_patterns = defaultdict(list)\n        \n        # Advanced feature extractors\n        self.feature_extractors = [\n            self.extract_geometric_means,\n            self.extract_weighted_centroids,\n            self.extract_color_gradients,\n            self.extract_segmented_features,\n            self.extract_relative_segment_vectors,\n            self.extract_hash_features,\n            self.extract_compression_features,\n            self.extract_fourier_descriptors,\n            self.extract_wavelet_features,\n            self.extract_texture_features,\n            self.extract_shape_contexts,\n            self.extract_persistent_homology,\n            self.extract_graph_features,\n            self.extract_semantic_features,\n            self.extract_invariant_moments\n        ]\n        \n    def extract_geometric_means(self, grid):\n        \"\"\"Extract geometric mean features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Avoid log of zero\n        grid_safe = np.where(grid > 0, grid, 1)\n        \n        # Global geometric mean\n        geom_mean = np.exp(np.mean(np.log(grid_safe)))\n        features.append(geom_mean)\n        \n        # Row-wise geometric means\n        for i in range(min(3, grid.shape[0])):\n            row = grid_safe[i]\n            features.append(np.exp(np.mean(np.log(row))))\n            \n        # Column-wise geometric means\n        for j in range(min(3, grid.shape[1])):\n            col = grid_safe[:, j]\n            features.append(np.exp(np.mean(np.log(col))))\n            \n        # Quadrant geometric means\n        h, w = grid.shape\n        quadrants = [\n            grid_safe[:h//2, :w//2],\n            grid_safe[:h//2, w//2:],\n            grid_safe[h//2:, :w//2],\n            grid_safe[h//2:, w//2:]\n        ]\n        \n        for q in quadrants:\n            if q.size > 0:\n                features.append(np.exp(np.mean(np.log(q))))\n            else:\n                features.append(0)\n                \n        return np.array(features[:15])\n    \n    def extract_weighted_centroids(self, grid):\n        \"\"\"Extract centroids weighted by color values\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Weight by color value\n        total_weight = np.sum(grid)\n        if total_weight > 0:\n            y_coords, x_coords = np.meshgrid(range(grid.shape[0]), range(grid.shape[1]), indexing='ij')\n            weighted_y = np.sum(y_coords * grid) / total_weight\n            weighted_x = np.sum(x_coords * grid) / total_weight\n            features.extend([weighted_y, weighted_x])\n        else:\n            features.extend([0, 0])\n            \n        # Per-color weighted centroids\n        for color in range(1, 10):\n            mask = (grid == color)\n            if mask.sum() > 0:\n                y, x = np.where(mask)\n                # Weight by distance from center\n                cy, cx = grid.shape[0]/2, grid.shape[1]/2\n                distances = np.sqrt((y - cy)**2 + (x - cx)**2)\n                weights = 1 / (distances + 1)\n                wy = np.average(y, weights=weights)\n                wx = np.average(x, weights=weights)\n                features.extend([wy, wx])\n            else:\n                features.extend([0, 0])\n                \n        return np.array(features[:20])\n    \n    def extract_color_gradients(self, grid):\n        \"\"\"Extract color gradient features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Sobel gradients\n        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n        \n        grad_x = correlate2d(grid, sobel_x, mode='same', boundary='wrap')\n        grad_y = correlate2d(grid, sobel_y, mode='same', boundary='wrap')\n        \n        grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n        grad_dir = np.arctan2(grad_y, grad_x)\n        \n        features.extend([\n            grad_mag.mean(),\n            grad_mag.std(),\n            grad_mag.max(),\n            np.mean(np.abs(grad_dir)),\n            np.std(grad_dir)\n        ])\n        \n        # Color transition matrix\n        h, w = grid.shape\n        transitions = np.zeros((10, 10))\n        \n        for i in range(h):\n            for j in range(w-1):\n                c1, c2 = int(grid[i,j]), int(grid[i,j+1])\n                transitions[c1, c2] += 1\n                \n        for i in range(h-1):\n            for j in range(w):\n                c1, c2 = int(grid[i,j]), int(grid[i+1,j])\n                transitions[c1, c2] += 1\n                \n        # Transition features\n        features.extend([\n            entropy(transitions.flatten() + 1e-10),\n            np.sum(np.diag(transitions)) / (transitions.sum() + 1e-10),  # Self-transition ratio\n            np.count_nonzero(transitions)  # Number of unique transitions\n        ])\n        \n        return np.array(features[:10])\n    \n    def extract_segmented_features(self, grid):\n        \"\"\"Extract features from grid segments\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Segment by connected components\n        labeled, num = label(grid > 0)\n        \n        segment_features = []\n        for i in range(1, min(num + 1, 6)):\n            mask = labeled == i\n            if mask.sum() > 0:\n                # Segment properties\n                y, x = np.where(mask)\n                seg_feat = [\n                    y.mean(), x.mean(),  # Centroid\n                    y.std(), x.std(),    # Spread\n                    mask.sum(),          # Size\n                    grid[mask][0],       # Color\n                    y.max() - y.min(),  # Height\n                    x.max() - x.min()   # Width\n                ]\n                segment_features.append(seg_feat)\n                \n        # Aggregate segment features\n        if segment_features:\n            segment_features = np.array(segment_features)\n            features.extend([\n                segment_features.mean(axis=0),\n                segment_features.std(axis=0),\n                segment_features.max(axis=0) - segment_features.min(axis=0)\n            ].flatten()[:30])\n        else:\n            features.extend([0] * 30)\n            \n        # Grid-based segmentation (divide into regions)\n        h, w = grid.shape\n        regions = []\n        for i in range(2):\n            for j in range(2):\n                region = grid[i*h//2:(i+1)*h//2, j*w//2:(j+1)*w//2]\n                if region.size > 0:\n                    regions.append([\n                        region.mean(),\n                        region.std(),\n                        mode(region.flatten())[0][0],\n                        entropy(np.bincount(region.flatten()) + 1e-10)\n                    ])\n                    \n        if regions:\n            regions = np.array(regions)\n            features.extend(regions.flatten()[:16])\n        else:\n            features.extend([0] * 16)\n            \n        return np.array(features[:50])\n    \n    def extract_relative_segment_vectors(self, grid):\n        \"\"\"Extract relative vectors between segments\"\"\"\n        grid = np.array(grid)\n        labeled, num = label(grid > 0)\n        features = []\n        \n        # Get segment centroids and properties\n        segments = []\n        for i in range(1, min(num + 1, 8)):\n            mask = labeled == i\n            if mask.sum() > 0:\n                y, x = np.where(mask)\n                segments.append({\n                    'id': i,\n                    'centroid': (y.mean(), x.mean()),\n                    'size': mask.sum(),\n                    'color': grid[mask][0],\n                    'bbox': (y.min(), y.max(), x.min(), x.max()),\n                    'aspect': (x.max() - x.min() + 1) / (y.max() - y.min() + 1)\n                })\n                \n        # Pairwise relative features\n        if len(segments) >= 2:\n            vectors = []\n            for i, s1 in enumerate(segments):\n                for j, s2 in enumerate(segments):\n                    if i < j:\n                        # Relative position\n                        dy = s2['centroid'][0] - s1['centroid'][0]\n                        dx = s2['centroid'][1] - s1['centroid'][1]\n                        dist = np.sqrt(dy**2 + dx**2)\n                        angle = np.arctan2(dy, dx)\n                        \n                        # Relative properties\n                        size_ratio = s2['size'] / (s1['size'] + 1e-10)\n                        aspect_ratio = s2['aspect'] / (s1['aspect'] + 1e-10)\n                        color_diff = abs(s2['color'] - s1['color'])\n                        \n                        # Spatial relationship\n                        overlap_y = max(0, min(s1['bbox'][1], s2['bbox'][1]) - \n                                       max(s1['bbox'][0], s2['bbox'][0]))\n                        overlap_x = max(0, min(s1['bbox'][3], s2['bbox'][3]) - \n                                       max(s1['bbox'][2], s2['bbox'][2]))\n                        \n                        vectors.append([\n                            dy, dx, dist, angle,\n                            size_ratio, aspect_ratio, color_diff,\n                            overlap_y > 0, overlap_x > 0\n                        ])\n                        \n            if vectors:\n                vectors = np.array(vectors)\n                features.extend([\n                    vectors.mean(axis=0),\n                    vectors.std(axis=0),\n                    vectors.max(axis=0),\n                    vectors.min(axis=0)\n                ].flatten()[:40])\n            else:\n                features.extend([0] * 40)\n        else:\n            features.extend([0] * 40)\n            \n        return np.array(features[:40])\n    \n    def extract_hash_features(self, grid):\n        \"\"\"Extract hash-based features for pattern matching\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Grid hash\n        grid_bytes = grid.astype(np.uint8).tobytes()\n        grid_hash = int(hashlib.md5(grid_bytes).hexdigest()[:8], 16)\n        features.append(grid_hash / 1e10)  # Normalize\n        \n        # Row hashes\n        for i in range(min(3, grid.shape[0])):\n            row_hash = int(hashlib.md5(grid[i].tobytes()).hexdigest()[:8], 16)\n            features.append(row_hash / 1e10)\n            \n        # Column hashes\n        for j in range(min(3, grid.shape[1])):\n            col_hash = int(hashlib.md5(grid[:, j].tobytes()).hexdigest()[:8], 16)\n            features.append(col_hash / 1e10)\n            \n        # Transformation hashes\n        transforms = [\n            np.rot90(grid),\n            np.fliplr(grid),\n            np.flipud(grid)\n        ]\n        \n        for trans in transforms:\n            trans_hash = int(hashlib.md5(trans.tobytes()).hexdigest()[:8], 16)\n            features.append(trans_hash / 1e10)\n            \n        # Pattern hashes (2x2, 3x3 blocks)\n        pattern_hashes = []\n        for size in [2, 3]:\n            for i in range(min(2, grid.shape[0] - size + 1)):\n                for j in range(min(2, grid.shape[1] - size + 1)):\n                    block = grid[i:i+size, j:j+size]\n                    block_hash = int(hashlib.md5(block.tobytes()).hexdigest()[:8], 16)\n                    pattern_hashes.append(block_hash / 1e10)\n                    \n        features.extend(pattern_hashes[:5])\n        \n        return np.array(features[:15])\n    \n    def extract_compression_features(self, grid):\n        \"\"\"Extract compression-based complexity features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Run-length encoding compression ratio\n        flat = grid.flatten()\n        runs = []\n        if len(flat) > 0:\n            current = flat[0]\n            count = 1\n            for val in flat[1:]:\n                if val == current:\n                    count += 1\n                else:\n                    runs.append((current, count))\n                    current = val\n                    count = 1\n            runs.append((current, count))\n            \n        compression_ratio = len(runs) / (len(flat) + 1e-10)\n        features.append(compression_ratio)\n        \n        # Entropy-based complexity\n        counts = np.bincount(flat)\n        probs = counts[counts > 0] / len(flat)\n        entropy_val = -np.sum(probs * np.log2(probs + 1e-10))\n        features.append(entropy_val)\n        \n        # Kolmogorov complexity approximation (using zlib)\n        import zlib\n        grid_bytes = grid.astype(np.uint8).tobytes()\n        compressed_size = len(zlib.compress(grid_bytes))\n        original_size = len(grid_bytes)\n        complexity = compressed_size / (original_size + 1e-10)\n        features.append(complexity)\n        \n        # Pattern repetition score\n        h, w = grid.shape\n        repetition_score = 0\n        \n        # Check horizontal repetition\n        for period in range(1, w//2 + 1):\n            if np.all(grid[:, :w-period] == grid[:, period:]):\n                repetition_score += 1 / period\n                break\n                \n        # Check vertical repetition\n        for period in range(1, h//2 + 1):\n            if np.all(grid[:h-period, :] == grid[period:, :]):\n                repetition_score += 1 / period\n                break\n                \n        features.append(repetition_score)\n        \n        # Information content\n        unique_values = len(np.unique(grid))\n        info_content = unique_values / 10  # Normalized by max colors\n        features.append(info_content)\n        \n        return np.array(features[:10])\n    \n    def extract_fourier_descriptors(self, grid):\n        \"\"\"Extract Fourier descriptors for shape analysis\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # 2D FFT\n        fft = np.fft.fft2(grid)\n        fft_mag = np.abs(fft)\n        fft_phase = np.angle(fft)\n        \n        # Low-frequency descriptors (shape)\n        h, w = grid.shape\n        low_freq = fft_mag[:min(4, h), :min(4, w)].flatten()\n        features.extend(low_freq[:10])\n        \n        # Radial distribution\n        center = (h//2, w//2)\n        y, x = np.ogrid[:h, :w]\n        r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n        \n        radial_bins = np.linspace(0, np.sqrt(h**2 + w**2)/2, 5)\n        radial_profile = []\n        \n        for i in range(len(radial_bins)-1):\n            mask = (r >= radial_bins[i]) & (r < radial_bins[i+1])\n            radial_profile.append(fft_mag[mask].mean() if mask.sum() > 0 else 0)\n            \n        features.extend(radial_profile)\n        \n        # Angular distribution\n        theta = np.arctan2(y - center[0], x - center[1])\n        angular_bins = np.linspace(-np.pi, np.pi, 9)\n        angular_profile = []\n        \n        for i in range(len(angular_bins)-1):\n            mask = (theta >= angular_bins[i]) & (theta < angular_bins[i+1])\n            angular_profile.append(fft_mag[mask].mean() if mask.sum() > 0 else 0)\n            \n        features.extend(angular_profile[:5])\n        \n        return np.array(features[:20])\n    \n    def extract_wavelet_features(self, grid):\n        \"\"\"Extract wavelet-based features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Simple Haar wavelet decomposition\n        h, w = grid.shape\n        \n        # Level 1 decomposition\n        if h >= 2 and w >= 2:\n            # Approximate\n            cA = (grid[::2, ::2] + grid[1::2, ::2] + \n                  grid[::2, 1::2] + grid[1::2, 1::2]) / 4\n            \n            # Horizontal detail\n            cH = (grid[::2, ::2] - grid[1::2, ::2] + \n                  grid[::2, 1::2] - grid[1::2, 1::2]) / 4\n            \n            # Vertical detail\n            cV = (grid[::2, ::2] + grid[1::2, ::2] - \n                  grid[::2, 1::2] - grid[1::2, 1::2]) / 4\n            \n            # Diagonal detail\n            cD = (grid[::2, ::2] - grid[1::2, ::2] - \n                  grid[::2, 1::2] + grid[1::2, 1::2]) / 4\n            \n            features.extend([\n                cA.mean(), cA.std(),\n                np.abs(cH).mean(), np.abs(cH).std(),\n                np.abs(cV).mean(), np.abs(cV).std(),\n                np.abs(cD).mean(), np.abs(cD).std()\n            ])\n        else:\n            features.extend([0] * 8)\n            \n        # Energy distribution\n        if h >= 2 and w >= 2:\n            total_energy = np.sum(grid**2)\n            if total_energy > 0:\n                features.extend([\n                    np.sum(cA**2) / total_energy,\n                    np.sum(cH**2) / total_energy,\n                    np.sum(cV**2) / total_energy,\n                    np.sum(cD**2) / total_energy\n                ])\n            else:\n                features.extend([0] * 4)\n        else:\n            features.extend([0] * 4)\n            \n        return np.array(features[:15])\n    \n    def extract_texture_features(self, grid):\n        \"\"\"Extract texture features using GLCM-like approach\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Co-occurrence matrix\n        levels = 10\n        glcm = np.zeros((levels, levels))\n        \n        # Horizontal pairs\n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]-1):\n                glcm[grid[i,j], grid[i,j+1]] += 1\n                \n        # Vertical pairs\n        for i in range(grid.shape[0]-1):\n            for j in range(grid.shape[1]):\n                glcm[grid[i,j], grid[i+1,j]] += 1\n                \n        # Normalize\n        if glcm.sum() > 0:\n            glcm = glcm / glcm.sum()\n            \n        # Texture features from GLCM\n        # Contrast\n        contrast = 0\n        for i in range(levels):\n            for j in range(levels):\n                contrast += (i-j)**2 * glcm[i,j]\n        features.append(contrast)\n        \n        # Homogeneity\n        homogeneity = 0\n        for i in range(levels):\n            for j in range(levels):\n                homogeneity += glcm[i,j] / (1 + abs(i-j))\n        features.append(homogeneity)\n        \n        # Energy\n        energy = np.sum(glcm**2)\n        features.append(energy)\n        \n        # Correlation\n        if glcm.sum() > 0:\n            mu_i = np.sum(np.arange(levels)[:, None] * glcm)\n            mu_j = np.sum(np.arange(levels)[None, :] * glcm)\n            sigma_i = np.sqrt(np.sum((np.arange(levels)[:, None] - mu_i)**2 * glcm))\n            sigma_j = np.sqrt(np.sum((np.arange(levels)[None, :] - mu_j)**2 * glcm))\n            \n            if sigma_i > 0 and sigma_j > 0:\n                correlation = np.sum((np.arange(levels)[:, None] - mu_i) * \n                                   (np.arange(levels)[None, :] - mu_j) * glcm) / (sigma_i * sigma_j)\n            else:\n                correlation = 0\n        else:\n            correlation = 0\n        features.append(correlation)\n        \n        # Local Binary Patterns (simplified)\n        lbp_hist = np.zeros(8)\n        for i in range(1, grid.shape[0]-1):\n            for j in range(1, grid.shape[1]-1):\n                center = grid[i,j]\n                pattern = 0\n                neighbors = [\n                    grid[i-1,j-1], grid[i-1,j], grid[i-1,j+1],\n                    grid[i,j+1], grid[i+1,j+1], grid[i+1,j],\n                    grid[i+1,j-1], grid[i,j-1]\n                ]\n                for k, neighbor in enumerate(neighbors):\n                    if neighbor > center:\n                        pattern |= (1 << k)\n                lbp_hist[pattern % 8] += 1\n                \n        if lbp_hist.sum() > 0:\n            lbp_hist = lbp_hist / lbp_hist.sum()\n        features.extend(lbp_hist[:5])\n        \n        return np.array(features[:10])\n    \n    def extract_shape_contexts(self, grid):\n        \"\"\"Extract shape context features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Find object boundaries\n        binary = grid > 0\n        if not binary.any():\n            return np.zeros(20)\n            \n        # Edge detection\n        edges = np.zeros_like(binary)\n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]):\n                if binary[i,j]:\n                    for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                        ni, nj = i + di, j + dj\n                        if (0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1] and \n                            not binary[ni,nj]) or \\\n                           (ni < 0 or ni >= grid.shape[0] or nj < 0 or nj >= grid.shape[1]):\n                            edges[i,j] = True\n                            break\n                            \n        # Sample points on boundary\n        edge_points = np.column_stack(np.where(edges))\n        if len(edge_points) == 0:\n            return np.zeros(20)\n            \n        # Sample up to 10 points\n        if len(edge_points) > 10:\n            indices = np.random.choice(len(edge_points), 10, replace=False)\n            sample_points = edge_points[indices]\n        else:\n            sample_points = edge_points\n            \n        # Compute shape contexts\n        shape_contexts = []\n        for point in sample_points:\n            # Compute histogram of relative positions\n            rel_positions = edge_points - point\n            distances = np.sqrt(np.sum(rel_positions**2, axis=1))\n            angles = np.arctan2(rel_positions[:, 0], rel_positions[:, 1])\n            \n            # Bin into histogram\n            dist_bins = np.logspace(0, np.log10(np.max(distances) + 1), 4)\n            angle_bins = np.linspace(-np.pi, np.pi, 9)\n            \n            hist = np.zeros((3, 8))\n            for d, a in zip(distances, angles):\n                if d > 0:  # Skip self\n                    d_bin = np.searchsorted(dist_bins, d) - 1\n                    a_bin = np.searchsorted(angle_bins, a) - 1\n                    if 0 <= d_bin < 3 and 0 <= a_bin < 8:\n                        hist[d_bin, a_bin] += 1\n                        \n            shape_contexts.append(hist.flatten())\n            \n        # Average shape contexts\n        if shape_contexts:\n            avg_context = np.mean(shape_contexts, axis=0)\n            features.extend(avg_context[:20])\n        else:\n            features.extend([0] * 20)\n            \n        return np.array(features[:20])\n    \n    def extract_persistent_homology(self, grid):\n        \"\"\"Extract topological features using persistent homology concepts\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Filtration by color values\n        max_val = grid.max()\n        \n        # Compute connected components at different thresholds\n        birth_death = []\n        for threshold in range(1, int(max_val) + 1):\n            binary = grid >= threshold\n            labeled, num = label(binary)\n            birth_death.append((threshold, num))\n            \n        # Persistence features\n        if birth_death:\n            births = [bd[0] for bd in birth_death]\n            deaths = [bd[1] for bd in birth_death]\n            \n            features.extend([\n                np.mean(births),\n                np.std(births),\n                np.mean(deaths),\n                np.std(deaths),\n                len(birth_death)\n            ])\n            \n            # Persistence diagram features\n            persistences = []\n            for i in range(len(birth_death)-1):\n                if deaths[i] > deaths[i+1]:\n                    persistences.append(births[i+1] - births[i])\n                    \n            if persistences:\n                features.extend([\n                    np.mean(persistences),\n                    np.max(persistences),\n                    len(persistences)\n                ])\n            else:\n                features.extend([0, 0, 0])\n        else:\n            features.extend([0] * 8)\n            \n        # Betti numbers at different scales\n        for scale in [1, 2, 3]:\n            binary = grid >= scale\n            labeled, num_components = label(binary)\n            \n            # Simplified hole detection\n            holes = 0\n            for i in range(1, grid.shape[0]-1):\n                for j in range(1, grid.shape[1]-1):\n                    if not binary[i,j]:\n                        # Check if surrounded\n                        if all(binary[i+di,j+dj] for di,dj in \n                              [(-1,0), (1,0), (0,-1), (0,1)]):\n                            holes += 1\n                            \n            features.extend([num_components, holes])\n            \n        return np.array(features[:20])\n    \n    def extract_graph_features(self, grid):\n        \"\"\"Extract graph-based features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Build adjacency graph\n        h, w = grid.shape\n        nodes = []\n        node_map = {}\n        \n        # Create nodes for each non-zero cell\n        for i in range(h):\n            for j in range(w):\n                if grid[i,j] > 0:\n                    node_id = len(nodes)\n                    nodes.append((i, j, grid[i,j]))\n                    node_map[(i,j)] = node_id\n                    \n        # Build edges\n        edges = []\n        for idx, (i, j, color) in enumerate(nodes):\n            for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                ni, nj = i + di, j + dj\n                if (ni, nj) in node_map:\n                    neighbor_idx = node_map[(ni, nj)]\n                    if nodes[neighbor_idx][2] == color:  # Same color\n                        edges.append((idx, neighbor_idx))\n                        \n        # Graph features\n        num_nodes = len(nodes)\n        num_edges = len(edges) // 2  # Undirected\n        \n        features.extend([num_nodes, num_edges])\n        \n        # Degree distribution\n        degrees = np.zeros(num_nodes)\n        for u, v in edges:\n            degrees[u] += 1\n            \n        if num_nodes > 0:\n            features.extend([\n                degrees.mean(),\n                degrees.std(),\n                degrees.max(),\n                np.sum(degrees == 0) / num_nodes  # Isolated nodes ratio\n            ])\n        else:\n            features.extend([0] * 4)\n            \n        # Connected components (using simple DFS)\n        if num_nodes > 0:\n            visited = [False] * num_nodes\n            components = 0\n            \n            def dfs(node):\n                visited[node] = True\n                for u, v in edges:\n                    if u == node and not visited[v]:\n                        dfs(v)\n                    elif v == node and not visited[u]:\n                        dfs(u)\n                        \n            for i in range(num_nodes):\n                if not visited[i]:\n                    dfs(i)\n                    components += 1\n                    \n            features.append(components)\n        else:\n            features.append(0)\n            \n        # Clustering coefficient (simplified)\n        triangles = 0\n        for i in range(num_nodes):\n            neighbors = [v for u, v in edges if u == i] + [u for u, v in edges if v == i]\n            neighbors = list(set(neighbors))\n            \n            # Count triangles\n            for j in range(len(neighbors)):\n                for k in range(j+1, len(neighbors)):\n                    if (neighbors[j], neighbors[k]) in edges or (neighbors[k], neighbors[j]) in edges:\n                        triangles += 1\n                        \n        if num_edges > 0:\n            clustering = triangles / (num_edges + 1e-10)\n        else:\n            clustering = 0\n        features.append(clustering)\n        \n        return np.array(features[:10])\n    \n    def extract_semantic_features(self, grid):\n        \"\"\"Extract high-level semantic features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Object count by color\n        for color in range(1, 10):\n            mask = grid == color\n            if mask.any():\n                labeled, num = label(mask)\n                features.append(num)\n            else:\n                features.append(0)\n                \n        # Symmetry detection\n        h_symmetry = np.mean(grid == np.fliplr(grid))\n        v_symmetry = np.mean(grid == np.flipud(grid))\n        \n        if grid.shape[0] == grid.shape[1]:\n            d_symmetry = np.mean(grid == grid.T)\n            r_symmetry = np.mean(grid == np.rot90(grid, 2))\n        else:\n            d_symmetry = r_symmetry = 0\n            \n        features.extend([h_symmetry, v_symmetry, d_symmetry, r_symmetry])\n        \n        # Pattern detection scores\n        # Checkerboard\n        h, w = grid.shape\n        checker = np.array([[(i+j)%2 for j in range(w)] for i in range(h)])\n        colors = np.unique(grid)\n        \n        checker_score = 0\n        if len(colors) == 2:\n            c1, c2 = colors\n            score1 = np.mean((grid == c1) == (checker == 0))\n            score2 = np.mean((grid == c1) == (checker == 1))\n            checker_score = max(score1, score2)\n        features.append(checker_score)\n        \n        # Stripe detection\n        h_stripe_score = 0\n        for period in range(1, min(4, h)):\n            stripe = np.array([[i//period % 2 for j in range(w)] for i in range(h)])\n            if len(colors) == 2:\n                score = max(np.mean((grid == colors[0]) == (stripe == 0)),\n                           np.mean((grid == colors[0]) == (stripe == 1)))\n                h_stripe_score = max(h_stripe_score, score)\n        features.append(h_stripe_score)\n        \n        # Border detection\n        border_mask = np.zeros_like(grid, dtype=bool)\n        border_mask[0,:] = border_mask[-1,:] = True\n        border_mask[:,0] = border_mask[:,-1] = True\n        \n        border_consistency = 0\n        for color in colors:\n            if np.all(grid[border_mask] == color):\n                border_consistency = 1\n                break\n        features.append(border_consistency)\n        \n        return np.array(features[:20])\n    \n    def extract_invariant_moments(self, grid):\n        \"\"\"Extract Hu moments and other invariant features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Spatial moments\n        m00 = np.sum(grid)\n        if m00 == 0:\n            return np.zeros(15)\n            \n        y, x = np.mgrid[:grid.shape[0], :grid.shape[1]]\n        m10 = np.sum(x * grid)\n        m01 = np.sum(y * grid)\n        \n        # Centroid\n        cx = m10 / m00\n        cy = m01 / m00\n        \n        # Central moments\n        mu20 = np.sum((x - cx)**2 * grid) / m00\n        mu02 = np.sum((y - cy)**2 * grid) / m00\n        mu11 = np.sum((x - cx) * (y - cy) * grid) / m00\n        mu30 = np.sum((x - cx)**3 * grid) / m00\n        mu03 = np.sum((y - cy)**3 * grid) / m00\n        mu21 = np.sum((x - cx)**2 * (y - cy) * grid) / m00\n        mu12 = np.sum((x - cx) * (y - cy)**2 * grid) / m00\n        \n        # Normalized moments\n        norm = m00 ** (2/2 + 1)\n        nu20 = mu20 / norm\n        nu02 = mu02 / norm\n        nu11 = mu11 / norm\n        \n        norm3 = m00 ** (3/2 + 1)\n        nu30 = mu30 / norm3\n        nu03 = mu03 / norm3\n        nu21 = mu21 / norm3\n        nu12 = mu12 / norm3\n        \n        # Hu moments (first 7)\n        h1 = nu20 + nu02\n        h2 = (nu20 - nu02)**2 + 4*nu11**2\n        h3 = (nu30 - 3*nu12)**2 + (3*nu21 - nu03)**2\n        h4 = (nu30 + nu12)**2 + (nu21 + nu03)**2\n        h5 = (nu30 - 3*nu12)*(nu30 + nu12)*((nu30 + nu12)**2 - 3*(nu21 + nu03)**2) + \\\n             (3*nu21 - nu03)*(nu21 + nu03)*(3*(nu30 + nu12)**2 - (nu21 + nu03)**2)\n        h6 = (nu20 - nu02)*((nu30 + nu12)**2 - (nu21 + nu03)**2) + \\\n             4*nu11*(nu30 + nu12)*(nu21 + nu03)\n        h7 = (3*nu21 - nu03)*(nu30 + nu12)*((nu30 + nu12)**2 - 3*(nu21 + nu03)**2) - \\\n             (nu30 - 3*nu12)*(nu21 + nu03)*(3*(nu30 + nu12)**2 - (nu21 + nu03)**2)\n        \n        # Log transform for scale invariance\n        hu_moments = [h1, h2, h3, h4, h5, h6, h7]\n        for i, h in enumerate(hu_moments):\n            if h != 0:\n                hu_moments[i] = -np.sign(h) * np.log10(abs(h) + 1e-10)\n            else:\n                hu_moments[i] = 0\n                \n        features.extend(hu_moments)\n        \n        # Additional invariants\n        features.extend([\n            mu20 + mu02,  # Trace\n            mu20 * mu02 - mu11**2,  # Determinant\n            np.sqrt(mu20**2 + mu02**2 + 2*mu11**2)  # Frobenius norm\n        ])\n        \n        return np.array(features[:15])\n    \n    @lru_cache(maxsize=1000)\n    def detect_pattern_cached(self, grid_hash, grid_shape):\n        \"\"\"Cached pattern detection\"\"\"\n        return self.pattern_cache.get(grid_hash, None)\n    \n    def analyze_transformation(self, examples):\n        \"\"\"Comprehensive transformation analysis\"\"\"\n        transformations = []\n        \n        for ex in examples:\n            inp = np.array(ex['input'])\n            out = np.array(ex['output'])\n            \n            # Extract comprehensive features\n            inp_features = self.extract_all_features(inp)\n            out_features = self.extract_all_features(out)\n            \n            # Detect transformation type\n            trans_type = self.detect_transformation_type(inp, out)\n            \n            transformations.append({\n                'input': inp,\n                'output': out,\n                'inp_features': inp_features,\n                'out_features': out_features,\n                'type': trans_type,\n                'params': self.extract_transformation_params(inp, out, trans_type)\n            })\n            \n        return transformations\n    \n    def detect_transformation_type(self, inp, out):\n        \"\"\"Detect the type of transformation\"\"\"\n        # Size change\n        if inp.shape != out.shape:\n            if out.shape[0] > inp.shape[0] or out.shape[1] > inp.shape[1]:\n                return 'scale_up'\n            else:\n                return 'scale_down'\n                \n        # Check for simple transformations\n        if np.array_equal(out, np.rot90(inp, 1)):\n            return 'rotate_90'\n        elif np.array_equal(out, np.rot90(inp, 2)):\n            return 'rotate_180'\n        elif np.array_equal(out, np.rot90(inp, 3)):\n            return 'rotate_270'\n        elif np.array_equal(out, np.fliplr(inp)):\n            return 'flip_h'\n        elif np.array_equal(out, np.flipud(inp)):\n            return 'flip_v'\n        elif np.array_equal(out, inp.T):\n            return 'transpose'\n            \n        # Color operations\n        if len(np.unique(out)) == 1:\n            return 'fill_single'\n        elif self.is_color_mapping(inp, out):\n            return 'color_map'\n            \n        # Pattern operations\n        if self.is_pattern_fill(out):\n            return 'pattern_fill'\n            \n        return 'complex'\n    \n    def is_color_mapping(self, inp, out):\n        \"\"\"Check if transformation is a color mapping\"\"\"\n        if inp.shape != out.shape:\n            return False\n            \n        mapping = {}\n        for i in range(inp.shape[0]):\n            for j in range(inp.shape[1]):\n                if inp[i,j] in mapping:\n                    if mapping[inp[i,j]] != out[i,j]:\n                        return False\n                else:\n                    mapping[inp[i,j]] = out[i,j]\n                    \n        return len(mapping) > 0\n    \n    def is_pattern_fill(self, grid):\n        \"\"\"Check if grid is a pattern fill\"\"\"\n        # Check for checkerboard\n        h, w = grid.shape\n        checker = np.array([[(i+j)%2 for j in range(w)] for i in range(h)])\n        colors = np.unique(grid)\n        \n        if len(colors) == 2:\n            c1, c2 = colors\n            if np.array_equal(grid, np.where(checker, c1, c2)) or \\\n               np.array_equal(grid, np.where(checker, c2, c1)):\n                return True\n                \n        return False\n    \n    def extract_transformation_params(self, inp, out, trans_type):\n        \"\"\"Extract parameters for transformation\"\"\"\n        params = {}\n        \n        if trans_type == 'scale_up':\n            params['sy'] = out.shape[0] // inp.shape[0]\n            params['sx'] = out.shape[1] // inp.shape[1]\n        elif trans_type == 'scale_down':\n            params['sy'] = inp.shape[0] // out.shape[0]\n            params['sx'] = inp.shape[1] // out.shape[1]\n        elif trans_type == 'fill_single':\n            params['color'] = int(out[0,0])\n        elif trans_type == 'color_map':\n            mapping = {}\n            for i in range(inp.shape[0]):\n                for j in range(inp.shape[1]):\n                    mapping[int(inp[i,j])] = int(out[i,j])\n            params['mapping'] = mapping\n            \n        return params\n    \n    def extract_all_features(self, grid):\n        \"\"\"Extract all features from grid\"\"\"\n        features = []\n        \n        for extractor in self.feature_extractors:\n            try:\n                feat = extractor(grid)\n                features.extend(feat)\n            except Exception as e:\n                # Add zeros if extraction fails\n                features.extend([0] * 10)\n                \n        return np.array(features[:500])\n    \n    def generate_optimized_code(self, task_data):\n        \"\"\"Generate highly optimized code for the task\"\"\"\n        examples = task_data['train']\n        \n        # Quick pattern matching first\n        for pattern_name, (code_template, _) in self.verified_patterns.items():\n            if self.matches_pattern(examples, pattern_name):\n                return self.instantiate_pattern(code_template, examples)\n                \n        # Analyze transformations\n        transformations = self.analyze_transformation(examples)\n        \n        # Ensemble approach - try multiple strategies\n        candidates = []\n        \n        # Strategy 1: Direct pattern detection\n        pattern_code = self.generate_pattern_based_code(transformations)\n        if pattern_code:\n            candidates.append(pattern_code)\n            \n        # Strategy 2: Feature-based generation\n        feature_code = self.generate_feature_based_code(transformations)\n        if feature_code:\n            candidates.append(feature_code)\n            \n        # Strategy 3: Template matching\n        template_code = self.generate_template_based_code(transformations)\n        if template_code:\n            candidates.append(template_code)\n            \n        # Select best candidate\n        if candidates:\n            # Return shortest valid code\n            return min(candidates, key=len)\n            \n        # Fallback\n        return \"def p(g):return g\"\n    \n    def matches_pattern(self, examples, pattern_name):\n        \"\"\"Check if examples match a known pattern\"\"\"\n        if pattern_name == 'rot90':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'rot180':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']), 2)) \n                      for ex in examples)\n        elif pattern_name == 'rot270':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']), 3)) \n                      for ex in examples)\n        elif pattern_name == 'fliph':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.fliplr(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'flipv':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.flipud(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'trans':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.array(ex['input']).T) \n                      for ex in examples)\n        elif pattern_name == 'id':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.array(ex['input'])) \n                      for ex in examples)\n            \n        # Check scaling\n        inp = np.array(examples[0]['input'])\n        out = np.array(examples[0]['output'])\n        \n        if pattern_name == 'scale2x':\n            return (out.shape[0] == inp.shape[0] * 2 and \n                   out.shape[1] == inp.shape[1] * 2)\n        elif pattern_name == 'scale3x':\n            return (out.shape[0] == inp.shape[0] * 3 and \n                   out.shape[1] == inp.shape[1] * 3)\n        elif pattern_name == 'half':\n            return (out.shape[0] == inp.shape[0] // 2 and \n                   out.shape[1] == inp.shape[1] // 2)\n            \n        return False\n    \n    def instantiate_pattern(self, template, examples):\n        \"\"\"Instantiate pattern template with specific values\"\"\"\n        # Get parameters from examples\n        inp = np.array(examples[0]['input'])\n        out = np.array(examples[0]['output'])\n        \n        # Replace placeholders\n        code = template\n        \n        # Color placeholders\n        if '{c}' in code:\n            color = int(out[0,0])\n            code = code.replace('{c}', str(color))\n            \n        # Size placeholders\n        if '{h}' in code:\n            code = code.replace('{h}', str(out.shape[0]))\n        if '{w}' in code:\n            code = code.replace('{w}', str(out.shape[1]))\n            \n        # Mapping placeholders\n        if '{m}' in code:\n            mapping = {}\n            for i in range(min(inp.shape[0], out.shape[0])):\n                for j in range(min(inp.shape[1], out.shape[1])):\n                    if inp[i,j] != out[i,j]:\n                        mapping[int(inp[i,j])] = int(out[i,j])\n            code = code.replace('{m}', str(mapping))\n            \n        # Swap placeholders\n        if '{a}' in code and '{b}' in code:\n            # Find swapped colors\n            colors_in = set(inp.flatten())\n            colors_out = set(out.flatten())\n            if len(colors_in) == 2 and colors_in == colors_out:\n                a, b = sorted(colors_in)\n                code = code.replace('{a}', str(a)).replace('{b}', str(b))\n                \n        return code\n    \n    def generate_pattern_based_code(self, transformations):\n        \"\"\"Generate code based on detected patterns\"\"\"\n        trans = transformations[0]\n        trans_type = trans['type']\n        params = trans['params']\n        \n        if trans_type == 'rotate_90':\n            return self.verified_patterns['rot90'][0]\n        elif trans_type == 'rotate_180':\n            return self.verified_patterns['rot180'][0]\n        elif trans_type == 'rotate_270':\n            return self.verified_patterns['rot270'][0]\n        elif trans_type == 'flip_h':\n            return self.verified_patterns['fliph'][0]\n        elif trans_type == 'flip_v':\n            return self.verified_patterns['flipv'][0]\n        elif trans_type == 'transpose':\n            return self.verified_patterns['trans'][0]\n        elif trans_type == 'scale_up':\n            sy, sx = params['sy'], params['sx']\n            return f\"def p(g):return[[g[i//{sy}][j//{sx}]for j in range(len(g[0])*{sx})]for i in range(len(g)*{sy})]\"\n        elif trans_type == 'scale_down':\n            sy, sx = params['sy'], params['sx']\n            return f\"def p(g):return[r[::{sx}]for r in g[::{sy}]]\"\n        elif trans_type == 'fill_single':\n            c = params['color']\n            return f\"def p(g):return[[{c}]*len(g[0])for _ in g]\"\n        elif trans_type == 'color_map':\n            m = params['mapping']\n            return f\"def p(g):m={m};return[[m.get(x,x)for x in r]for r in g]\"\n            \n        return None\n    \n    def generate_feature_based_code(self, transformations):\n        \"\"\"Generate code based on feature analysis\"\"\"\n        # Analyze feature differences\n        trans = transformations[0]\n        inp = trans['input']\n        out = trans['output']\n        \n        # Check for simple operations\n        if inp.shape == out.shape:\n            # Check if output is a function of position\n            is_position_based = True\n            for i in range(out.shape[0]):\n                for j in range(out.shape[1]):\n                    # Check various position-based patterns\n                    if out[i,j] == (i + j) % 10:\n                        continue\n                    elif out[i,j] == i % 10:\n                        continue\n                    elif out[i,j] == j % 10:\n                        continue\n                    else:\n                        is_position_based = False\n                        break\n                        \n            if is_position_based:\n                # Determine exact pattern\n                if all(out[i,j] == (i + j) % 10 for i in range(out.shape[0]) \n                      for j in range(out.shape[1])):\n                    return \"def p(g):return[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n                    \n        return None\n    \n    def generate_template_based_code(self, transformations):\n        \"\"\"Generate code using template matching\"\"\"\n        trans = transformations[0]\n        out = trans['output']\n        \n        # Check if output matches any simple template\n        h, w = out.shape\n        \n        # Single value\n        if len(np.unique(out)) == 1:\n            c = int(out[0,0])\n            return f\"def p(g):return[[{c}]*{w}for _ in range({h})]\"\n            \n        # Row pattern\n        if all(np.array_equal(out[i], out[0]) for i in range(h)):\n            row = out[0].tolist()\n            return f\"def p(g):return[{row}for _ in range({h})]\"\n            \n        # Column pattern\n        if all(np.array_equal(out[:,j], out[:,0]) for j in range(w)):\n            col = out[:,0].tolist()\n            return f\"def p(g):return[[{col[i]}]*{w}for i in range({h})]\"\n            \n        return None\n\ndef create_ultra_advanced_submission():\n    \"\"\"Create submission using ultra-advanced approach\"\"\"\n    solver = UltraAdvancedARCSolver()\n    solutions = {}\n    \n    print(\" Generating Ultra-Advanced Neural ARC Solutions...\")\n    print(\"=\" * 70)\n    \n    successful = 0\n    total_bytes = 0\n    pattern_usage = defaultdict(int)\n    \n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        task_file = f\"/kaggle/input/google-code-golf-2025/task{task_id}.json\"\n        \n        try:\n            with open(task_file) as f:\n                task_data = json.load(f)\n                \n            # Generate optimized code\n            code = solver.generate_optimized_code(task_data)\n            solutions[task_id] = code\n            \n            # Track pattern usage\n            for pattern_name, (pattern_code, _) in solver.verified_patterns.items():\n                if code == pattern_code:\n                    pattern_usage[pattern_name] += 1\n                    break\n                    \n            # Verify solution\n            try:\n                namespace = {}\n                exec(code, namespace)\n                \n                # Test on examples\n                valid = True\n                for ex in task_data['train'][:3]:\n                    p = namespace['p']\n                    result = p([row[:] for row in ex['input']])\n                    if result != ex['output']:\n                        valid = False\n                        break\n                        \n                if valid:\n                    successful += 1\n                    status = \"\"\n                else:\n                    status = \"\"\n            except:\n                status = \"\"\n                \n            bytes_count = len(code)\n            total_bytes += bytes_count\n            \n            if task_num % 25 == 0:\n                print(f\"Progress: {task_num}/400 | Success rate: {successful/task_num:.1%} | \"\n                      f\"Avg bytes: {total_bytes/task_num:.1f}\")\n                \n        except Exception as e:\n            code = \"def p(g):return g\"\n            solutions[task_id] = code\n            total_bytes += len(code)\n            \n    print(f\"\\n{'='*70}\")\n    print(f\" Completed: {successful}/400 valid solutions ({successful/400:.1%})\")\n    print(f\" Total bytes: {total_bytes:,}\")\n    print(f\" Average bytes per solution: {total_bytes/400:.1f}\")\n    \n    print(f\"\\n Pattern Usage Statistics:\")\n    for pattern, count in sorted(pattern_usage.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  {pattern}: {count} times\")\n        \n    # Create submission\n    os.makedirs(\"submission\", exist_ok=True)\n    \n    for task_id, code in solutions.items():\n        with open(f\"submission/task{task_id}.py\", \"w\") as f:\n            f.write(code)\n            \n    with zipfile.ZipFile(\"submission.zip\", \"w\") as zipf:\n        for task_id in solutions:\n            zipf.write(f\"submission/task{task_id}.py\", f\"task{task_id}.py\")\n            \n    print(f\"\\n Ultra-Advanced submission created: submission.zip\")\n    \n    # Show example solutions\n    print(f\"\\n Example solutions:\")\n    for i, (task_id, code) in enumerate(list(solutions.items())[:5]):\n        print(f\"\\nTask {task_id}:\")\n        print(code if len(code) <= 100 else code[:100] + \"...\")\n        \n    return solutions\n\nif __name__ == \"__main__\":\n    create_ultra_advanced_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}