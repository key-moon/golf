{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13245791,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport zipfile\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom scipy.ndimage import convolve, label, distance_transform_edt, binary_erosion, binary_dilation, gaussian_filter, zoom\nfrom scipy.spatial import distance_matrix, ConvexHull\nfrom scipy.stats import mode, entropy, skew, kurtosis\nfrom scipy.signal import find_peaks, correlate2d\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.decomposition import PCA, NMF, FastICA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\nfrom sklearn.neighbors import NearestNeighbors\nimport hashlib\nimport itertools\nfrom functools import lru_cache\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass EnhancedUltraAdvancedARCSolver:\n    \"\"\"\n    Enhanced solver with:\n    - Matrix supersampling for increased resolution\n    - Continuous learning with quantization adaptation\n    - Advanced raster conversion with pattern repetition\n    - Position-based feature engineering\n    - Pseudo-labeling and label balancing\n    \"\"\"\n    \n    def __init__(self):\n        # Extended pattern library with verified solutions\n        self.verified_patterns = {\n            # Basic transformations\n            'rot90': (\"def p(g):return[list(r)for r in zip(*g[::-1])]\", 46),\n            'rot180': (\"def p(g):return[r[::-1]for r in g[::-1]]\", 40),\n            'rot270': (\"def p(g):return[list(r)for r in zip(*g)][::-1]\", 46),\n            'fliph': (\"def p(g):return[r[::-1]for r in g]\", 34),\n            'flipv': (\"def p(g):return g[::-1]\", 23),\n            'trans': (\"def p(g):return[list(r)for r in zip(*g)]\", 40),\n            \n            # Scaling patterns\n            'scale2x': (\"def p(g):return[[g[i//2][j//2]for j in range(len(g[0])*2)]for i in range(len(g)*2)]\", 88),\n            'scale3x': (\"def p(g):return[[g[i//3][j//3]for j in range(len(g[0])*3)]for i in range(len(g)*3)]\", 88),\n            'half': (\"def p(g):return[r[::2]for r in g[::2]]\", 39),\n            \n            # Color operations\n            'fill': (\"def p(g):return[[{c}]*len(g[0])for _ in g]\", 42),\n            'swap': (\"def p(g):return[[{b}if x=={a}else{a}if x=={b}else x for x in r]for r in g]\", 70),\n            'map': (\"def p(g):m={m};return[[m.get(x,x)for x in r]for r in g]\", 54),\n            \n            # Pattern fills\n            'checker': (\"def p(g):return[[{a}if(i+j)%2else{b}for j in range(len(g[0]))]for i in range(len(g))]\", 82),\n            'border': (\"def p(g):h,w=len(g),len(g[0]);return[[{c}if i in[0,h-1]or j in[0,w-1]else g[i][j]for j in range(w)]for i in range(h)]\", 119),\n            \n            # Advanced patterns\n            'crop': (\"def p(g):return[r[{x1}:{x2}]for r in g[{y1}:{y2}]]\", 48),\n            'pad': (\"def p(g):return[[0]*{w}for _ in range({h})]+[r+[0]*({w}-len(r))for r in g]\", 69),\n            'extract': (\"def p(g):return[[x if x=={c}else 0for x in r]for r in g]\", 56),\n            'count': (\"def p(g):n=sum(r.count({c})for r in g);return[[n]]\", 48),\n            \n            # Complex operations\n            'outline': (\"def p(g):h,w=len(g),len(g[0]);return[[g[i][j]if g[i][j]and any(i+di<0or i+di>=h or j+dj<0or j+dj>=w or not g[i+di][j+dj]for di,dj in[(0,1),(0,-1),(1,0),(-1,0)])else 0for j in range(w)]for i in range(h)]\", 200),\n            'fill_line': (\"def p(g):return[[max(r)]*len(r)for r in g]\", 41),\n            'propagate': (\"def p(g):c=[max(set(r),key=r.count)for r in g];return[[c[i]]*len(r)for i,r in enumerate(g)]\", 93),\n            \n            # Ultra-compact patterns\n            'id': (\"def p(g):return g\", 18),\n            'const': (\"def p(g):return[[{v}]]\", 23),\n            'size': (\"def p(g):return[[len(g)]]\", 26),\n        }\n        \n        # Pattern detection hierarchy\n        self.pattern_hierarchy = {\n            'level1': ['id', 'rot90', 'rot180', 'rot270', 'fliph', 'flipv', 'trans'],\n            'level2': ['scale2x', 'scale3x', 'half', 'fill', 'crop'],\n            'level3': ['swap', 'map', 'checker', 'border', 'extract'],\n            'level4': ['outline', 'fill_line', 'propagate', 'count']\n        }\n        \n        # Shared learning repository\n        self.pattern_cache = {}\n        self.feature_cache = {}\n        self.success_patterns = defaultdict(list)\n        \n        # Continuous learning components\n        self.continuous_models = []\n        self.pseudo_label_generator = None\n        self.label_balancer = None\n        self.feature_scaler = None\n        \n        # Supersampling parameters\n        self.supersample_factors = [2, 3, 4]\n        \n        # Advanced feature extractors (including original ones)\n        self.feature_extractors = [\n            # Original extractors\n            self.extract_geometric_means,\n            self.extract_weighted_centroids,\n            self.extract_color_gradients,\n            self.extract_segmented_features,\n            self.extract_relative_segment_vectors,\n            self.extract_hash_features,\n            self.extract_compression_features,\n            self.extract_fourier_descriptors,\n            self.extract_wavelet_features,\n            self.extract_texture_features,\n            self.extract_shape_contexts,\n            self.extract_persistent_homology,\n            self.extract_graph_features,\n            self.extract_semantic_features,\n            self.extract_invariant_moments,\n            # New extractors\n            self.extract_supersampled_features,\n            self.extract_raster_position_features,\n            self.extract_regime_transition_features,\n            self.extract_continuous_adaptation_features\n        ]\n        \n    def supersample_grid(self, grid, factor=2):\n        \"\"\"Supersample the grid to increase resolution\"\"\"\n        grid = np.array(grid)\n        \n        # Method 1: Nearest neighbor upsampling\n        supersampled = np.repeat(np.repeat(grid, factor, axis=0), factor, axis=1)\n        \n        # Method 2: Bilinear interpolation for smoother transitions\n        # Use zoom but round to nearest integer for discrete values\n        supersampled_smooth = zoom(grid.astype(float), factor, order=1)\n        supersampled_smooth = np.round(supersampled_smooth).astype(int)\n        \n        return supersampled, supersampled_smooth\n    \n    def extract_supersampled_features(self, grid):\n        \"\"\"Extract features from supersampled versions of the grid\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        for factor in self.supersample_factors:\n            # Get supersampled versions\n            ss_nearest, ss_smooth = self.supersample_grid(grid, factor)\n            \n            # Extract basic statistics from supersampled grids\n            for ss_grid in [ss_nearest, ss_smooth]:\n                features.extend([\n                    ss_grid.mean(),\n                    ss_grid.std(),\n                    entropy(np.bincount(ss_grid.flatten()) + 1e-10),\n                    len(np.unique(ss_grid))\n                ])\n                \n                # Edge detection on supersampled grid\n                edges = self.detect_edges(ss_grid)\n                features.extend([\n                    edges.sum() / ss_grid.size,  # Edge density\n                    self.compute_edge_continuity(edges)\n                ])\n                \n        # Multi-scale analysis\n        multi_scale_features = []\n        for factor in self.supersample_factors:\n            ss_grid, _ = self.supersample_grid(grid, factor)\n            \n            # Compute features at different scales\n            scale_feat = [\n                self.compute_local_homogeneity(ss_grid),\n                self.compute_pattern_complexity(ss_grid),\n                self.compute_symmetry_score(ss_grid)\n            ]\n            multi_scale_features.extend(scale_feat)\n            \n        features.extend(multi_scale_features)\n        \n        return np.array(features[:40])\n    \n    def detect_edges(self, grid):\n        \"\"\"Detect edges in the grid\"\"\"\n        grid = np.array(grid)\n        edges = np.zeros_like(grid, dtype=bool)\n        \n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]):\n                for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                    ni, nj = i + di, j + dj\n                    if 0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1]:\n                        if grid[i,j] != grid[ni,nj]:\n                            edges[i,j] = True\n                            break\n                            \n        return edges\n    \n    def compute_edge_continuity(self, edges):\n        \"\"\"Compute continuity score of edges\"\"\"\n        if not edges.any():\n            return 0\n            \n        # Find connected edge components\n        labeled, num = label(edges)\n        \n        # Compute continuity as ratio of largest component to total edges\n        if num > 0:\n            component_sizes = [np.sum(labeled == i) for i in range(1, num+1)]\n            return max(component_sizes) / edges.sum()\n        return 0\n    \n    def compute_local_homogeneity(self, grid):\n        \"\"\"Compute local homogeneity measure\"\"\"\n        grid = np.array(grid)\n        homogeneity = 0\n        count = 0\n        \n        # Check 3x3 neighborhoods\n        for i in range(1, grid.shape[0]-1):\n            for j in range(1, grid.shape[1]-1):\n                neighborhood = grid[i-1:i+2, j-1:j+2]\n                unique_values = len(np.unique(neighborhood))\n                homogeneity += 1 / unique_values\n                count += 1\n                \n        return homogeneity / (count + 1e-10)\n    \n    def compute_pattern_complexity(self, grid):\n        \"\"\"Compute pattern complexity using compression ratio\"\"\"\n        grid = np.array(grid)\n        \n        # Convert to bytes\n        grid_bytes = grid.astype(np.uint8).tobytes()\n        \n        # Compress\n        import zlib\n        compressed = zlib.compress(grid_bytes)\n        \n        # Complexity is compression ratio\n        return len(compressed) / (len(grid_bytes) + 1e-10)\n    \n    def compute_symmetry_score(self, grid):\n        \"\"\"Compute comprehensive symmetry score\"\"\"\n        grid = np.array(grid)\n        \n        scores = []\n        \n        # Horizontal symmetry\n        scores.append(np.mean(grid == np.fliplr(grid)))\n        \n        # Vertical symmetry\n        scores.append(np.mean(grid == np.flipud(grid)))\n        \n        # Diagonal symmetry (if square)\n        if grid.shape[0] == grid.shape[1]:\n            scores.append(np.mean(grid == grid.T))\n            scores.append(np.mean(grid == np.rot90(grid, 2)))\n            \n        # Rotational symmetry\n        if grid.shape[0] == grid.shape[1]:\n            scores.append(np.mean(grid == np.rot90(grid)))\n            scores.append(np.mean(grid == np.rot90(grid, 3)))\n            \n        return np.mean(scores)\n    \n    def grid_to_raster_with_repetition(self, grid, repetitions=3):\n        \"\"\"Convert 2D grid to 1D raster with pattern repetition\"\"\"\n        grid = np.array(grid)\n        \n        # Multiple rasterization strategies\n        rasters = []\n        \n        # Row-major order\n        raster_row = grid.flatten()\n        rasters.append(raster_row)\n        \n        # Column-major order\n        raster_col = grid.T.flatten()\n        rasters.append(raster_col)\n        \n        # Spiral order\n        raster_spiral = self.spiral_flatten(grid)\n        rasters.append(raster_spiral)\n        \n        # Zigzag order\n        raster_zigzag = self.zigzag_flatten(grid)\n        rasters.append(raster_zigzag)\n        \n        # Repeat patterns\n        repeated_rasters = []\n        for raster in rasters:\n            repeated = np.tile(raster, repetitions)\n            repeated_rasters.append(repeated)\n            \n        return repeated_rasters\n    \n    def spiral_flatten(self, grid):\n        \"\"\"Flatten grid in spiral order\"\"\"\n        grid = np.array(grid)\n        h, w = grid.shape\n        result = []\n        \n        top, bottom, left, right = 0, h-1, 0, w-1\n        \n        while top <= bottom and left <= right:\n            # Right\n            for j in range(left, right+1):\n                result.append(grid[top, j])\n            top += 1\n            \n            # Down\n            for i in range(top, bottom+1):\n                result.append(grid[i, right])\n            right -= 1\n            \n            # Left\n            if top <= bottom:\n                for j in range(right, left-1, -1):\n                    result.append(grid[bottom, j])\n                bottom -= 1\n                \n            # Up\n            if left <= right:\n                for i in range(bottom, top-1, -1):\n                    result.append(grid[i, left])\n                left += 1\n                \n        return np.array(result)\n    \n    def zigzag_flatten(self, grid):\n        \"\"\"Flatten grid in zigzag order\"\"\"\n        grid = np.array(grid)\n        h, w = grid.shape\n        result = []\n        \n        for i in range(h):\n            if i % 2 == 0:\n                result.extend(grid[i, :])\n            else:\n                result.extend(grid[i, ::-1])\n                \n        return np.array(result)\n    \n    def extract_raster_position_features(self, grid):\n        \"\"\"Extract position-based features from rasterized grid\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Get different rasterizations\n        rasters = self.grid_to_raster_with_repetition(grid, repetitions=2)\n        \n        for raster in rasters[:2]:  # Use first two rasterizations\n            # Position of each color\n            for color in range(10):\n                positions = np.where(raster == color)[0]\n                if len(positions) > 0:\n                    features.extend([\n                        positions[0],  # First occurrence\n                        positions[-1],  # Last occurrence\n                        positions[-1] - positions[0],  # Span\n                        np.mean(positions),  # Average position\n                        np.std(positions) if len(positions) > 1 else 0  # Position variance\n                    ])\n                else:\n                    features.extend([0, 0, 0, 0, 0])\n                    \n        # Relative position features\n        raster = rasters[0]\n        for c1 in range(3):\n            for c2 in range(c1+1, 4):\n                pos1 = np.where(raster == c1)[0]\n                pos2 = np.where(raster == c2)[0]\n                \n                if len(pos1) > 0 and len(pos2) > 0:\n                    # Relative positions\n                    features.extend([\n                        np.mean(pos2) - np.mean(pos1),  # Average distance\n                        np.min(np.abs(pos1[:, None] - pos2[None, :])),  # Min distance\n                    ])\n                else:\n                    features.extend([0, 0])\n                    \n        return np.array(features[:100])\n    \n    def find_regime_transitions(self, raster):\n        \"\"\"Find regime transitions in rasterized data\"\"\"\n        transitions = []\n        \n        # Find points where color changes\n        for i in range(len(raster)-1):\n            if raster[i] != raster[i+1]:\n                transitions.append({\n                    'position': i,\n                    'from': raster[i],\n                    'to': raster[i+1],\n                    'type': 'color_change'\n                })\n                \n        # Find pattern boundaries (where local statistics change)\n        window_size = 5\n        for i in range(window_size, len(raster)-window_size):\n            window_before = raster[i-window_size:i]\n            window_after = raster[i:i+window_size]\n            \n            # Check if statistics differ significantly\n            if len(np.unique(window_before)) != len(np.unique(window_after)):\n                transitions.append({\n                    'position': i,\n                    'type': 'diversity_change',\n                    'before_diversity': len(np.unique(window_before)),\n                    'after_diversity': len(np.unique(window_after))\n                })\n                \n        return transitions\n    \n    def extract_regime_transition_features(self, grid):\n        \"\"\"Extract features from regime transitions\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Get rasterized versions\n        rasters = self.grid_to_raster_with_repetition(grid, repetitions=1)\n        \n        for raster in rasters[:2]:\n            transitions = self.find_regime_transitions(raster)\n            \n            # Transition statistics\n            features.extend([\n                len(transitions),  # Number of transitions\n                len([t for t in transitions if t['type'] == 'color_change']),\n                len([t for t in transitions if t['type'] == 'diversity_change'])\n            ])\n            \n            # Transition spacing\n            if len(transitions) > 1:\n                positions = [t['position'] for t in transitions]\n                spacings = np.diff(positions)\n                features.extend([\n                    np.mean(spacings),\n                    np.std(spacings),\n                    np.min(spacings),\n                    np.max(spacings)\n                ])\n            else:\n                features.extend([0, 0, 0, 0])\n                \n            # Color transition matrix\n            color_transitions = np.zeros((10, 10))\n            for t in transitions:\n                if t['type'] == 'color_change':\n                    color_transitions[int(t['from']), int(t['to'])] += 1\n                    \n            # Features from transition matrix\n            features.extend([\n                entropy(color_transitions.flatten() + 1e-10),\n                np.count_nonzero(color_transitions),\n                np.trace(color_transitions)  # Self-transitions\n            ])\n            \n        return np.array(features[:30])\n    \n    def create_pseudo_labels(self, unlabeled_data, labeled_data, labels):\n        \"\"\"Create pseudo labels for unlabeled data\"\"\"\n        if self.pseudo_label_generator is None:\n            # Initialize label propagation\n            self.pseudo_label_generator = LabelPropagation(kernel='rbf', gamma=20)\n            \n        # Combine labeled and unlabeled data\n        X_combined = np.vstack([labeled_data, unlabeled_data])\n        \n        # Create labels (-1 for unlabeled)\n        y_combined = np.concatenate([labels, np.full(len(unlabeled_data), -1)])\n        \n        # Propagate labels\n        self.pseudo_label_generator.fit(X_combined, y_combined)\n        pseudo_labels = self.pseudo_label_generator.transduction_[len(labeled_data):]\n        \n        # Get confidence scores\n        label_distributions = self.pseudo_label_generator.label_distributions_[len(labeled_data):]\n        confidences = np.max(label_distributions, axis=1)\n        \n        return pseudo_labels, confidences\n    \n    def balance_labels(self, features, labels, strategy='oversample'):\n        \"\"\"Balance labels using various strategies\"\"\"\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        max_count = np.max(counts)\n        \n        balanced_features = []\n        balanced_labels = []\n        \n        for label in unique_labels:\n            label_indices = np.where(labels == label)[0]\n            label_features = features[label_indices]\n            \n            if strategy == 'oversample':\n                # Oversample minority classes\n                n_samples = max_count\n                if len(label_indices) < n_samples:\n                    # Add synthetic samples\n                    synthetic = self.generate_synthetic_samples(label_features, \n                                                              n_samples - len(label_indices))\n                    label_features = np.vstack([label_features, synthetic])\n                    \n            elif strategy == 'smote':\n                # SMOTE-like synthetic generation\n                n_samples = max_count\n                if len(label_indices) < n_samples:\n                    synthetic = self.smote_generate(label_features, \n                                                   n_samples - len(label_indices))\n                    label_features = np.vstack([label_features, synthetic])\n                    \n            balanced_features.append(label_features)\n            balanced_labels.extend([label] * len(label_features))\n            \n        return np.vstack(balanced_features), np.array(balanced_labels)\n    \n    def generate_synthetic_samples(self, samples, n_synthetic):\n        \"\"\"Generate synthetic samples using interpolation\"\"\"\n        if len(samples) < 2:\n            # If only one sample, add noise\n            synthetic = samples[0] + np.random.normal(0, 0.1, (n_synthetic, samples.shape[1]))\n        else:\n            synthetic = []\n            for _ in range(n_synthetic):\n                # Select two random samples\n                idx1, idx2 = np.random.choice(len(samples), 2, replace=False)\n                alpha = np.random.random()\n                \n                # Interpolate\n                new_sample = alpha * samples[idx1] + (1 - alpha) * samples[idx2]\n                synthetic.append(new_sample)\n                \n            synthetic = np.array(synthetic)\n            \n        return synthetic\n    \n    def smote_generate(self, samples, n_synthetic):\n        \"\"\"SMOTE-like synthetic sample generation\"\"\"\n        if len(samples) < 2:\n            return self.generate_synthetic_samples(samples, n_synthetic)\n            \n        # Find k nearest neighbors\n        k = min(5, len(samples) - 1)\n        nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(samples)\n        \n        synthetic = []\n        for _ in range(n_synthetic):\n            # Select random sample\n            idx = np.random.randint(len(samples))\n            sample = samples[idx]\n            \n            # Find neighbors\n            distances, indices = nbrs.kneighbors([sample])\n            neighbor_idx = indices[0][np.random.randint(1, k+1)]  # Skip self\n            neighbor = samples[neighbor_idx]\n            \n            # Generate synthetic sample\n            alpha = np.random.random()\n            new_sample = sample + alpha * (neighbor - sample)\n            synthetic.append(new_sample)\n            \n        return np.array(synthetic)\n    \n    def extract_continuous_adaptation_features(self, grid):\n        \"\"\"Extract features for continuous learning adaptation\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Quantization analysis\n        unique_values = np.unique(grid)\n        n_unique = len(unique_values)\n        \n        # Value distribution\n        value_counts = np.bincount(grid.flatten())\n        value_probs = value_counts / grid.size\n        \n        features.extend([\n            n_unique,\n            entropy(value_probs + 1e-10),\n            np.max(value_probs),\n            np.std(value_probs)\n        ])\n        \n        # Spatial autocorrelation (Moran's I approximation)\n        mean_val = grid.mean()\n        \n        spatial_auto = 0\n        weight_sum = 0\n        \n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]):\n                for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                    ni, nj = i + di, j + dj\n                    if 0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1]:\n                        spatial_auto += (grid[i,j] - mean_val) * (grid[ni,nj] - mean_val)\n                        weight_sum += 1\n                        \n        if weight_sum > 0 and np.var(grid) > 0:\n            morans_i = (grid.size / weight_sum) * (spatial_auto / (grid.size * np.var(grid)))\n            features.append(morans_i)\n        else:\n            features.append(0)\n            \n        # Quantization error analysis\n        # Simulate continuous version and measure error\n        continuous_grid = gaussian_filter(grid.astype(float), sigma=0.5)\n        quantized_grid = np.round(continuous_grid).astype(int)\n        quant_error = np.mean(np.abs(quantized_grid - grid))\n        features.append(quant_error)\n        \n        # Multi-resolution quantization\n        for n_levels in [2, 4, 8]:\n            # Quantize to n levels\n            min_val, max_val = grid.min(), grid.max()\n            if max_val > min_val:\n                quantized = np.floor((grid - min_val) / (max_val - min_val + 1e-10) * n_levels)\n                reconstruction_error = np.mean(np.abs(quantized - grid))\n            else:\n                reconstruction_error = 0\n            features.append(reconstruction_error)\n            \n        # Pattern stability under perturbation\n        stability_scores = []\n        for _ in range(3):\n            # Add small noise\n            noisy = grid + np.random.normal(0, 0.1, grid.shape)\n            quantized_noisy = np.round(noisy).astype(int)\n            quantized_noisy = np.clip(quantized_noisy, 0, 9)\n            \n            # Measure stability\n            stability = np.mean(quantized_noisy == grid)\n            stability_scores.append(stability)\n            \n        features.extend([\n            np.mean(stability_scores),\n            np.std(stability_scores)\n        ])\n        \n        return np.array(features[:15])\n    \n    # Original feature extraction methods\n    def extract_geometric_means(self, grid):\n        \"\"\"Extract geometric mean features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Avoid log of zero\n        grid_safe = np.where(grid > 0, grid, 1)\n        \n        # Global geometric mean\n        geom_mean = np.exp(np.mean(np.log(grid_safe)))\n        features.append(geom_mean)\n        \n        # Row-wise geometric means\n        for i in range(min(3, grid.shape[0])):\n            row = grid_safe[i]\n            features.append(np.exp(np.mean(np.log(row))))\n            \n        # Column-wise geometric means\n        for j in range(min(3, grid.shape[1])):\n            col = grid_safe[:, j]\n            features.append(np.exp(np.mean(np.log(col))))\n            \n        # Quadrant geometric means\n        h, w = grid.shape\n        quadrants = [\n            grid_safe[:h//2, :w//2],\n            grid_safe[:h//2, w//2:],\n            grid_safe[h//2:, :w//2],\n            grid_safe[h//2:, w//2:]\n        ]\n        \n        for q in quadrants:\n            if q.size > 0:\n                features.append(np.exp(np.mean(np.log(q))))\n            else:\n                features.append(0)\n                \n        return np.array(features[:15])\n    \n    def extract_weighted_centroids(self, grid):\n        \"\"\"Extract centroids weighted by color values\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Weight by color value\n        total_weight = np.sum(grid)\n        if total_weight > 0:\n            y_coords, x_coords = np.meshgrid(range(grid.shape[0]), range(grid.shape[1]), indexing='ij')\n            weighted_y = np.sum(y_coords * grid) / total_weight\n            weighted_x = np.sum(x_coords * grid) / total_weight\n            features.extend([weighted_y, weighted_x])\n        else:\n            features.extend([0, 0])\n            \n        # Per-color weighted centroids\n        for color in range(1, 10):\n            mask = (grid == color)\n            if mask.sum() > 0:\n                y, x = np.where(mask)\n                # Weight by distance from center\n                cy, cx = grid.shape[0]/2, grid.shape[1]/2\n                distances = np.sqrt((y - cy)**2 + (x - cx)**2)\n                weights = 1 / (distances + 1)\n                wy = np.average(y, weights=weights)\n                wx = np.average(x, weights=weights)\n                features.extend([wy, wx])\n            else:\n                features.extend([0, 0])\n                \n        return np.array(features[:20])\n    \n    def extract_color_gradients(self, grid):\n        \"\"\"Extract color gradient features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Sobel gradients\n        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n        \n        grad_x = correlate2d(grid, sobel_x, mode='same', boundary='wrap')\n        grad_y = correlate2d(grid, sobel_y, mode='same', boundary='wrap')\n        \n        grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n        grad_dir = np.arctan2(grad_y, grad_x)\n        \n        features.extend([\n            grad_mag.mean(),\n            grad_mag.std(),\n            grad_mag.max(),\n            np.mean(np.abs(grad_dir)),\n            np.std(grad_dir)\n        ])\n        \n        # Color transition matrix\n        h, w = grid.shape\n        transitions = np.zeros((10, 10))\n        \n        for i in range(h):\n            for j in range(w-1):\n                c1, c2 = int(grid[i,j]), int(grid[i,j+1])\n                transitions[c1, c2] += 1\n                \n        for i in range(h-1):\n            for j in range(w):\n                c1, c2 = int(grid[i,j]), int(grid[i+1,j])\n                transitions[c1, c2] += 1\n                \n        # Transition features\n        features.extend([\n            entropy(transitions.flatten() + 1e-10),\n            np.sum(np.diag(transitions)) / (transitions.sum() + 1e-10),  # Self-transition ratio\n            np.count_nonzero(transitions)  # Number of unique transitions\n        ])\n        \n        return np.array(features[:10])\n    \n    def extract_segmented_features(self, grid):\n        \"\"\"Extract features from grid segments\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Segment by connected components\n        labeled, num = label(grid > 0)\n        \n        segment_features = []\n        for i in range(1, min(num + 1, 6)):\n            mask = labeled == i\n            if mask.sum() > 0:\n                # Segment properties\n                y, x = np.where(mask)\n                seg_feat = [\n                    y.mean(), x.mean(),  # Centroid\n                    y.std(), x.std(),    # Spread\n                    mask.sum(),          # Size\n                    grid[mask][0],       # Color\n                    y.max() - y.min(),  # Height\n                    x.max() - x.min()   # Width\n                ]\n                segment_features.append(seg_feat)\n                \n        # Aggregate segment features\n        if segment_features:\n            segment_features = np.array(segment_features)\n            features.extend([\n                segment_features.mean(axis=0),\n                segment_features.std(axis=0),\n                segment_features.max(axis=0) - segment_features.min(axis=0)\n            ].flatten()[:30])\n        else:\n            features.extend([0] * 30)\n            \n        # Grid-based segmentation (divide into regions)\n        h, w = grid.shape\n        regions = []\n        for i in range(2):\n            for j in range(2):\n                region = grid[i*h//2:(i+1)*h//2, j*w//2:(j+1)*w//2]\n                if region.size > 0:\n                    regions.append([\n                        region.mean(),\n                        region.std(),\n                        mode(region.flatten())[0][0],\n                        entropy(np.bincount(region.flatten()) + 1e-10)\n                    ])\n                    \n        if regions:\n            regions = np.array(regions)\n            features.extend(regions.flatten()[:16])\n        else:\n            features.extend([0] * 16)\n            \n        return np.array(features[:50])\n    \n    def extract_relative_segment_vectors(self, grid):\n        \"\"\"Extract relative vectors between segments\"\"\"\n        grid = np.array(grid)\n        labeled, num = label(grid > 0)\n        features = []\n        \n        # Get segment centroids and properties\n        segments = []\n        for i in range(1, min(num + 1, 8)):\n            mask = labeled == i\n            if mask.sum() > 0:\n                y, x = np.where(mask)\n                segments.append({\n                    'id': i,\n                    'centroid': (y.mean(), x.mean()),\n                    'size': mask.sum(),\n                    'color': grid[mask][0],\n                    'bbox': (y.min(), y.max(), x.min(), x.max()),\n                    'aspect': (x.max() - x.min() + 1) / (y.max() - y.min() + 1)\n                })\n                \n        # Pairwise relative features\n        if len(segments) >= 2:\n            vectors = []\n            for i, s1 in enumerate(segments):\n                for j, s2 in enumerate(segments):\n                    if i < j:\n                        # Relative position\n                        dy = s2['centroid'][0] - s1['centroid'][0]\n                        dx = s2['centroid'][1] - s1['centroid'][1]\n                        dist = np.sqrt(dy**2 + dx**2)\n                        angle = np.arctan2(dy, dx)\n                        \n                        # Relative properties\n                        size_ratio = s2['size'] / (s1['size'] + 1e-10)\n                        aspect_ratio = s2['aspect'] / (s1['aspect'] + 1e-10)\n                        color_diff = abs(s2['color'] - s1['color'])\n                        \n                        # Spatial relationship\n                        overlap_y = max(0, min(s1['bbox'][1], s2['bbox'][1]) - \n                                       max(s1['bbox'][0], s2['bbox'][0]))\n                        overlap_x = max(0, min(s1['bbox'][3], s2['bbox'][3]) - \n                                       max(s1['bbox'][2], s2['bbox'][2]))\n                        \n                        vectors.append([\n                            dy, dx, dist, angle,\n                            size_ratio, aspect_ratio, color_diff,\n                            overlap_y > 0, overlap_x > 0\n                        ])\n                        \n            if vectors:\n                vectors = np.array(vectors)\n                features.extend([\n                    vectors.mean(axis=0),\n                    vectors.std(axis=0),\n                    vectors.max(axis=0),\n                    vectors.min(axis=0)\n                ].flatten()[:40])\n            else:\n                features.extend([0] * 40)\n        else:\n            features.extend([0] * 40)\n            \n        return np.array(features[:40])\n    \n    def extract_hash_features(self, grid):\n        \"\"\"Extract hash-based features for pattern matching\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Grid hash\n        grid_bytes = grid.astype(np.uint8).tobytes()\n        grid_hash = int(hashlib.md5(grid_bytes).hexdigest()[:8], 16)\n        features.append(grid_hash / 1e10)  # Normalize\n        \n        # Row hashes\n        for i in range(min(3, grid.shape[0])):\n            row_hash = int(hashlib.md5(grid[i].tobytes()).hexdigest()[:8], 16)\n            features.append(row_hash / 1e10)\n            \n        # Column hashes\n        for j in range(min(3, grid.shape[1])):\n            col_hash = int(hashlib.md5(grid[:, j].tobytes()).hexdigest()[:8], 16)\n            features.append(col_hash / 1e10)\n            \n        # Transformation hashes\n        transforms = [\n            np.rot90(grid),\n            np.fliplr(grid),\n            np.flipud(grid)\n        ]\n        \n        for trans in transforms:\n            trans_hash = int(hashlib.md5(trans.tobytes()).hexdigest()[:8], 16)\n            features.append(trans_hash / 1e10)\n            \n        # Pattern hashes (2x2, 3x3 blocks)\n        pattern_hashes = []\n        for size in [2, 3]:\n            for i in range(min(2, grid.shape[0] - size + 1)):\n                for j in range(min(2, grid.shape[1] - size + 1)):\n                    block = grid[i:i+size, j:j+size]\n                    block_hash = int(hashlib.md5(block.tobytes()).hexdigest()[:8], 16)\n                    pattern_hashes.append(block_hash / 1e10)\n                    \n        features.extend(pattern_hashes[:5])\n        \n        return np.array(features[:15])\n    \n    def extract_compression_features(self, grid):\n        \"\"\"Extract compression-based complexity features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Run-length encoding compression ratio\n        flat = grid.flatten()\n        runs = []\n        if len(flat) > 0:\n            current = flat[0]\n            count = 1\n            for val in flat[1:]:\n                if val == current:\n                    count += 1\n                else:\n                    runs.append((current, count))\n                    current = val\n                    count = 1\n            runs.append((current, count))\n            \n        compression_ratio = len(runs) / (len(flat) + 1e-10)\n        features.append(compression_ratio)\n        \n        # Entropy-based complexity\n        counts = np.bincount(flat)\n        probs = counts[counts > 0] / len(flat)\n        entropy_val = -np.sum(probs * np.log2(probs + 1e-10))\n        features.append(entropy_val)\n        \n        # Kolmogorov complexity approximation (using zlib)\n        import zlib\n        grid_bytes = grid.astype(np.uint8).tobytes()\n        compressed_size = len(zlib.compress(grid_bytes))\n        original_size = len(grid_bytes)\n        complexity = compressed_size / (original_size + 1e-10)\n        features.append(complexity)\n        \n        # Pattern repetition score\n        h, w = grid.shape\n        repetition_score = 0\n        \n        # Check horizontal repetition\n        for period in range(1, w//2 + 1):\n            if np.all(grid[:, :w-period] == grid[:, period:]):\n                repetition_score += 1 / period\n                break\n                \n        # Check vertical repetition\n        for period in range(1, h//2 + 1):\n            if np.all(grid[:h-period, :] == grid[period:, :]):\n                repetition_score += 1 / period\n                break\n                \n        features.append(repetition_score)\n        \n        # Information content\n        unique_values = len(np.unique(grid))\n        info_content = unique_values / 10  # Normalized by max colors\n        features.append(info_content)\n        \n        return np.array(features[:10])\n    \n    def extract_fourier_descriptors(self, grid):\n        \"\"\"Extract Fourier descriptors for shape analysis\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # 2D FFT\n        fft = np.fft.fft2(grid)\n        fft_mag = np.abs(fft)\n        fft_phase = np.angle(fft)\n        \n        # Low-frequency descriptors (shape)\n        h, w = grid.shape\n        low_freq = fft_mag[:min(4, h), :min(4, w)].flatten()\n        features.extend(low_freq[:10])\n        \n        # Radial distribution\n        center = (h//2, w//2)\n        y, x = np.ogrid[:h, :w]\n        r = np.sqrt((x - center[1])**2 + (y - center[0])**2)\n        \n        radial_bins = np.linspace(0, np.sqrt(h**2 + w**2)/2, 5)\n        radial_profile = []\n        \n        for i in range(len(radial_bins)-1):\n            mask = (r >= radial_bins[i]) & (r < radial_bins[i+1])\n            radial_profile.append(fft_mag[mask].mean() if mask.sum() > 0 else 0)\n            \n        features.extend(radial_profile)\n        \n        # Angular distribution\n        theta = np.arctan2(y - center[0], x - center[1])\n        angular_bins = np.linspace(-np.pi, np.pi, 9)\n        angular_profile = []\n        \n        for i in range(len(angular_bins)-1):\n            mask = (theta >= angular_bins[i]) & (theta < angular_bins[i+1])\n            angular_profile.append(fft_mag[mask].mean() if mask.sum() > 0 else 0)\n            \n        features.extend(angular_profile[:5])\n        \n        return np.array(features[:20])\n    \n    def extract_wavelet_features(self, grid):\n        \"\"\"Extract wavelet-based features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Simple Haar wavelet decomposition\n        h, w = grid.shape\n        \n        # Level 1 decomposition\n        if h >= 2 and w >= 2:\n            # Approximate\n            cA = (grid[::2, ::2] + grid[1::2, ::2] + \n                  grid[::2, 1::2] + grid[1::2, 1::2]) / 4\n            \n            # Horizontal detail\n            cH = (grid[::2, ::2] - grid[1::2, ::2] + \n                  grid[::2, 1::2] - grid[1::2, 1::2]) / 4\n            \n            # Vertical detail\n            cV = (grid[::2, ::2] + grid[1::2, ::2] - \n                  grid[::2, 1::2] - grid[1::2, 1::2]) / 4\n            \n            # Diagonal detail\n            cD = (grid[::2, ::2] - grid[1::2, ::2] - \n                  grid[::2, 1::2] + grid[1::2, 1::2]) / 4\n            \n            features.extend([\n                cA.mean(), cA.std(),\n                np.abs(cH).mean(), np.abs(cH).std(),\n                np.abs(cV).mean(), np.abs(cV).std(),\n                np.abs(cD).mean(), np.abs(cD).std()\n            ])\n        else:\n            features.extend([0] * 8)\n            \n        # Energy distribution\n        if h >= 2 and w >= 2:\n            total_energy = np.sum(grid**2)\n            if total_energy > 0:\n                features.extend([\n                    np.sum(cA**2) / total_energy,\n                    np.sum(cH**2) / total_energy,\n                    np.sum(cV**2) / total_energy,\n                    np.sum(cD**2) / total_energy\n                ])\n            else:\n                features.extend([0] * 4)\n        else:\n            features.extend([0] * 4)\n            \n        return np.array(features[:15])\n    \n    def extract_texture_features(self, grid):\n        \"\"\"Extract texture features using GLCM-like approach\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Co-occurrence matrix\n        levels = 10\n        glcm = np.zeros((levels, levels))\n        \n        # Horizontal pairs\n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]-1):\n                glcm[grid[i,j], grid[i,j+1]] += 1\n                \n        # Vertical pairs\n        for i in range(grid.shape[0]-1):\n            for j in range(grid.shape[1]):\n                glcm[grid[i,j], grid[i+1,j]] += 1\n                \n        # Normalize\n        if glcm.sum() > 0:\n            glcm = glcm / glcm.sum()\n            \n        # Texture features from GLCM\n        # Contrast\n        contrast = 0\n        for i in range(levels):\n            for j in range(levels):\n                contrast += (i-j)**2 * glcm[i,j]\n        features.append(contrast)\n        \n        # Homogeneity\n        homogeneity = 0\n        for i in range(levels):\n            for j in range(levels):\n                homogeneity += glcm[i,j] / (1 + abs(i-j))\n        features.append(homogeneity)\n        \n        # Energy\n        energy = np.sum(glcm**2)\n        features.append(energy)\n        \n        # Correlation\n        if glcm.sum() > 0:\n            mu_i = np.sum(np.arange(levels)[:, None] * glcm)\n            mu_j = np.sum(np.arange(levels)[None, :] * glcm)\n            sigma_i = np.sqrt(np.sum((np.arange(levels)[:, None] - mu_i)**2 * glcm))\n            sigma_j = np.sqrt(np.sum((np.arange(levels)[None, :] - mu_j)**2 * glcm))\n            \n            if sigma_i > 0 and sigma_j > 0:\n                correlation = np.sum((np.arange(levels)[:, None] - mu_i) * \n                                   (np.arange(levels)[None, :] - mu_j) * glcm) / (sigma_i * sigma_j)\n            else:\n                correlation = 0\n        else:\n            correlation = 0\n        features.append(correlation)\n        \n        # Local Binary Patterns (simplified)\n        lbp_hist = np.zeros(8)\n        for i in range(1, grid.shape[0]-1):\n            for j in range(1, grid.shape[1]-1):\n                center = grid[i,j]\n                pattern = 0\n                neighbors = [\n                    grid[i-1,j-1], grid[i-1,j], grid[i-1,j+1],\n                    grid[i,j+1], grid[i+1,j+1], grid[i+1,j],\n                    grid[i+1,j-1], grid[i,j-1]\n                ]\n                for k, neighbor in enumerate(neighbors):\n                    if neighbor > center:\n                        pattern |= (1 << k)\n                lbp_hist[pattern % 8] += 1\n                \n        if lbp_hist.sum() > 0:\n            lbp_hist = lbp_hist / lbp_hist.sum()\n        features.extend(lbp_hist[:5])\n        \n        return np.array(features[:10])\n    \n    def extract_shape_contexts(self, grid):\n        \"\"\"Extract shape context features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Find object boundaries\n        binary = grid > 0\n        if not binary.any():\n            return np.zeros(20)\n            \n        # Edge detection\n        edges = np.zeros_like(binary)\n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]):\n                if binary[i,j]:\n                    for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                        ni, nj = i + di, j + dj\n                        if (0 <= ni < grid.shape[0] and 0 <= nj < grid.shape[1] and \n                            not binary[ni,nj]) or \\\n                           (ni < 0 or ni >= grid.shape[0] or nj < 0 or nj >= grid.shape[1]):\n                            edges[i,j] = True\n                            break\n                            \n        # Sample points on boundary\n        edge_points = np.column_stack(np.where(edges))\n        if len(edge_points) == 0:\n            return np.zeros(20)\n            \n        # Sample up to 10 points\n        if len(edge_points) > 10:\n            indices = np.random.choice(len(edge_points), 10, replace=False)\n            sample_points = edge_points[indices]\n        else:\n            sample_points = edge_points\n            \n        # Compute shape contexts\n        shape_contexts = []\n        for point in sample_points:\n            # Compute histogram of relative positions\n            rel_positions = edge_points - point\n            distances = np.sqrt(np.sum(rel_positions**2, axis=1))\n            angles = np.arctan2(rel_positions[:, 0], rel_positions[:, 1])\n            \n            # Bin into histogram\n            dist_bins = np.logspace(0, np.log10(np.max(distances) + 1), 4)\n            angle_bins = np.linspace(-np.pi, np.pi, 9)\n            \n            hist = np.zeros((3, 8))\n            for d, a in zip(distances, angles):\n                if d > 0:  # Skip self\n                    d_bin = np.searchsorted(dist_bins, d) - 1\n                    a_bin = np.searchsorted(angle_bins, a) - 1\n                    if 0 <= d_bin < 3 and 0 <= a_bin < 8:\n                        hist[d_bin, a_bin] += 1\n                        \n            shape_contexts.append(hist.flatten())\n            \n        # Average shape contexts\n        if shape_contexts:\n            avg_context = np.mean(shape_contexts, axis=0)\n            features.extend(avg_context[:20])\n        else:\n            features.extend([0] * 20)\n            \n        return np.array(features[:20])\n    \n    def extract_persistent_homology(self, grid):\n        \"\"\"Extract topological features using persistent homology concepts\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Filtration by color values\n        max_val = grid.max()\n        \n        # Compute connected components at different thresholds\n        birth_death = []\n        for threshold in range(1, int(max_val) + 1):\n            binary = grid >= threshold\n            labeled, num = label(binary)\n            birth_death.append((threshold, num))\n            \n        # Persistence features\n        if birth_death:\n            births = [bd[0] for bd in birth_death]\n            deaths = [bd[1] for bd in birth_death]\n            \n            features.extend([\n                np.mean(births),\n                np.std(births),\n                np.mean(deaths),\n                np.std(deaths),\n                len(birth_death)\n            ])\n            \n            # Persistence diagram features\n            persistences = []\n            for i in range(len(birth_death)-1):\n                if deaths[i] > deaths[i+1]:\n                    persistences.append(births[i+1] - births[i])\n                    \n            if persistences:\n                features.extend([\n                    np.mean(persistences),\n                    np.max(persistences),\n                    len(persistences)\n                ])\n            else:\n                features.extend([0, 0, 0])\n        else:\n            features.extend([0] * 8)\n            \n        # Betti numbers at different scales\n        for scale in [1, 2, 3]:\n            binary = grid >= scale\n            labeled, num_components = label(binary)\n            \n            # Simplified hole detection\n            holes = 0\n            for i in range(1, grid.shape[0]-1):\n                for j in range(1, grid.shape[1]-1):\n                    if not binary[i,j]:\n                        # Check if surrounded\n                        if all(binary[i+di,j+dj] for di,dj in \n                              [(-1,0), (1,0), (0,-1), (0,1)]):\n                            holes += 1\n                            \n            features.extend([num_components, holes])\n            \n        return np.array(features[:20])\n    \n    def extract_graph_features(self, grid):\n        \"\"\"Extract graph-based features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Build adjacency graph\n        h, w = grid.shape\n        nodes = []\n        node_map = {}\n        \n        # Create nodes for each non-zero cell\n        for i in range(h):\n            for j in range(w):\n                if grid[i,j] > 0:\n                    node_id = len(nodes)\n                    nodes.append((i, j, grid[i,j]))\n                    node_map[(i,j)] = node_id\n                    \n        # Build edges\n        edges = []\n        for idx, (i, j, color) in enumerate(nodes):\n            for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                ni, nj = i + di, j + dj\n                if (ni, nj) in node_map:\n                    neighbor_idx = node_map[(ni, nj)]\n                    if nodes[neighbor_idx][2] == color:  # Same color\n                        edges.append((idx, neighbor_idx))\n                        \n        # Graph features\n        num_nodes = len(nodes)\n        num_edges = len(edges) // 2  # Undirected\n        \n        features.extend([num_nodes, num_edges])\n        \n        # Degree distribution\n        degrees = np.zeros(num_nodes)\n        for u, v in edges:\n            degrees[u] += 1\n            \n        if num_nodes > 0:\n            features.extend([\n                degrees.mean(),\n                degrees.std(),\n                degrees.max(),\n                np.sum(degrees == 0) / num_nodes  # Isolated nodes ratio\n            ])\n        else:\n            features.extend([0] * 4)\n            \n        # Connected components (using simple DFS)\n        if num_nodes > 0:\n            visited = [False] * num_nodes\n            components = 0\n            \n            def dfs(node):\n                visited[node] = True\n                for u, v in edges:\n                    if u == node and not visited[v]:\n                        dfs(v)\n                    elif v == node and not visited[u]:\n                        dfs(u)\n                        \n            for i in range(num_nodes):\n                if not visited[i]:\n                    dfs(i)\n                    components += 1\n                    \n            features.append(components)\n        else:\n            features.append(0)\n            \n        # Clustering coefficient (simplified)\n        triangles = 0\n        for i in range(num_nodes):\n            neighbors = [v for u, v in edges if u == i] + [u for u, v in edges if v == i]\n            neighbors = list(set(neighbors))\n            \n            # Count triangles\n            for j in range(len(neighbors)):\n                for k in range(j+1, len(neighbors)):\n                    if (neighbors[j], neighbors[k]) in edges or (neighbors[k], neighbors[j]) in edges:\n                        triangles += 1\n                        \n        if num_edges > 0:\n            clustering = triangles / (num_edges + 1e-10)\n        else:\n            clustering = 0\n        features.append(clustering)\n        \n        return np.array(features[:10])\n    \n    def extract_semantic_features(self, grid):\n        \"\"\"Extract high-level semantic features\"\"\"\n        grid = np.array(grid)\n        features = []\n        \n        # Object count by color\n        for color in range(1, 10):\n            mask = grid == color\n            if mask.any():\n                labeled, num = label(mask)\n                features.append(num)\n            else:\n                features.append(0)\n                \n        # Symmetry detection\n        h_symmetry = np.mean(grid == np.fliplr(grid))\n        v_symmetry = np.mean(grid == np.flipud(grid))\n        \n        if grid.shape[0] == grid.shape[1]:\n            d_symmetry = np.mean(grid == grid.T)\n            r_symmetry = np.mean(grid == np.rot90(grid, 2))\n        else:\n            d_symmetry = r_symmetry = 0\n            \n        features.extend([h_symmetry, v_symmetry, d_symmetry, r_symmetry])\n        \n        # Pattern detection scores\n        # Checkerboard\n        h, w = grid.shape\n        checker = np.array([[(i+j)%2 for j in range(w)] for i in range(h)])\n        colors = np.unique(grid)\n        \n        checker_score = 0\n        if len(colors) == 2:\n            c1, c2 = colors\n            score1 = np.mean((grid == c1) == (checker == 0))\n            score2 = np.mean((grid == c1) == (checker == 1))\n            checker_score = max(score1, score2)\n        features.append(checker_score)\n        \n        # Stripe detection\n        h_stripe_score = 0\n        for period in range(1, min(4, h)):\n            stripe = np.array([[i//period % 2 for j in range(w)] for i in range(h)])\n            if len(colors) == 2:\n                score = max(np.mean((grid == colors[0]) == (stripe == 0)),\n                           np.mean((grid == colors[0]) == (stripe == 1)))\n                h_stripe_score = max(h_stripe_score, score)\n        features.append(h_stripe_score)\n        \n        # Border detection\n        border_mask = np.zeros_like(grid, dtype=bool)\n        border_mask[0,:] = border_mask[-1,:] = True\n        border_mask[:,0] = border_mask[:,-1] = True\n        \n        border_consistency = 0\n        for color in colors:\n            if np.all(grid[border_mask] == color):\n                border_consistency = 1\n                break\n        features.append(border_consistency)\n        \n        return np.array(features[:20])\n    \n    def extract_invariant_moments(self, grid):\n        \"\"\"Extract Hu moments and other invariant features\"\"\"\n        grid = np.array(grid, dtype=float)\n        features = []\n        \n        # Spatial moments\n        m00 = np.sum(grid)\n        if m00 == 0:\n            return np.zeros(15)\n            \n        y, x = np.mgrid[:grid.shape[0], :grid.shape[1]]\n        m10 = np.sum(x * grid)\n        m01 = np.sum(y * grid)\n        \n        # Centroid\n        cx = m10 / m00\n        cy = m01 / m00\n        \n        # Central moments\n        mu20 = np.sum((x - cx)**2 * grid) / m00\n        mu02 = np.sum((y - cy)**2 * grid) / m00\n        mu11 = np.sum((x - cx) * (y - cy) * grid) / m00\n        mu30 = np.sum((x - cx)**3 * grid) / m00\n        mu03 = np.sum((y - cy)**3 * grid) / m00\n        mu21 = np.sum((x - cx)**2 * (y - cy) * grid) / m00\n        mu12 = np.sum((x - cx) * (y - cy)**2 * grid) / m00\n        \n        # Normalized moments\n        norm = m00 ** (2/2 + 1)\n        nu20 = mu20 / norm\n        nu02 = mu02 / norm\n        nu11 = mu11 / norm\n        \n        norm3 = m00 ** (3/2 + 1)\n        nu30 = mu30 / norm3\n        nu03 = mu03 / norm3\n        nu21 = mu21 / norm3\n        nu12 = mu12 / norm3\n        \n        # Hu moments (first 7)\n        h1 = nu20 + nu02\n        h2 = (nu20 - nu02)**2 + 4*nu11**2\n        h3 = (nu30 - 3*nu12)**2 + (3*nu21 - nu03)**2\n        h4 = (nu30 + nu12)**2 + (nu21 + nu03)**2\n        h5 = (nu30 - 3*nu12)*(nu30 + nu12)*((nu30 + nu12)**2 - 3*(nu21 + nu03)**2) + \\\n             (3*nu21 - nu03)*(nu21 + nu03)*(3*(nu30 + nu12)**2 - (nu21 + nu03)**2)\n        h6 = (nu20 - nu02)*((nu30 + nu12)**2 - (nu21 + nu03)**2) + \\\n             4*nu11*(nu30 + nu12)*(nu21 + nu03)\n        h7 = (3*nu21 - nu03)*(nu30 + nu12)*((nu30 + nu12)**2 - 3*(nu21 + nu03)**2) - \\\n             (nu30 - 3*nu12)*(nu21 + nu03)*(3*(nu30 + nu12)**2 - (nu21 + nu03)**2)\n        \n        # Log transform for scale invariance\n        hu_moments = [h1, h2, h3, h4, h5, h6, h7]\n        for i, h in enumerate(hu_moments):\n            if h != 0:\n                hu_moments[i] = -np.sign(h) * np.log10(abs(h) + 1e-10)\n            else:\n                hu_moments[i] = 0\n                \n        features.extend(hu_moments)\n        \n        # Additional invariants\n        features.extend([\n            mu20 + mu02,  # Trace\n            mu20 * mu02 - mu11**2,  # Determinant\n            np.sqrt(mu20**2 + mu02**2 + 2*mu11**2)  # Frobenius norm\n        ])\n        \n        return np.array(features[:15])\n    \n    def extract_all_features(self, grid):\n        \"\"\"Extract all features from grid\"\"\"\n        features = []\n        \n        for extractor in self.feature_extractors:\n            try:\n                feat = extractor(grid)\n                features.extend(feat)\n            except Exception as e:\n                # Add zeros if extraction fails\n                features.extend([0] * 10)\n                \n        return np.array(features[:600])  # Increased from 500 to accommodate new features\n    \n    def train_continuous_models(self, training_data):\n        \"\"\"Train continuous learning models on the data\"\"\"\n        X_train = []\n        y_train = []\n        \n        for task_data in training_data:\n            for example in task_data['train']:\n                inp = np.array(example['input'])\n                out = np.array(example['output'])\n                \n                # Extract features\n                inp_features = self.extract_all_features(inp)\n                \n                # Create multiple training samples from output\n                if inp.shape == out.shape:\n                    # Pixel-wise prediction\n                    for i in range(out.shape[0]):\n                        for j in range(out.shape[1]):\n                            # Add position features\n                            pos_features = [i/out.shape[0], j/out.shape[1], \n                                          i, j, inp[i,j]]\n                            combined_features = np.concatenate([inp_features, pos_features])\n                            X_train.append(combined_features)\n                            y_train.append(out[i,j])\n                            \n        X_train = np.array(X_train)\n        y_train = np.array(y_train)\n        \n        # Balance labels\n        X_balanced, y_balanced = self.balance_labels(X_train, y_train, strategy='smote')\n        \n        # Train ensemble of models\n        self.continuous_models = [\n            MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000),\n            RandomForestRegressor(n_estimators=100),\n            GradientBoostingRegressor(n_estimators=100)\n        ]\n        \n        # Standardize features\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_balanced)\n        \n        for model in self.continuous_models:\n            model.fit(X_scaled, y_balanced)\n            \n        self.feature_scaler = scaler\n        \n    def generate_enhanced_code(self, task_data):\n        \"\"\"Generate code using enhanced techniques\"\"\"\n        examples = task_data['train']\n        \n        # Quick pattern matching first\n        for pattern_name, (code_template, _) in self.verified_patterns.items():\n            if self.matches_pattern(examples, pattern_name):\n                return self.instantiate_pattern(code_template, examples)\n                \n        # Analyze transformations with enhanced features\n        transformations = self.analyze_enhanced_transformation(examples)\n        \n        # Try multiple generation strategies\n        candidates = []\n        \n        # Strategy 1: Supersampling-based analysis\n        ss_code = self.generate_supersampling_based_code(transformations)\n        if ss_code:\n            candidates.append(ss_code)\n            \n        # Strategy 2: Raster position-based\n        raster_code = self.generate_raster_based_code(transformations)\n        if raster_code:\n            candidates.append(raster_code)\n            \n        # Strategy 3: Continuous model prediction\n        if self.continuous_models:\n            cont_code = self.generate_continuous_model_code(transformations)\n            if cont_code:\n                candidates.append(cont_code)\n                \n        # Strategy 4: Original strategies\n        pattern_code = self.generate_pattern_based_code(transformations)\n        if pattern_code:\n            candidates.append(pattern_code)\n            \n        feature_code = self.generate_feature_based_code(transformations)\n        if feature_code:\n            candidates.append(feature_code)\n            \n        template_code = self.generate_template_based_code(transformations)\n        if template_code:\n            candidates.append(template_code)\n            \n        # Select best candidate\n        if candidates:\n            # Return shortest valid code\n            return min(candidates, key=len)\n            \n        # Fallback\n        return \"def p(g):return g\"\n    \n    def analyze_enhanced_transformation(self, examples):\n        \"\"\"Enhanced transformation analysis with new features\"\"\"\n        transformations = []\n        \n        for ex in examples:\n            inp = np.array(ex['input'])\n            out = np.array(ex['output'])\n            \n            # Extract comprehensive features including new ones\n            inp_features = self.extract_all_features(inp)\n            out_features = self.extract_all_features(out)\n            \n            # Supersampled analysis\n            inp_ss2, _ = self.supersample_grid(inp, 2)\n            out_ss2, _ = self.supersample_grid(out, 2)\n            \n            # Raster analysis\n            inp_rasters = self.grid_to_raster_with_repetition(inp)\n            out_rasters = self.grid_to_raster_with_repetition(out)\n            \n            # Detect transformation type with enhanced detection\n            trans_type = self.detect_enhanced_transformation_type(inp, out, inp_ss2, out_ss2)\n            \n            transformations.append({\n                'input': inp,\n                'output': out,\n                'inp_features': inp_features,\n                'out_features': out_features,\n                'inp_supersampled': inp_ss2,\n                'out_supersampled': out_ss2,\n                'inp_rasters': inp_rasters,\n                'out_rasters': out_rasters,\n                'type': trans_type,\n                'params': self.extract_transformation_params(inp, out, trans_type)\n            })\n            \n        return transformations\n    \n    def detect_enhanced_transformation_type(self, inp, out, inp_ss, out_ss):\n        \"\"\"Enhanced transformation type detection\"\"\"\n        # Original detection\n        basic_type = self.detect_transformation_type(inp, out)\n        if basic_type != 'complex':\n            return basic_type\n            \n        # Check supersampled versions for patterns\n        if np.array_equal(out_ss, np.rot90(inp_ss)):\n            return 'rotate_90'\n            \n        # Check raster-based patterns\n        inp_raster = inp.flatten()\n        out_raster = out.flatten()\n        \n        # Check if it's a position-based transformation\n        if inp.shape == out.shape:\n            position_based = True\n            for i in range(inp.shape[0]):\n                for j in range(inp.shape[1]):\n                    # Check various position formulas\n                    expected_values = [\n                        (i + j) % 10,\n                        (i * j) % 10,\n                        (i - j) % 10,\n                        max(i, j) % 10,\n                        min(i, j) % 10\n                    ]\n                    if out[i,j] not in expected_values:\n                        position_based = False\n                        break\n                        \n            if position_based:\n                return 'position_based'\n                \n        return 'complex'\n    \n    def detect_transformation_type(self, inp, out):\n        \"\"\"Detect the type of transformation\"\"\"\n        # Size change\n        if inp.shape != out.shape:\n            if out.shape[0] > inp.shape[0] or out.shape[1] > inp.shape[1]:\n                return 'scale_up'\n            else:\n                return 'scale_down'\n                \n        # Check for simple transformations\n        if np.array_equal(out, np.rot90(inp, 1)):\n            return 'rotate_90'\n        elif np.array_equal(out, np.rot90(inp, 2)):\n            return 'rotate_180'\n        elif np.array_equal(out, np.rot90(inp, 3)):\n            return 'rotate_270'\n        elif np.array_equal(out, np.fliplr(inp)):\n            return 'flip_h'\n        elif np.array_equal(out, np.flipud(inp)):\n            return 'flip_v'\n        elif np.array_equal(out, inp.T):\n            return 'transpose'\n            \n        # Color operations\n        if len(np.unique(out)) == 1:\n            return 'fill_single'\n        elif self.is_color_mapping(inp, out):\n            return 'color_map'\n            \n        # Pattern operations\n        if self.is_pattern_fill(out):\n            return 'pattern_fill'\n            \n        return 'complex'\n    \n    def is_color_mapping(self, inp, out):\n        \"\"\"Check if transformation is a color mapping\"\"\"\n        if inp.shape != out.shape:\n            return False\n            \n        mapping = {}\n        for i in range(inp.shape[0]):\n            for j in range(inp.shape[1]):\n                if inp[i,j] in mapping:\n                    if mapping[inp[i,j]] != out[i,j]:\n                        return False\n                else:\n                    mapping[inp[i,j]] = out[i,j]\n                    \n        return len(mapping) > 0\n    \n    def is_pattern_fill(self, grid):\n        \"\"\"Check if grid is a pattern fill\"\"\"\n        # Check for checkerboard\n        h, w = grid.shape\n        checker = np.array([[(i+j)%2 for j in range(w)] for i in range(h)])\n        colors = np.unique(grid)\n        \n        if len(colors) == 2:\n            c1, c2 = colors\n            if np.array_equal(grid, np.where(checker, c1, c2)) or \\\n               np.array_equal(grid, np.where(checker, c2, c1)):\n                return True\n                \n        return False\n    \n    def extract_transformation_params(self, inp, out, trans_type):\n        \"\"\"Extract parameters for transformation\"\"\"\n        params = {}\n        \n        if trans_type == 'scale_up':\n            params['sy'] = out.shape[0] // inp.shape[0]\n            params['sx'] = out.shape[1] // inp.shape[1]\n        elif trans_type == 'scale_down':\n            params['sy'] = inp.shape[0] // out.shape[0]\n            params['sx'] = inp.shape[1] // out.shape[1]\n        elif trans_type == 'fill_single':\n            params['color'] = int(out[0,0])\n        elif trans_type == 'color_map':\n            mapping = {}\n            for i in range(inp.shape[0]):\n                for j in range(inp.shape[1]):\n                    mapping[int(inp[i,j])] = int(out[i,j])\n            params['mapping'] = mapping\n            \n        return params\n    \n    def matches_pattern(self, examples, pattern_name):\n        \"\"\"Check if examples match a known pattern\"\"\"\n        if pattern_name == 'rot90':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'rot180':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']), 2)) \n                      for ex in examples)\n        elif pattern_name == 'rot270':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.rot90(np.array(ex['input']), 3)) \n                      for ex in examples)\n        elif pattern_name == 'fliph':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.fliplr(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'flipv':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.flipud(np.array(ex['input']))) \n                      for ex in examples)\n        elif pattern_name == 'trans':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.array(ex['input']).T) \n                      for ex in examples)\n        elif pattern_name == 'id':\n            return all(np.array_equal(np.array(ex['output']), \n                                    np.array(ex['input'])) \n                      for ex in examples)\n            \n        # Check scaling\n        inp = np.array(examples[0]['input'])\n        out = np.array(examples[0]['output'])\n        \n        if pattern_name == 'scale2x':\n            return (out.shape[0] == inp.shape[0] * 2 and \n                   out.shape[1] == inp.shape[1] * 2)\n        elif pattern_name == 'scale3x':\n            return (out.shape[0] == inp.shape[0] * 3 and \n                   out.shape[1] == inp.shape[1] * 3)\n        elif pattern_name == 'half':\n            return (out.shape[0] == inp.shape[0] // 2 and \n                   out.shape[1] == inp.shape[1] // 2)\n            \n        return False\n    \n    def instantiate_pattern(self, template, examples):\n        \"\"\"Instantiate pattern template with specific values\"\"\"\n        # Get parameters from examples\n        inp = np.array(examples[0]['input'])\n        out = np.array(examples[0]['output'])\n        \n        # Replace placeholders\n        code = template\n        \n        # Color placeholders\n        if '{c}' in code:\n            color = int(out[0,0])\n            code = code.replace('{c}', str(color))\n            \n        # Size placeholders\n        if '{h}' in code:\n            code = code.replace('{h}', str(out.shape[0]))\n        if '{w}' in code:\n            code = code.replace('{w}', str(out.shape[1]))\n            \n        # Mapping placeholders\n        if '{m}' in code:\n            mapping = {}\n            for i in range(min(inp.shape[0], out.shape[0])):\n                for j in range(min(inp.shape[1], out.shape[1])):\n                    if inp[i,j] != out[i,j]:\n                        mapping[int(inp[i,j])] = int(out[i,j])\n            code = code.replace('{m}', str(mapping))\n            \n        # Swap placeholders\n        if '{a}' in code and '{b}' in code:\n            # Find swapped colors\n            colors_in = set(inp.flatten())\n            colors_out = set(out.flatten())\n            if len(colors_in) == 2 and colors_in == colors_out:\n                a, b = sorted(colors_in)\n                code = code.replace('{a}', str(a)).replace('{b}', str(b))\n                \n        return code\n    \n    def generate_pattern_based_code(self, transformations):\n        \"\"\"Generate code based on detected patterns\"\"\"\n        trans = transformations[0]\n        trans_type = trans['type']\n        params = trans['params']\n        \n        if trans_type == 'rotate_90':\n            return self.verified_patterns['rot90'][0]\n        elif trans_type == 'rotate_180':\n            return self.verified_patterns['rot180'][0]\n        elif trans_type == 'rotate_270':\n            return self.verified_patterns['rot270'][0]\n        elif trans_type == 'flip_h':\n            return self.verified_patterns['fliph'][0]\n        elif trans_type == 'flip_v':\n            return self.verified_patterns['flipv'][0]\n        elif trans_type == 'transpose':\n            return self.verified_patterns['trans'][0]\n        elif trans_type == 'scale_up':\n            sy, sx = params['sy'], params['sx']\n            return f\"def p(g):return[[g[i//{sy}][j//{sx}]for j in range(len(g[0])*{sx})]for i in range(len(g)*{sy})]\"\n        elif trans_type == 'scale_down':\n            sy, sx = params['sy'], params['sx']\n            return f\"def p(g):return[r[::{sx}]for r in g[::{sy}]]\"\n        elif trans_type == 'fill_single':\n            c = params['color']\n            return f\"def p(g):return[[{c}]*len(g[0])for _ in g]\"\n        elif trans_type == 'color_map':\n            m = params['mapping']\n            return f\"def p(g):m={m};return[[m.get(x,x)for x in r]for r in g]\"\n            \n        return None\n    \n    def generate_feature_based_code(self, transformations):\n        \"\"\"Generate code based on feature analysis\"\"\"\n        # Analyze feature differences\n        trans = transformations[0]\n        inp = trans['input']\n        out = trans['output']\n        \n        # Check for simple operations\n        if inp.shape == out.shape:\n            # Check if output is a function of position\n            is_position_based = True\n            for i in range(out.shape[0]):\n                for j in range(out.shape[1]):\n                    # Check various position-based patterns\n                    if out[i,j] == (i + j) % 10:\n                        continue\n                    elif out[i,j] == i % 10:\n                        continue\n                    elif out[i,j] == j % 10:\n                        continue\n                    else:\n                        is_position_based = False\n                        break\n                        \n            if is_position_based:\n                # Determine exact pattern\n                if all(out[i,j] == (i + j) % 10 for i in range(out.shape[0]) \n                      for j in range(out.shape[1])):\n                    return \"def p(g):return[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n                    \n        return None\n    \n    def generate_template_based_code(self, transformations):\n        \"\"\"Generate code using template matching\"\"\"\n        trans = transformations[0]\n        out = trans['output']\n        \n        # Check if output matches any simple template\n        h, w = out.shape\n        \n        # Single value\n        if len(np.unique(out)) == 1:\n            c = int(out[0,0])\n            return f\"def p(g):return[[{c}]*{w}for _ in range({h})]\"\n            \n        # Row pattern\n        if all(np.array_equal(out[i], out[0]) for i in range(h)):\n            row = out[0].tolist()\n            return f\"def p(g):return[{row}for _ in range({h})]\"\n            \n        # Column pattern\n        if all(np.array_equal(out[:,j], out[:,0]) for j in range(w)):\n            col = out[:,0].tolist()\n            return f\"def p(g):return[[{col[i]}]*{w}for i in range({h})]\"\n            \n        return None\n    \n    def generate_supersampling_based_code(self, transformations):\n        \"\"\"Generate code based on supersampling analysis\"\"\"\n        trans = transformations[0]\n        \n        # Check if transformation is clearer at higher resolution\n        inp_ss = trans['inp_supersampled']\n        out_ss = trans['out_supersampled']\n        \n        # Analyze patterns in supersampled space\n        if inp_ss.shape == out_ss.shape:\n            # Check for local patterns\n            pattern_found = True\n            pattern_type = None\n            \n            # Check 2x2 blocks\n            for i in range(0, inp_ss.shape[0]-1, 2):\n                for j in range(0, inp_ss.shape[1]-1, 2):\n                    block_in = inp_ss[i:i+2, j:j+2]\n                    block_out = out_ss[i:i+2, j:j+2]\n                    \n                    # Detect block transformation\n                    if np.all(block_out == block_in[0,0]):\n                        pattern_type = 'fill_blocks'\n                    elif np.all(block_out == np.rot90(block_in)):\n                        pattern_type = 'rotate_blocks'\n                    else:\n                        pattern_found = False\n                        break\n                        \n            if pattern_found and pattern_type == 'fill_blocks':\n                return \"def p(g):return[[g[i//2][j//2]for j in range(len(g[0])*2)]for i in range(len(g)*2)][:len(g)][:len(g[0])]\"\n                \n        return None\n    \n    def generate_raster_based_code(self, transformations):\n        \"\"\"Generate code based on raster position analysis\"\"\"\n        trans = transformations[0]\n        inp = trans['input']\n        out = trans['output']\n        \n        if inp.shape != out.shape:\n            return None\n            \n        # Analyze raster transformations\n        inp_raster = trans['inp_rasters'][0]  # Row-major\n        out_raster = trans['out_rasters'][0]\n        \n        # Check for position-based mapping\n        h, w = inp.shape\n        \n        # Check if output depends on raster position\n        position_map = {}\n        for i in range(h):\n            for j in range(w):\n                raster_pos = i * w + j\n                if raster_pos not in position_map:\n                    position_map[raster_pos] = out[i,j]\n                elif position_map[raster_pos] != out[i,j]:\n                    # Not a simple position mapping\n                    break\n                    \n        # Generate code if pattern found\n        if len(position_map) == h * w:\n            # Check for simple formulas\n            if all(position_map[i] == i % 10 for i in range(len(position_map))):\n                return f\"def p(g):h,w=len(g),len(g[0]);return[[(i*w+j)%10 for j in range(w)]for i in range(h)]\"\n                \n        return None\n    \n    def generate_continuous_model_code(self, transformations):\n        \"\"\"Generate code using continuous model predictions\"\"\"\n        # This would use the trained continuous models\n        # For now, return None as models need to be trained on full dataset\n        return None\n    \n    def generate_optimized_code(self, task_data):\n        \"\"\"Generate highly optimized code for the task using enhanced techniques\"\"\"\n        return self.generate_enhanced_code(task_data)\n\ndef create_enhanced_ultra_advanced_submission():\n    \"\"\"Create submission using enhanced ultra-advanced approach\"\"\"\n    solver = EnhancedUltraAdvancedARCSolver()\n    solutions = {}\n    \n    print(\" Generating Enhanced Ultra-Advanced Neural ARC Solutions...\")\n    print(\" Using supersampling, continuous learning, and position-based features\")\n    print(\"=\" * 70)\n    \n    successful = 0\n    total_bytes = 0\n    pattern_usage = defaultdict(int)\n    \n    # Optional: Pre-train continuous models on a subset of tasks\n    # This would require loading multiple tasks for training\n    # solver.train_continuous_models(training_tasks)\n    \n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        task_file = f\"/kaggle/input/google-code-golf-2025/task{task_id}.json\"\n        \n        try:\n            with open(task_file) as f:\n                task_data = json.load(f)\n                \n            # Generate optimized code with enhanced techniques\n            code = solver.generate_optimized_code(task_data)\n            solutions[task_id] = code\n            \n            # Track pattern usage\n            for pattern_name, (pattern_code, _) in solver.verified_patterns.items():\n                if code == pattern_code:\n                    pattern_usage[pattern_name] += 1\n                    break\n                    \n            # Verify solution\n            try:\n                namespace = {}\n                exec(code, namespace)\n                \n                # Test on examples\n                valid = True\n                for ex in task_data['train'][:3]:\n                    p = namespace['p']\n                    result = p([row[:] for row in ex['input']])\n                    if result != ex['output']:\n                        valid = False\n                        break\n                        \n                if valid:\n                    successful += 1\n                    status = \"\"\n                else:\n                    status = \"\"\n            except:\n                status = \"\"\n                \n            bytes_count = len(code)\n            total_bytes += bytes_count\n            \n            if task_num % 25 == 0:\n                print(f\"Progress: {task_num}/400 | Success rate: {successful/task_num:.1%} | \"\n                      f\"Avg bytes: {total_bytes/task_num:.1f}\")\n                \n        except Exception as e:\n            code = \"def p(g):return g\"\n            solutions[task_id] = code\n            total_bytes += len(code)\n            \n    print(f\"\\n{'='*70}\")\n    print(f\" Completed: {successful}/400 valid solutions ({successful/400:.1%})\")\n    print(f\" Total bytes: {total_bytes:,}\")\n    print(f\" Average bytes per solution: {total_bytes/400:.1f}\")\n    \n    print(f\"\\n Pattern Usage Statistics:\")\n    for pattern, count in sorted(pattern_usage.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  {pattern}: {count} times\")\n        \n    # Create submission\n    os.makedirs(\"submission\", exist_ok=True)\n    \n    for task_id, code in solutions.items():\n        with open(f\"submission/task{task_id}.py\", \"w\") as f:\n            f.write(code)\n            \n    with zipfile.ZipFile(\"submission.zip\", \"w\") as zipf:\n        for task_id in solutions:\n            zipf.write(f\"submission/task{task_id}.py\", f\"task{task_id}.py\")\n            \n    print(f\"\\n Enhanced Ultra-Advanced submission created: submission.zip\")\n    \n    # Show example solutions\n    print(f\"\\n Example solutions:\")\n    for i, (task_id, code) in enumerate(list(solutions.items())[:5]):\n        print(f\"\\nTask {task_id}:\")\n        print(code if len(code) <= 100 else code[:100] + \"...\")\n        \n    return solutions\n\nif __name__ == \"__main__\":\n    create_enhanced_ultra_advanced_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}