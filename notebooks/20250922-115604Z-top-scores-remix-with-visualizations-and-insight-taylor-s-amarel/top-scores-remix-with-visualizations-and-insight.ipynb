{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13472782,"sourceType":"competition"},{"sourceId":13054335,"sourceType":"datasetVersion","datasetId":8252851},{"sourceId":13099530,"sourceType":"datasetVersion","datasetId":8295919},{"sourceId":262833225,"sourceType":"kernelVersion"},{"sourceId":262885685,"sourceType":"kernelVersion"},{"sourceId":263000792,"sourceType":"kernelVersion"},{"sourceId":263053035,"sourceType":"kernelVersion"},{"sourceId":263274445,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Enhanced Code Golf Solution Analyzer with Multiple Visualizations\n# Install required packages\nimport subprocess\nimport sys\n\nprint(\"ðŸ”§ Installing required packages...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n                      \"python-minifier\", \"matplotlib\", \"seaborn\", \"tqdm\", \n                      \"numpy\", \"pandas\", \"scipy\", \"plotly\", \"scikit-learn\", \n                      \"wordcloud\", \"networkx\"])\n\nimport os\nfrom python_minifier import minify\nimport warnings\nfrom zlib import compress\nimport zipfile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\nimport time\nimport tempfile\nimport shutil\nfrom scipy import stats as scipy_stats\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport json\nfrom wordcloud import WordCloud\nimport networkx as nx\nfrom datetime import datetime\nimport re\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nwarnings.filterwarnings(\"ignore\")\n\n# ASCII Art Header\nprint(\"\\n\" + \"=\"*100)\nprint(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘     ðŸ”¬ ULTRA-ENHANCED CODE GOLF SOLUTION ANALYZER & SUBMISSION OPTIMIZER ðŸ”¬                  â•‘\nâ•‘                                                                                               â•‘\nâ•‘     Advanced Analytics â€¢ Multi-Source Comparison â€¢ Performance Insights                       â•‘\nâ•‘     Final Submission: golf-code-optimization-trials                                          â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")\nprint(\"=\"*100)\n\n# The PROVEN submission to use for final output - Using optimization trials\nPROVEN_SUBMISSION = \"/kaggle/input/golf-code-optimization-trials/submission.zip\"\n\n# Source directories for ANALYSIS\nanalysis_sources = [\n    \"/kaggle/input/arc-code-golf-starter-solutions\",\n    # Author: https://www.kaggle.com/cristianocalcagno\n    \"/kaggle/input/google-code-golf-2025-submit\",\n    # Author: https://www.kaggle.com/tonylica\n    \"/kaggle/input/source-code-for-full-400-solutions\",\n    # Full 400 solutions\n    \"/kaggle/input/400-task-with-smart-solution-search-verification/source/big-zippa\",\n    # Alternative solution for comparison\n    \"/kaggle/input/neuroips-4-of-some-lessons-learned\",\n    # Neuroips-4 lessons learned\n    \"/kaggle/input/solutions-for-all-400-tasks-ensemble\",\n    # Ensemble solution for comparison\n    \"/kaggle/input/golf-code-optimization-trials\",\n    # Optimization trials - USED FOR SUBMISSION\n]\n\n# Known failing solutions to skip in analysis\nfailing = [\n    \"/kaggle/input/google-code-golf-2025-submit/task071.py\",\n    \"/kaggle/input/google-code-golf-2025-submit/task074.py\", \n    \"/kaggle/input/google-code-golf-2025-submit/task343.py\",\n]\n\nprint(f\"\\nðŸ“ ANALYSIS CONFIGURATION:\")\nprint(f\"   ðŸ“Š Sources for analysis: {len(analysis_sources)}\")\nfor i, source in enumerate(analysis_sources, 1):\n    source_name = os.path.basename(source)\n    if \"golf-code-optimization-trials\" in source:\n        print(f\"   {i}. ðŸŒŸ {source_name} [SUBMISSION SOURCE]\")\n    else:\n        print(f\"   {i}. ðŸ“‚ {source_name}\")\n\nprint(f\"\\nâœ… Final submission will use: {os.path.basename(PROVEN_SUBMISSION)}\")\nprint(f\"âŒ Failing solutions to skip: {len(failing)} files\")\nprint(f\"ðŸ“… Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Function to extract submission.zip if needed\ndef extract_submission_if_needed(source_path, for_analysis=True):\n    \"\"\"\n    Check if submission.zip exists in the source path and extract it if needed\n    Returns the path to use for loading .py files\n    \"\"\"\n    submission_zip = os.path.join(source_path, \"submission.zip\")\n    \n    # First check if .py files exist directly\n    if os.path.exists(source_path):\n        has_py_files = any(f.endswith('.py') for f in os.listdir(source_path) if os.path.isfile(os.path.join(source_path, f)))\n        \n        if has_py_files:\n            if for_analysis:\n                print(f\"   âœ… Found .py files directly in {os.path.basename(source_path)}\")\n            return source_path\n        elif os.path.exists(submission_zip):\n            if for_analysis:\n                print(f\"   ðŸ“¦ Found submission.zip in {os.path.basename(source_path)}, extracting for analysis...\")\n            # Create a temporary directory for extraction\n            temp_dir = tempfile.mkdtemp()\n            with zipfile.ZipFile(submission_zip, 'r') as zipf:\n                zipf.extractall(temp_dir)\n            if for_analysis:\n                print(f\"   ðŸ“¤ Extracted to temporary directory\")\n            return temp_dir\n        else:\n            if for_analysis:\n                print(f\"   âš ï¸ No .py files or submission.zip found in {os.path.basename(source_path)}\")\n            return None\n    else:\n        if for_analysis:\n            print(f\"   âš ï¸ Source not found: {source_path}\")\n        return None\n\n# Process sources for ANALYSIS\nprocessed_sources = []\ntemp_dirs = []  # Keep track of temp directories to clean up later\n\nprint(\"\\nðŸ” PROCESSING SOURCE DIRECTORIES FOR ANALYSIS...\")\nprint(\"-\" * 60)\nfor source in analysis_sources:\n    result = extract_submission_if_needed(source)\n    if result:\n        processed_sources.append(result)\n        if result != source:  # It's a temp directory\n            temp_dirs.append(result)\n\nanalysis_sources = processed_sources\nprint(f\"\\nâœ… Successfully processed {len(analysis_sources)} sources for analysis\")\n\ndef calculate_zlib_compression(src):\n    \"\"\"Calculate what zlib compression would achieve (for analysis only)\"\"\"\n    compression_level = 9\n    \n    # Avoid compressed data ending with quote character\n    while (compressed := compress(src, compression_level))[-1] == ord('\"'): \n        src += b\"#\"\n    \n    def sanitize(b_in):\n        b_out = bytearray()\n        for b in b_in:\n            if   b==0:         b_out += b\"\\\\x00\"\n            elif b==ord(\"\\r\"): b_out += b\"\\\\r\"\n            elif b==ord(\"\\\\\"): b_out += b\"\\\\\\\\\"\n            else: b_out.append(b)\n        return bytes(b_out)\n    \n    compressed = sanitize(compressed)\n    delim = b'\"\"\"' if ord(\"\\n\") in compressed or ord('\"') in compressed else b'\"'\n    \n    return b\"#coding:L1\\nimport zlib\\nexec(zlib.decompress(bytes(\" + \\\n           delim + compressed + delim + \\\n           b',\"L1\")))'\n\ndef analyze_code_complexity(code):\n    \"\"\"Analyze code complexity metrics\"\"\"\n    lines = code.split('\\n')\n    metrics = {\n        'line_count': len(lines),\n        'char_count': len(code),\n        'import_count': sum(1 for line in lines if line.strip().startswith('import') or line.strip().startswith('from')),\n        'function_count': sum(1 for line in lines if line.strip().startswith('def ')),\n        'class_count': sum(1 for line in lines if line.strip().startswith('class ')),\n        'loop_count': sum(1 for line in lines if 'for ' in line or 'while ' in line),\n        'comprehension_count': code.count('[') + code.count('{') - code.count('{}') - code.count('[]'),\n        'lambda_count': code.count('lambda'),\n        'avg_line_length': np.mean([len(line) for line in lines]) if lines else 0,\n        'max_line_length': max([len(line) for line in lines]) if lines else 0,\n        'empty_lines': sum(1 for line in lines if not line.strip()),\n        'comment_lines': sum(1 for line in lines if line.strip().startswith('#')),\n        'nested_loops': sum(1 for i, line in enumerate(lines) if 'for ' in line and i > 0 and \n                           any('for ' in lines[j] or 'while ' in lines[j] for j in range(max(0, i-5), i))),\n        'unique_vars': len(set(re.findall(r'\\b[a-zA-Z_]\\w*\\b', code))),\n        'operators': sum(code.count(op) for op in ['+', '-', '*', '/', '%', '**', '//', '==', '!=', '<', '>', '<=', '>=']),\n    }\n    return metrics\n\ndef analyze_code_patterns(code):\n    \"\"\"Analyze common code patterns and techniques\"\"\"\n    patterns = {\n        'list_comp': len(re.findall(r'\\[.*for.*in.*\\]', code)),\n        'dict_comp': len(re.findall(r'\\{.*for.*in.*\\}', code)),\n        'generator': code.count('yield'),\n        'decorators': code.count('@'),\n        'f_strings': len(re.findall(r'f[\"\\']', code)),\n        'walrus': code.count(':='),\n        'ternary': len(re.findall(r'.*if.*else.*', code)),\n        'map_usage': code.count('map('),\n        'filter_usage': code.count('filter('),\n        'reduce_usage': code.count('reduce('),\n        'lambda_usage': code.count('lambda'),\n        'enumerate_usage': code.count('enumerate('),\n        'zip_usage': code.count('zip('),\n        'range_usage': code.count('range('),\n        'slice_usage': len(re.findall(r'\\[.*:.*\\]', code)),\n    }\n    return patterns\n\n# Initialize enhanced tracking variables\nstats = {\n    'task_nums': [],\n    'original_sizes': [],\n    'minified_sizes': [],\n    'compressed_sizes': [],\n    'final_sizes': [],\n    'improvements': [],\n    'compression_ratios': [],\n    'potential_scores': [],\n    'source_used': [],\n    'compression_beneficial': [],\n    'all_source_sizes': [],\n    'complexity_metrics': [],\n    'minification_reduction': [],\n    'task_difficulty': [],\n    'best_combination_potential': [],\n    'source_availability': [],\n    'code_patterns': [],\n    'efficiency_score': [],\n    'optimization_potential': [],\n}\n\n# Track additional metrics\nsource_wins = defaultdict(int)\nsource_task_coverage = defaultdict(set)\ntask_source_comparison = defaultdict(dict)\ntask_complexity_data = defaultdict(dict)\nsource_pair_wins = defaultdict(int)\ntask_patterns = defaultdict(list)\npattern_frequency = defaultdict(int)\noptimization_techniques = defaultdict(list)\nsource_performance_timeline = defaultdict(list)\ntask_size_progression = []\n\n# ANALYSIS PHASE - Analyze all sources\nprint(\"\\n\" + \"=\"*100)\nprint(\"ðŸ”¬ COMPREHENSIVE ANALYSIS PHASE\")\nprint(\"=\"*100)\nprint(\"\\nAnalyzing all available solutions with ultra-enhanced metrics...\")\nprint(\"This analysis will provide deep insights into:\")\nprint(\"  â€¢ Code complexity and efficiency\")\nprint(\"  â€¢ Compression opportunities\")\nprint(\"  â€¢ Source performance comparison\")\nprint(\"  â€¢ Optimization patterns and techniques\")\nprint(\"  â€¢ Task difficulty classification\")\n\nstart_time = time.time()\ntasks_analyzed = 0\ntotal_code_analyzed = 0\n\nfor task_num in tqdm(range(1, 401), desc=\"ðŸ” Analyzing tasks\", unit=\"task\", ncols=100):\n    \n    # Analyze all sources for this task\n    best_src = b\"#\" * 1_000_000\n    original_size = 0\n    best_source = \"none\"\n    task_sizes_by_source = {}\n    available_sources = []\n    task_code_patterns = {}\n    \n    for source in analysis_sources:\n        path_in = f\"{source}/task{task_num:03d}.py\"\n        if not os.path.exists(path_in): continue\n        if path_in in failing: continue\n        \n        try:\n            with open(path_in, \"r\") as task_in:\n                new_src = task_in.read()\n                current_original_size = len(new_src.encode())\n                total_code_analyzed += current_original_size\n                minified = minify(new_src).encode()\n                \n                # Analyze complexity and patterns\n                complexity = analyze_code_complexity(new_src)\n                patterns = analyze_code_patterns(new_src)\n                \n                # Choose shortest version: original vs minified\n                current_best = min([new_src.encode(), minified], key=len)\n                \n                # Determine source name\n                if \"arc-code-golf-starter\" in source:\n                    source_name = \"arc-starter\"\n                elif \"google-code-golf-2025\" in source:\n                    source_name = \"google-submit\"\n                elif \"source-code-for-full-400\" in source:\n                    source_name = \"full-400\"\n                elif \"big-zippa\" in source:\n                    source_name = \"big-zippa\"\n                elif \"neuroips-4-of-some-lessons-learned\" in source:\n                    source_name = \"neuroips-4\"\n                elif \"ensemble\" in source:\n                    source_name = \"ensemble\"\n                elif \"golf-code-optimization-trials\" in source:\n                    source_name = \"optimization-trials\"\n                else:\n                    source_name = os.path.basename(source)\n                \n                # Track coverage and metrics\n                source_task_coverage[source_name].add(task_num)\n                task_sizes_by_source[source_name] = len(current_best)\n                task_source_comparison[task_num][source_name] = len(current_best)\n                task_complexity_data[task_num][source_name] = complexity\n                task_code_patterns[source_name] = patterns\n                available_sources.append(source_name)\n                \n                # Track pattern frequency\n                for pattern, count in patterns.items():\n                    if count > 0:\n                        pattern_frequency[pattern] += count\n                \n                # Check if this is the best so far\n                if len(current_best) < len(best_src):\n                    best_src = current_best\n                    original_size = current_original_size\n                    best_source = source_name\n                    best_complexity = complexity\n                    best_patterns = patterns\n                    \n        except Exception as e:\n            continue\n    \n    # Skip if no valid source found\n    if len(best_src) == 1_000_000:\n        continue\n    \n    tasks_analyzed += 1\n    task_size_progression.append((task_num, len(best_src)))\n    \n    # Track winner\n    if best_source != \"none\":\n        source_wins[best_source] += 1\n        source_performance_timeline[best_source].append(task_num)\n        \n    # Analyze compression potential\n    compressed_src = calculate_zlib_compression(best_src)\n    improvement = len(best_src) - len(compressed_src)\n    compression_beneficial = improvement > 0\n    \n    # Determine what would be used\n    final_src = compressed_src if compression_beneficial else best_src\n    \n    # Calculate potential score\n    potential_score = max(1, 2500 - len(final_src))\n    \n    # Calculate minification reduction\n    minification_reduction = (original_size - len(best_src)) / original_size * 100 if original_size > 0 else 0\n    \n    # Calculate efficiency score (custom metric)\n    efficiency_score = potential_score / (best_complexity.get('line_count', 1) + 1)\n    \n    # Calculate optimization potential\n    if len(task_sizes_by_source) > 1:\n        size_variance = np.var(list(task_sizes_by_source.values()))\n        optimization_potential = size_variance / np.mean(list(task_sizes_by_source.values())) if task_sizes_by_source else 0\n    else:\n        optimization_potential = 0\n    \n    # Estimate task difficulty based on multiple factors\n    if len(final_src) < 100 and best_complexity.get('line_count', 0) < 5:\n        difficulty = \"Easy\"\n    elif len(final_src) < 300 and best_complexity.get('line_count', 0) < 15:\n        difficulty = \"Medium\"\n    elif len(final_src) < 600 and best_complexity.get('line_count', 0) < 30:\n        difficulty = \"Hard\"\n    else:\n        difficulty = \"Expert\"\n    \n    # Calculate best combination potential (theoretical best from all sources)\n    if len(task_sizes_by_source) > 1:\n        best_combination = min(task_sizes_by_source.values())\n        combination_potential = len(final_src) - best_combination\n    else:\n        combination_potential = 0\n    \n    # Record optimization techniques used\n    if 'best_patterns' in locals():\n        for pattern, count in best_patterns.items():\n            if count > 0:\n                optimization_techniques[pattern].append(task_num)\n    \n    # Record statistics\n    stats['task_nums'].append(task_num)\n    stats['original_sizes'].append(original_size)\n    stats['minified_sizes'].append(len(best_src))\n    stats['compressed_sizes'].append(len(compressed_src))\n    stats['final_sizes'].append(len(final_src))\n    stats['improvements'].append(max(0, improvement))\n    stats['compression_ratios'].append(len(compressed_src) / len(best_src) if len(best_src) > 0 else 1.0)\n    stats['potential_scores'].append(potential_score)\n    stats['source_used'].append(best_source)\n    stats['compression_beneficial'].append(compression_beneficial)\n    stats['all_source_sizes'].append(task_sizes_by_source)\n    stats['complexity_metrics'].append(best_complexity if 'best_complexity' in locals() else {})\n    stats['minification_reduction'].append(minification_reduction)\n    stats['task_difficulty'].append(difficulty)\n    stats['best_combination_potential'].append(combination_potential)\n    stats['source_availability'].append(len(available_sources))\n    stats['code_patterns'].append(task_code_patterns)\n    stats['efficiency_score'].append(efficiency_score)\n    stats['optimization_potential'].append(optimization_potential)\n    \n    # Track patterns\n    task_patterns[task_num // 50].append(len(final_src))\n\nanalysis_time = time.time() - start_time\n\n# Calculate source synergies\nprint(\"\\nðŸ”„ Calculating source synergies and optimization opportunities...\")\nfor task_num, sources in task_source_comparison.items():\n    if len(sources) >= 2:\n        sorted_sources = sorted(sources.items(), key=lambda x: x[1])\n        if len(sorted_sources) >= 2:\n            source_pair_wins[f\"{sorted_sources[0][0]} + {sorted_sources[1][0]}\"] += 1\n\n# Clean up temporary directories from analysis\nfor temp_dir in temp_dirs:\n    shutil.rmtree(temp_dir)\nprint(f\"ðŸ§¹ Cleaned up {len(temp_dirs)} temporary directories\")\n\n# SUBMISSION PHASE - Copy the optimization-trials submission\nprint(\"\\n\" + \"=\"*100)\nprint(\"ðŸ“¦ SUBMISSION PHASE: Using Golf Code Optimization Trials Solution\")\nprint(\"=\"*100)\n\nsubmission_path = \"/kaggle/working/submission.zip\"\n\nif os.path.exists(PROVEN_SUBMISSION):\n    print(f\"âœ… Found optimization trials submission: submission.zip\")\n    print(f\"ðŸ“‹ Copying to: {submission_path}\")\n    shutil.copy2(PROVEN_SUBMISSION, submission_path)\n    submission_size = os.path.getsize(submission_path)\n    print(f\"âœ… Submission copied successfully!\")\n    print(f\"ðŸ“¦ Final submission size: {submission_size:,} bytes ({submission_size/1024:.1f} KB)\")\n    print(f\"ðŸ“Š Size per task average: {submission_size/400:.1f} bytes\")\nelse:\n    print(f\"âŒ ERROR: Optimization trials submission not found at {PROVEN_SUBMISSION}\")\n    print(f\"   Please check the path and try again.\")\n    submission_size = 0\n\n# ENHANCED ANALYTICS PHASE\nprint(\"\\n\" + \"=\"*100)\nprint(\"ðŸ“Š ULTRA-COMPREHENSIVE ANALYSIS RESULTS\")\nprint(\"=\"*100)\n\ndf = pd.DataFrame({k: v for k, v in stats.items() if k not in ['all_source_sizes', 'complexity_metrics', 'code_patterns']})\nanalyzed_tasks = len(df)\ncompression_beneficial_count = sum(stats['compression_beneficial'])\ntotal_potential_score = sum(stats['potential_scores'])\ntotal_bytes_saved = sum(stats['improvements'])\n\n# Create comprehensive summary\nprint(f\"\\nðŸŽ¯ EXECUTIVE SUMMARY\")\nprint(\"â”€\" * 60)\nprint(f\"â±ï¸  Analysis Performance:\")\nprint(f\"    â€¢ Total time: {analysis_time:.2f} seconds\")\nprint(f\"    â€¢ Per-task average: {analysis_time/400:.3f} seconds\")\nprint(f\"    â€¢ Code analyzed: {total_code_analyzed:,} bytes ({total_code_analyzed/1024/1024:.2f} MB)\")\nprint(f\"    â€¢ Analysis speed: {total_code_analyzed/analysis_time/1024:.1f} KB/sec\")\n\nprint(f\"\\nðŸ“‹ Task Coverage:\")\nprint(f\"    â€¢ Tasks analyzed: {tasks_analyzed}/400 ({tasks_analyzed/400*100:.1f}%)\")\nprint(f\"    â€¢ Sources compared: {len(analysis_sources)}\")\nprint(f\"    â€¢ Total comparisons: {tasks_analyzed * len(analysis_sources)}\")\n\nprint(f\"\\nðŸ—œï¸  Compression Analysis:\")\nprint(f\"    â€¢ Compression beneficial: {compression_beneficial_count}/{analyzed_tasks} tasks\")\nprint(f\"    â€¢ Success rate: {compression_beneficial_count/analyzed_tasks*100:.1f}%\")\nprint(f\"    â€¢ Total bytes saved: {total_bytes_saved:,} bytes\")\nprint(f\"    â€¢ Average savings: {total_bytes_saved/compression_beneficial_count:.1f} bytes/task (when beneficial)\")\n\nprint(f\"\\nðŸŽ¯ Scoring Potential:\")\nprint(f\"    â€¢ Total potential score: {total_potential_score:,} points\")\nprint(f\"    â€¢ Average score per task: {total_potential_score/analyzed_tasks:.1f} points\")\nprint(f\"    â€¢ Maximum possible score: {2500 * analyzed_tasks:,} points\")\nprint(f\"    â€¢ Score efficiency: {total_potential_score/(2500*analyzed_tasks)*100:.1f}%\")\n\n# Advanced statistics\nprint(f\"\\nðŸ“ˆ ADVANCED SIZE STATISTICS:\")\nprint(\"â”€\" * 60)\nprint(f\"Original sizes:\")\nprint(f\"    â€¢ Mean: {np.mean(stats['original_sizes']):.0f} bytes Â± {np.std(stats['original_sizes']):.0f}\")\nprint(f\"    â€¢ Median: {np.median(stats['original_sizes']):.0f} bytes\")\nprint(f\"    â€¢ Mode: {scipy_stats.mode(stats['original_sizes'], keepdims=True)[0][0]:.0f} bytes\")\nprint(f\"    â€¢ Range: [{min(stats['original_sizes'])}, {max(stats['original_sizes'])}]\")\nprint(f\"    â€¢ IQR: Q1={np.percentile(stats['original_sizes'], 25):.0f}, Q3={np.percentile(stats['original_sizes'], 75):.0f}\")\nprint(f\"    â€¢ Skewness: {scipy_stats.skew(stats['original_sizes']):.2f}\")\nprint(f\"    â€¢ Kurtosis: {scipy_stats.kurtosis(stats['original_sizes']):.2f}\")\n\nprint(f\"\\nFinal sizes:\")\nprint(f\"    â€¢ Mean: {np.mean(stats['final_sizes']):.0f} bytes Â± {np.std(stats['final_sizes']):.0f}\")\nprint(f\"    â€¢ Median: {np.median(stats['final_sizes']):.0f} bytes\")\nprint(f\"    â€¢ Mode: {scipy_stats.mode(stats['final_sizes'], keepdims=True)[0][0]:.0f} bytes\")\nprint(f\"    â€¢ Range: [{min(stats['final_sizes'])}, {max(stats['final_sizes'])}]\")\nprint(f\"    â€¢ IQR: Q1={np.percentile(stats['final_sizes'], 25):.0f}, Q3={np.percentile(stats['final_sizes'], 75):.0f}\")\n\nprint(f\"\\nSize reduction statistics:\")\ntotal_reduction = sum(stats['original_sizes']) - sum(stats['final_sizes'])\nprint(f\"    â€¢ Total reduction: {total_reduction:,} bytes\")\nprint(f\"    â€¢ Average reduction: {total_reduction/analyzed_tasks:.1f} bytes/task\")\nprint(f\"    â€¢ Reduction percentage: {total_reduction/sum(stats['original_sizes'])*100:.1f}%\")\n\nprint(f\"\\nMinification effectiveness:\")\nprint(f\"    â€¢ Average reduction: {np.mean(stats['minification_reduction']):.1f}%\")\nprint(f\"    â€¢ Median reduction: {np.median(stats['minification_reduction']):.1f}%\")\nprint(f\"    â€¢ Max reduction: {np.max(stats['minification_reduction']):.1f}%\")\nprint(f\"    â€¢ Tasks with >50% reduction: {sum(1 for r in stats['minification_reduction'] if r > 50)}\")\n\n# Task difficulty distribution\ndifficulty_counts = pd.Series(stats['task_difficulty']).value_counts()\nprint(f\"\\nðŸŽ¯ TASK DIFFICULTY DISTRIBUTION:\")\nprint(\"â”€\" * 60)\nfor difficulty, count in difficulty_counts.items():\n    difficulty_emoji = {'Easy': 'ðŸŸ¢', 'Medium': 'ðŸŸ¡', 'Hard': 'ðŸ”´', 'Expert': 'ðŸŸ£'}.get(difficulty, 'âšª')\n    bar_length = int(count / analyzed_tasks * 50)\n    bar = 'â–ˆ' * bar_length + 'â–‘' * (50 - bar_length)\n    print(f\"    {difficulty_emoji} {difficulty:8s}: {bar} {count:3d} ({count/analyzed_tasks*100:5.1f}%)\")\n\n# Source performance analysis\nprint(f\"\\nðŸ† SOURCE PERFORMANCE ANALYSIS:\")\nprint(\"â”€\" * 60)\nsource_counts = pd.Series(stats['source_used']).value_counts()\nfor i, (source, count) in enumerate(source_counts.items(), 1):\n    medal = {1: 'ðŸ¥‡', 2: 'ðŸ¥ˆ', 3: 'ðŸ¥‰'}.get(i, '  ')\n    bar_length = int(count / analyzed_tasks * 40)\n    bar = 'â–ˆ' * bar_length + 'â–‘' * (40 - bar_length)\n    # Special marking for optimization-trials\n    if source == \"optimization-trials\":\n        print(f\"    {medal} {source:20s}: {bar} {count:3d} wins ({count/analyzed_tasks*100:5.1f}%) ðŸŒŸ SUBMISSION\")\n    else:\n        print(f\"    {medal} {source:20s}: {bar} {count:3d} wins ({count/analyzed_tasks*100:5.1f}%)\")\n\n# Source availability analysis\nprint(f\"\\nðŸ“Š SOURCE AVAILABILITY ANALYSIS:\")\nprint(\"â”€\" * 60)\navg_sources_per_task = np.mean(stats['source_availability'])\nprint(f\"    â€¢ Average sources available per task: {avg_sources_per_task:.2f}\")\nprint(f\"    â€¢ Tasks with all {len(analysis_sources)} sources: {sum(1 for s in stats['source_availability'] if s == len(analysis_sources))}\")\nprint(f\"    â€¢ Tasks with only 1 source: {sum(1 for s in stats['source_availability'] if s == 1)}\")\nprint(f\"    â€¢ Most common availability: {scipy_stats.mode(stats['source_availability'], keepdims=True)[0][0]} sources\")\n\n# Code pattern analysis\nprint(f\"\\nðŸ” CODE PATTERN ANALYSIS (Top Techniques):\")\nprint(\"â”€\" * 60)\nsorted_patterns = sorted(pattern_frequency.items(), key=lambda x: x[1], reverse=True)[:10]\nfor pattern, frequency in sorted_patterns:\n    pattern_name = pattern.replace('_', ' ').title()\n    print(f\"    â€¢ {pattern_name:20s}: {frequency:4d} occurrences\")\n\n# Optimization insights\nprint(f\"\\nðŸ’¡ OPTIMIZATION INSIGHTS:\")\nprint(\"â”€\" * 60)\nhigh_potential_tasks = [i for i, p in enumerate(stats['optimization_potential']) if p > np.percentile(stats['optimization_potential'], 75)]\nprint(f\"    â€¢ Tasks with high optimization potential: {len(high_potential_tasks)}\")\nprint(f\"    â€¢ Average efficiency score: {np.mean(stats['efficiency_score']):.2f}\")\nprint(f\"    â€¢ Best efficiency score: {np.max(stats['efficiency_score']):.2f}\")\nif high_potential_tasks:\n    task_list = ', '.join([f'Task {stats[\"task_nums\"][i]:03d}' for i in high_potential_tasks[:5]])\n    print(f\"    â€¢ Top optimization candidates: {task_list}\")\n\n# =============================================================================\n# MULTIPLE VISUALIZATIONS PHASE\n# =============================================================================\nprint(\"\\n\" + \"=\"*100)\nprint(\"ðŸ“Š GENERATING MULTIPLE ENHANCED VISUALIZATIONS\")\nprint(\"=\"*100)\n\n# 1. SIZE DISTRIBUTION ANALYSIS\nprint(\"\\nðŸ“Š Creating Size Distribution Analysis...\")\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Code Golf Size Distribution Analysis', fontsize=16, fontweight='bold')\n\n# Original size distribution\naxes[0,0].hist(stats['original_sizes'], bins=50, alpha=0.7, color='#3498db', edgecolor='black')\naxes[0,0].axvline(np.mean(stats['original_sizes']), color='red', linestyle='--', linewidth=2)\naxes[0,0].axvline(np.median(stats['original_sizes']), color='green', linestyle='--', linewidth=2)\naxes[0,0].set_xlabel('Original Size (bytes)')\naxes[0,0].set_ylabel('Frequency')\naxes[0,0].set_title('Original Size Distribution')\naxes[0,0].legend(['Mean', 'Median', 'Data'])\n\n# Minified size distribution\naxes[0,1].hist(stats['minified_sizes'], bins=50, alpha=0.7, color='#9b59b6', edgecolor='black')\naxes[0,1].axvline(np.mean(stats['minified_sizes']), color='red', linestyle='--', linewidth=2)\naxes[0,1].axvline(np.median(stats['minified_sizes']), color='green', linestyle='--', linewidth=2)\naxes[0,1].set_xlabel('Minified Size (bytes)')\naxes[0,1].set_ylabel('Frequency')\naxes[0,1].set_title('Minified Size Distribution')\n\n# Final size distribution\naxes[0,2].hist(stats['final_sizes'], bins=50, alpha=0.7, color='#2ecc71', edgecolor='black')\naxes[0,2].axvline(np.mean(stats['final_sizes']), color='red', linestyle='--', linewidth=2)\naxes[0,2].axvline(np.median(stats['final_sizes']), color='green', linestyle='--', linewidth=2)\naxes[0,2].set_xlabel('Final Size (bytes)')\naxes[0,2].set_ylabel('Frequency')\naxes[0,2].set_title('Final Size Distribution')\n\n# Size reduction violin plot\nreduction_data = [stats['original_sizes'], stats['minified_sizes'], stats['final_sizes']]\naxes[1,0].violinplot(reduction_data, positions=[1, 2, 3], showmeans=True, showmedians=True)\naxes[1,0].set_xticks([1, 2, 3])\naxes[1,0].set_xticklabels(['Original', 'Minified', 'Final'])\naxes[1,0].set_ylabel('Size (bytes)')\naxes[1,0].set_title('Size Reduction Stages')\naxes[1,0].grid(True, alpha=0.3)\n\n# Compression ratio distribution\naxes[1,1].hist(stats['compression_ratios'], bins=30, alpha=0.7, color='#e74c3c', edgecolor='black')\naxes[1,1].axvline(1.0, color='black', linestyle='--', linewidth=2, label='No compression')\naxes[1,1].set_xlabel('Compression Ratio')\naxes[1,1].set_ylabel('Frequency')\naxes[1,1].set_title('Compression Effectiveness')\naxes[1,1].legend()\n\n# Potential score distribution\naxes[1,2].hist(stats['potential_scores'], bins=50, alpha=0.7, color='#f39c12', edgecolor='black')\naxes[1,2].axvline(np.mean(stats['potential_scores']), color='red', linestyle='--', linewidth=2)\naxes[1,2].set_xlabel('Potential Score')\naxes[1,2].set_ylabel('Frequency')\naxes[1,2].set_title('Score Distribution')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/size_distribution_analysis.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: size_distribution_analysis.png\")\nplt.show()\n\n# 2. SOURCE PERFORMANCE COMPARISON\nprint(\"\\nðŸ“Š Creating Source Performance Comparison...\")\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Source Performance Comparison Analysis', fontsize=16, fontweight='bold')\n\n# Source wins bar chart\nsources = list(source_counts.index)\nwins = list(source_counts.values)\ncolors = ['#FFD700' if s == 'optimization-trials' else '#87CEEB' for s in sources]\nbars = axes[0,0].bar(sources, wins, color=colors, edgecolor='black')\naxes[0,0].set_xlabel('Source')\naxes[0,0].set_ylabel('Number of Wins')\naxes[0,0].set_title('Source Performance (Task Wins)')\naxes[0,0].tick_params(axis='x', rotation=45)\nfor bar, value in zip(bars, wins):\n    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n                   str(value), ha='center', va='bottom', fontweight='bold')\n\n# Source coverage pie chart\ncoverage_data = {source: len(tasks) for source, tasks in source_task_coverage.items()}\nif coverage_data:\n    axes[0,1].pie(coverage_data.values(), labels=coverage_data.keys(), autopct='%1.1f%%', \n                  startangle=90, colors=plt.cm.Set3.colors)\n    axes[0,1].set_title('Task Coverage by Source')\n\n# Source performance over tasks\naxes[1,0].set_xlabel('Task Number')\naxes[1,0].set_ylabel('Cumulative Wins')\naxes[1,0].set_title('Source Performance Timeline')\nfor source in source_performance_timeline.keys():\n    if source_performance_timeline[source]:\n        cumulative_wins = list(range(1, len(source_performance_timeline[source]) + 1))\n        axes[1,0].plot(sorted(source_performance_timeline[source]), cumulative_wins, \n                      marker='o' if source == 'optimization-trials' else '', \n                      label=source, alpha=0.8, linewidth=2)\naxes[1,0].legend(loc='best')\naxes[1,0].grid(True, alpha=0.3)\n\n# Source synergy heatmap preparation\nif len(task_source_comparison) > 0:\n    # Create a matrix showing how often each source is best for each task range\n    task_ranges = [(i*50+1, min((i+1)*50, 400)) for i in range(8)]\n    source_names = list(set(stats['source_used']))\n    heatmap_data = np.zeros((len(source_names), len(task_ranges)))\n    \n    for task_idx, source in enumerate(stats['source_used']):\n        task_num = stats['task_nums'][task_idx]\n        range_idx = (task_num - 1) // 50\n        if range_idx < len(task_ranges) and source in source_names:\n            source_idx = source_names.index(source)\n            heatmap_data[source_idx, range_idx] += 1\n    \n    im = axes[1,1].imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n    axes[1,1].set_xticks(range(len(task_ranges)))\n    axes[1,1].set_xticklabels([f'{r[0]}-{r[1]}' for r in task_ranges], rotation=45)\n    axes[1,1].set_yticks(range(len(source_names)))\n    axes[1,1].set_yticklabels(source_names)\n    axes[1,1].set_xlabel('Task Range')\n    axes[1,1].set_ylabel('Source')\n    axes[1,1].set_title('Source Dominance by Task Range')\n    plt.colorbar(im, ax=axes[1,1])\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/source_performance_comparison.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: source_performance_comparison.png\")\nplt.show()\n\n# 3. OPTIMIZATION EFFECTIVENESS\nprint(\"\\nðŸ“Š Creating Optimization Effectiveness Visualization...\")\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Optimization Effectiveness Analysis', fontsize=16, fontweight='bold')\n\n# Size reduction scatter\naxes[0,0].scatter(stats['original_sizes'], stats['final_sizes'], \n                  alpha=0.5, c=stats['potential_scores'], cmap='viridis', s=30)\naxes[0,0].plot([0, max(stats['original_sizes'])], [0, max(stats['original_sizes'])], \n               'r--', alpha=0.5, label='No reduction')\naxes[0,0].set_xlabel('Original Size (bytes)')\naxes[0,0].set_ylabel('Final Size (bytes)')\naxes[0,0].set_title('Size Reduction Scatter')\naxes[0,0].legend()\n\n# Minification effectiveness\naxes[0,1].hist(stats['minification_reduction'], bins=30, alpha=0.7, \n               color='#8e44ad', edgecolor='black')\naxes[0,1].axvline(np.mean(stats['minification_reduction']), color='red', \n                  linestyle='--', linewidth=2, label=f'Mean: {np.mean(stats[\"minification_reduction\"]):.1f}%')\naxes[0,1].set_xlabel('Minification Reduction (%)')\naxes[0,1].set_ylabel('Frequency')\naxes[0,1].set_title('Minification Effectiveness')\naxes[0,1].legend()\n\n# Compression beneficial pie\ncompression_data = ['Beneficial', 'Not Beneficial']\ncompression_counts = [compression_beneficial_count, analyzed_tasks - compression_beneficial_count]\ncolors = ['#2ecc71', '#e74c3c']\naxes[0,2].pie(compression_counts, labels=compression_data, colors=colors, \n              autopct='%1.1f%%', startangle=90)\naxes[0,2].set_title('Compression Benefit Analysis')\n\n# Efficiency score distribution\naxes[1,0].hist(stats['efficiency_score'], bins=30, alpha=0.7, \n               color='#3498db', edgecolor='black')\naxes[1,0].set_xlabel('Efficiency Score')\naxes[1,0].set_ylabel('Frequency')\naxes[1,0].set_title('Task Efficiency Distribution')\n\n# Optimization potential vs actual reduction\nactual_reduction = [(orig - final) for orig, final in zip(stats['original_sizes'], stats['final_sizes'])]\naxes[1,1].scatter(stats['optimization_potential'], actual_reduction, alpha=0.5, s=20)\naxes[1,1].set_xlabel('Optimization Potential')\naxes[1,1].set_ylabel('Actual Reduction (bytes)')\naxes[1,1].set_title('Optimization Potential vs Actual')\naxes[1,1].grid(True, alpha=0.3)\n\n# Task difficulty vs final size\ndifficulty_order = ['Easy', 'Medium', 'Hard', 'Expert']\ndifficulty_sizes = {d: [] for d in difficulty_order}\nfor diff, size in zip(stats['task_difficulty'], stats['final_sizes']):\n    difficulty_sizes[diff].append(size)\n\nbp = axes[1,2].boxplot([difficulty_sizes[d] for d in difficulty_order], \n                        labels=difficulty_order, patch_artist=True)\ncolors = ['#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\naxes[1,2].set_xlabel('Difficulty')\naxes[1,2].set_ylabel('Final Size (bytes)')\naxes[1,2].set_title('Size Distribution by Difficulty')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/optimization_effectiveness.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: optimization_effectiveness.png\")\nplt.show()\n\n# 4. CODE PATTERNS AND TECHNIQUES\nprint(\"\\nðŸ“Š Creating Code Patterns Analysis...\")\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Code Patterns and Techniques Analysis', fontsize=16, fontweight='bold')\n\n# Top patterns bar chart\ntop_patterns = sorted(pattern_frequency.items(), key=lambda x: x[1], reverse=True)[:15]\npattern_names = [p[0].replace('_', ' ').title() for p in top_patterns]\npattern_counts = [p[1] for p in top_patterns]\n\naxes[0,0].barh(pattern_names, pattern_counts, color='#16a085')\naxes[0,0].set_xlabel('Occurrences')\naxes[0,0].set_title('Top 15 Code Patterns')\naxes[0,0].invert_yaxis()\n\n# Pattern correlation heatmap\nif len(pattern_frequency) > 5:\n    # Calculate co-occurrence of patterns\n    pattern_list = list(pattern_frequency.keys())[:10]\n    cooccurrence = np.zeros((len(pattern_list), len(pattern_list)))\n    \n    for patterns_dict in stats['code_patterns']:\n        for source_patterns in patterns_dict.values():\n            for i, p1 in enumerate(pattern_list):\n                for j, p2 in enumerate(pattern_list):\n                    if source_patterns.get(p1, 0) > 0 and source_patterns.get(p2, 0) > 0:\n                        cooccurrence[i, j] += 1\n    \n    im = axes[0,1].imshow(cooccurrence, cmap='Blues', aspect='auto')\n    axes[0,1].set_xticks(range(len(pattern_list)))\n    axes[0,1].set_xticklabels([p.replace('_', ' ')[:10] for p in pattern_list], rotation=45)\n    axes[0,1].set_yticks(range(len(pattern_list)))\n    axes[0,1].set_yticklabels([p.replace('_', ' ')[:10] for p in pattern_list])\n    axes[0,1].set_title('Pattern Co-occurrence Matrix')\n    plt.colorbar(im, ax=axes[0,1])\n\n# Complexity metrics distribution\ncomplexity_distributions = {\n    'Line Count': [m.get('line_count', 0) for m in stats['complexity_metrics']],\n    'Function Count': [m.get('function_count', 0) for m in stats['complexity_metrics']],\n    'Loop Count': [m.get('loop_count', 0) for m in stats['complexity_metrics']],\n    'Lambda Count': [m.get('lambda_count', 0) for m in stats['complexity_metrics']]\n}\n\nbp = axes[1,0].boxplot(list(complexity_distributions.values()), \n                        labels=list(complexity_distributions.keys()),\n                        patch_artist=True)\nfor patch, color in zip(bp['boxes'], plt.cm.Set2.colors):\n    patch.set_facecolor(color)\naxes[1,0].set_ylabel('Count')\naxes[1,0].set_title('Code Complexity Metrics')\naxes[1,0].tick_params(axis='x', rotation=45)\n\n# Word cloud of patterns\nif pattern_frequency:\n    pattern_text = ' '.join([f\"{pattern} \" * count for pattern, count in pattern_frequency.items()])\n    if pattern_text.strip():\n        wordcloud = WordCloud(width=400, height=300, background_color='white', \n                            colormap='viridis').generate(pattern_text)\n        axes[1,1].imshow(wordcloud, interpolation='bilinear')\n        axes[1,1].axis('off')\n        axes[1,1].set_title('Pattern Word Cloud')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/code_patterns_analysis.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: code_patterns_analysis.png\")\nplt.show()\n\n# 5. TASK PROGRESSION ANALYSIS\nprint(\"\\nðŸ“Š Creating Task Progression Analysis...\")\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Task Progression and Trends Analysis', fontsize=16, fontweight='bold')\n\n# Task size progression\nif task_size_progression:\n    task_nums_prog, sizes_prog = zip(*task_size_progression)\n    axes[0,0].plot(task_nums_prog, sizes_prog, alpha=0.5, linewidth=1)\n    \n    # Add rolling average\n    window_size = 20\n    if len(sizes_prog) >= window_size:\n        rolling_avg = pd.Series(sizes_prog).rolling(window=window_size, center=True).mean()\n        axes[0,0].plot(task_nums_prog, rolling_avg, 'r-', linewidth=2, label=f'{window_size}-task rolling avg')\n    \n    axes[0,0].set_xlabel('Task Number')\n    axes[0,0].set_ylabel('Final Size (bytes)')\n    axes[0,0].set_title('Task Size Progression')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n\n# Score progression\naxes[0,1].plot(stats['task_nums'], stats['potential_scores'], alpha=0.5, linewidth=1)\nif len(stats['potential_scores']) >= 20:\n    rolling_scores = pd.Series(stats['potential_scores']).rolling(window=20, center=True).mean()\n    axes[0,1].plot(stats['task_nums'], rolling_scores, 'g-', linewidth=2, label='20-task rolling avg')\naxes[0,1].set_xlabel('Task Number')\naxes[0,1].set_ylabel('Potential Score')\naxes[0,1].set_title('Score Progression')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# Difficulty distribution over task ranges\ntask_ranges = [(i*50+1, min((i+1)*50, 400)) for i in range(8)]\ndifficulty_by_range = {d: [0]*len(task_ranges) for d in difficulty_order}\n\nfor task_num, difficulty in zip(stats['task_nums'], stats['task_difficulty']):\n    range_idx = (task_num - 1) // 50\n    if range_idx < len(task_ranges):\n        difficulty_by_range[difficulty][range_idx] += 1\n\nbottom = np.zeros(len(task_ranges))\ncolors = {'Easy': '#2ecc71', 'Medium': '#f39c12', 'Hard': '#e74c3c', 'Expert': '#9b59b6'}\nfor difficulty in difficulty_order:\n    axes[1,0].bar(range(len(task_ranges)), difficulty_by_range[difficulty], \n                  bottom=bottom, label=difficulty, color=colors[difficulty])\n    bottom += difficulty_by_range[difficulty]\n\naxes[1,0].set_xticks(range(len(task_ranges)))\naxes[1,0].set_xticklabels([f'{r[0]}-{r[1]}' for r in task_ranges])\naxes[1,0].set_xlabel('Task Range')\naxes[1,0].set_ylabel('Count')\naxes[1,0].set_title('Difficulty Distribution by Task Range')\naxes[1,0].legend()\n\n# Compression benefit over tasks\ncompression_window = []\ncompression_x = []\ncompression_y = []\n\nfor i, (task_num, beneficial) in enumerate(zip(stats['task_nums'], stats['compression_beneficial'])):\n    compression_window.append(1 if beneficial else 0)\n    if len(compression_window) > 50:\n        compression_window.pop(0)\n    if len(compression_window) >= 10:\n        compression_x.append(task_num)\n        compression_y.append(sum(compression_window) / len(compression_window) * 100)\n\nif compression_x:\n    axes[1,1].plot(compression_x, compression_y, linewidth=2, color='#e67e22')\n    axes[1,1].fill_between(compression_x, compression_y, alpha=0.3, color='#e67e22')\n    axes[1,1].set_xlabel('Task Number')\n    axes[1,1].set_ylabel('Compression Benefit Rate (%)')\n    axes[1,1].set_title('Compression Effectiveness Over Tasks (50-task window)')\n    axes[1,1].grid(True, alpha=0.3)\n    axes[1,1].axhline(y=50, color='black', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/task_progression_analysis.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: task_progression_analysis.png\")\nplt.show()\n\n# 6. INTERACTIVE PLOTLY DASHBOARD\nprint(\"\\nðŸ“Š Creating Interactive Dashboard...\")\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Create subplots\nfig = make_subplots(\n    rows=3, cols=3,\n    subplot_titles=('Size Distribution', 'Score Distribution', 'Source Wins',\n                    'Size Reduction', 'Compression Benefit', 'Task Difficulty',\n                    'Pattern Frequency', 'Efficiency Score', 'Task Progression'),\n    specs=[[{\"type\": \"histogram\"}, {\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n           [{\"type\": \"scatter\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}],\n           [{\"type\": \"bar\"}, {\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n)\n\n# 1. Size distribution\nfig.add_trace(go.Histogram(x=stats['final_sizes'], name='Final Size', \n                           marker_color='lightblue', nbinsx=30),\n              row=1, col=1)\n\n# 2. Score distribution\nfig.add_trace(go.Histogram(x=stats['potential_scores'], name='Potential Score',\n                           marker_color='lightgreen', nbinsx=30),\n              row=1, col=2)\n\n# 3. Source wins\nfig.add_trace(go.Bar(x=list(source_counts.index), y=list(source_counts.values),\n                     name='Source Wins', marker_color='gold'),\n              row=1, col=3)\n\n# 4. Size reduction scatter\nfig.add_trace(go.Scatter(x=stats['original_sizes'], y=stats['final_sizes'],\n                        mode='markers', name='Size Reduction',\n                        marker=dict(size=5, color=stats['potential_scores'], \n                                  colorscale='Viridis', showscale=True)),\n              row=2, col=1)\n\n# 5. Compression benefit pie\nfig.add_trace(go.Pie(labels=['Beneficial', 'Not Beneficial'],\n                     values=[compression_beneficial_count, \n                            analyzed_tasks - compression_beneficial_count],\n                     name='Compression'),\n              row=2, col=2)\n\n# 6. Task difficulty pie\nfig.add_trace(go.Pie(labels=list(difficulty_counts.index),\n                     values=list(difficulty_counts.values),\n                     name='Difficulty'),\n              row=2, col=3)\n\n# 7. Top patterns\ntop_10_patterns = sorted(pattern_frequency.items(), key=lambda x: x[1], reverse=True)[:10]\nfig.add_trace(go.Bar(x=[p[1] for p in top_10_patterns],\n                     y=[p[0] for p in top_10_patterns],\n                     orientation='h', name='Patterns',\n                     marker_color='purple'),\n              row=3, col=1)\n\n# 8. Efficiency score\nfig.add_trace(go.Histogram(x=stats['efficiency_score'], name='Efficiency',\n                           marker_color='orange', nbinsx=30),\n              row=3, col=2)\n\n# 9. Task progression\nfig.add_trace(go.Scatter(x=stats['task_nums'], y=stats['final_sizes'],\n                        mode='lines', name='Size Progression',\n                        line=dict(color='blue', width=1)),\n              row=3, col=3)\n\n# Update layout\nfig.update_layout(height=1200, showlegend=False, \n                 title_text=\"Code Golf Analysis Interactive Dashboard\")\nfig.update_xaxes(title_text=\"Size (bytes)\", row=1, col=1)\nfig.update_xaxes(title_text=\"Score\", row=1, col=2)\nfig.update_xaxes(title_text=\"Original Size\", row=2, col=1)\nfig.update_yaxes(title_text=\"Final Size\", row=2, col=1)\nfig.update_xaxes(title_text=\"Task Number\", row=3, col=3)\nfig.update_yaxes(title_text=\"Size (bytes)\", row=3, col=3)\n\n# Save interactive plot\nfig.write_html(\"/kaggle/working/interactive_dashboard.html\")\nprint(\"âœ… Saved: interactive_dashboard.html\")\nfig.show()\n\n# 7. CORRELATION ANALYSIS\nprint(\"\\nðŸ“Š Creating Correlation Analysis...\")\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nfig.suptitle('Correlation Analysis', fontsize=16, fontweight='bold')\n\n# Prepare correlation data\ncorr_data = pd.DataFrame({\n    'Original Size': stats['original_sizes'],\n    'Final Size': stats['final_sizes'],\n    'Score': stats['potential_scores'],\n    'Compression Ratio': stats['compression_ratios'],\n    'Efficiency': stats['efficiency_score'],\n    'Minification %': stats['minification_reduction']\n})\n\n# Correlation matrix\ncorrelation_matrix = corr_data.corr()\nim = axes[0].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\naxes[0].set_xticks(range(len(correlation_matrix.columns)))\naxes[0].set_xticklabels(correlation_matrix.columns, rotation=45)\naxes[0].set_yticks(range(len(correlation_matrix.columns)))\naxes[0].set_yticklabels(correlation_matrix.columns)\naxes[0].set_title('Feature Correlation Matrix')\n\n# Add correlation values\nfor i in range(len(correlation_matrix)):\n    for j in range(len(correlation_matrix)):\n        text = axes[0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n                           ha=\"center\", va=\"center\", color=\"black\" if abs(correlation_matrix.iloc[i, j]) < 0.5 else \"white\")\n\nplt.colorbar(im, ax=axes[0])\n\n# Pairplot for key metrics\nkey_metrics = pd.DataFrame({\n    'Size': stats['final_sizes'],\n    'Score': stats['potential_scores'],\n    'Efficiency': stats['efficiency_score']\n})\n\n# Scatter matrix\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(key_metrics, ax=axes[1], figsize=(6, 6), diagonal='hist', alpha=0.5)\naxes[1].set_title('Key Metrics Relationships')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/correlation_analysis.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: correlation_analysis.png\")\nplt.show()\n\n# 8. SUMMARY INFOGRAPHIC\nprint(\"\\nðŸ“Š Creating Summary Infographic...\")\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n\n# Title\nfig.suptitle('Code Golf Analysis Summary Infographic', fontsize=20, fontweight='bold', y=0.98)\n\n# Key metrics boxes\nmetrics_to_display = [\n    ('Tasks Analyzed', f'{analyzed_tasks}/400', '#3498db'),\n    ('Total Score', f'{total_potential_score:,}', '#2ecc71'),\n    ('Avg Size', f'{np.mean(stats[\"final_sizes\"]):.0f} bytes', '#9b59b6'),\n    ('Compression Rate', f'{compression_beneficial_count/analyzed_tasks*100:.1f}%', '#e74c3c'),\n    ('Best Source', source_counts.index[0] if len(source_counts) > 0 else 'N/A', '#f39c12'),\n    ('Submission Size', f'{submission_size/1024:.1f} KB', '#16a085'),\n    ('Analysis Time', f'{analysis_time:.1f}s', '#95a5a6'),\n    ('Score Efficiency', f'{total_potential_score/(2500*analyzed_tasks)*100:.1f}%', '#e67e22')\n]\n\nfor i, (label, value, color) in enumerate(metrics_to_display):\n    ax = fig.add_subplot(gs[0, i % 4])\n    ax.text(0.5, 0.7, value, ha='center', va='center', fontsize=20, fontweight='bold', color=color)\n    ax.text(0.5, 0.3, label, ha='center', va='center', fontsize=12)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.add_patch(plt.Rectangle((0.1, 0.1), 0.8, 0.8, fill=False, edgecolor=color, linewidth=2))\n\n# Remaining visualizations for the infographic\n# ... [Add more summary visualizations as needed]\n\nplt.savefig('/kaggle/working/summary_infographic.png', dpi=150, bbox_inches='tight')\nprint(\"âœ… Saved: summary_infographic.png\")\nplt.show()\n\n# Final comprehensive summary\nprint(\"\\n\" + \"=\"*100)\nprint(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                           ðŸ“‹ ULTRA-ENHANCED FINAL SUMMARY                                    â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")\n\nsummary_data = [\n    (\"ANALYSIS PERFORMANCE\", [\n        (\"Analysis time\", f\"{analysis_time:.2f} seconds\"),\n        (\"Tasks analyzed\", f\"{analyzed_tasks}/400\"),\n        (\"Sources compared\", str(len(source_counts))),\n        (\"Total comparisons made\", str(tasks_analyzed * len(analysis_sources))),\n        (\"Code analyzed\", f\"{total_code_analyzed:,} bytes\"),\n        (\"Analysis speed\", f\"{total_code_analyzed/analysis_time/1024:.1f} KB/sec\"),\n    ]),\n    (\"OPTIMIZATION RESULTS\", [\n        (\"Compression beneficial for\", f\"{compression_beneficial_count} tasks ({compression_beneficial_count/analyzed_tasks*100:.1f}%)\"),\n        (\"Total bytes saved\", f\"{total_bytes_saved:,} bytes\"),\n        (\"Average minification\", f\"{np.mean(stats['minification_reduction']):.1f}%\"),\n        (\"High optimization potential\", f\"{len(high_potential_tasks)} tasks\"),\n    ]),\n    (\"SCORING METRICS\", [\n        (\"Potential score\", f\"{total_potential_score:,}\"),\n        (\"Average per task\", f\"{total_potential_score/analyzed_tasks:.1f}\"),\n        (\"Perfect scores (â‰¥2450)\", f\"{sum(1 for s in stats['potential_scores'] if s >= 2450)} tasks\"),\n        (\"Score efficiency\", f\"{total_potential_score/(2500*analyzed_tasks)*100:.1f}%\"),\n    ]),\n    (\"OPTIMIZATION TRIALS SUBMISSION\", [\n        (\"Source used\", \"golf-code-optimization-trials\"),\n        (\"Location\", \"/kaggle/working/submission.zip\"),\n        (\"Submission size\", f\"{submission_size:,} bytes ({submission_size/1024:.1f} KB)\" if submission_size > 0 else \"Not found\"),\n        (\"Size per task\", f\"{submission_size/400:.1f} bytes average\" if submission_size > 0 else \"N/A\"),\n    ]),\n    (\"VISUALIZATIONS CREATED\", [\n        (\"Size Distribution Analysis\", \"size_distribution_analysis.png\"),\n        (\"Source Performance\", \"source_performance_comparison.png\"),\n        (\"Optimization Effectiveness\", \"optimization_effectiveness.png\"),\n        (\"Code Patterns\", \"code_patterns_analysis.png\"),\n        (\"Task Progression\", \"task_progression_analysis.png\"),\n        (\"Interactive Dashboard\", \"interactive_dashboard.html\"),\n        (\"Correlation Analysis\", \"correlation_analysis.png\"),\n        (\"Summary Infographic\", \"summary_infographic.png\"),\n    ]),\n]\n\nfor section_title, metrics in summary_data:\n    print(f\"\\n{'â”€'*100}\")\n    print(f\"  {section_title}\")\n    print(f\"{'â”€'*100}\")\n    for metric, value in metrics:\n        print(f\"  {metric:<40} {value:<40}\")\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"\\nâœ¨ Ultra-enhanced analysis complete with multiple visualizations!\")\nprint(\"ðŸš€ Optimization trials submission ready at /kaggle/working/submission.zip\")\nprint(\"ðŸ“Š Check generated files for comprehensive visualizations:\")\nprint(\"   â€¢ 8 detailed PNG visualizations\")\nprint(\"   â€¢ 1 interactive HTML dashboard\")\n\nprint(\"\\nðŸŽ¯ Key Insights:\")\nprint(\"   â€¢ The golf-code-optimization-trials solution is being used for submission\")\nprint(f\"   â€¢ Best performing source in analysis: {source_counts.index[0] if len(source_counts) > 0 else 'N/A'}\")\nprint(f\"   â€¢ Compression beneficial for {compression_beneficial_count/analyzed_tasks*100:.1f}% of tasks\")\nprint(f\"   â€¢ Total optimization potential: {sum(stats['optimization_potential']):.1f}\")\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"ðŸ“ˆ ANALYSIS COMPLETE - OPTIMIZATION TRIALS SUBMISSION READY FOR COMPETITION\")\nprint(\"=\"*100)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}