{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13245791,"sourceType":"competition"},{"sourceId":499905,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":397065,"modelId":415485}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\nimport json\nimport os\nimport zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\nfrom scipy.ndimage import label\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Optional, Any\nimport random\nimport traceback\nimport warnings\nwarnings.filterwarnings('ignore')\n\nLLM_AVAILABLE = False\ntry:\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n    LLM_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: Transformers/Torch not available, LLM features disabled\")\n\nML_AVAILABLE = False\ntry:\n    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n    from scipy.ndimage import gaussian_filter\n    ML_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: ML libraries not available, using rule-based approach only\")\n\nARC_COLORS = {\n    0: '#000000',\n    1: '#0074D9',\n    2: '#FF4136',\n    3: '#2ECC40',\n    4: '#FFDC00',\n    5: '#AAAAAA',\n    6: '#F012BE',\n    7: '#FF851B',\n    8: '#7FDBFF',\n    9: '#870C25'\n}\n\nclass LLMCodeGenerator:\n    def __init__(self, model_path=\"/kaggle/input/qwen2.5-7b-instruct-bnb-4bit/transformers/default/1\"):\n        if not LLM_AVAILABLE:\n            self.model = None\n            self.tokenizer = None\n            return\n            \n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n            \n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\"\n            )\n            \n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n            \n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n                \n            print(\"✅ Qwen2.5-7B loaded successfully!\")\n            \n        except Exception as e:\n            print(f\"❌ Failed to load Qwen2.5-7B: {e}\")\n            self.model = None\n            self.tokenizer = None\n    \n    def generate_code(self, examples: List[Dict], max_attempts: int = 3) -> str:\n        if not self.model or not self.tokenizer:\n            return None\n            \n        prompt = self._create_prompt(examples)\n        \n        for attempt in range(max_attempts):\n            try:\n                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n                \n                with torch.no_grad():\n                    outputs = self.model.generate(\n                        inputs.input_ids,\n                        max_new_tokens=200,\n                        temperature=0.7 if attempt > 0 else 0.1,\n                        do_sample=True if attempt > 0 else False,\n                        pad_token_id=self.tokenizer.pad_token_id,\n                        eos_token_id=self.tokenizer.eos_token_id,\n                    )\n                \n                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                code = self._extract_code(response)\n                \n                if code and self._verify_code(code, examples):\n                    return self._minify_code(code)\n                    \n            except Exception as e:\n                if attempt == max_attempts - 1:\n                    print(f\"LLM generation error: {e}\")\n                    \n        return None\n    \n    def _create_prompt(self, examples: List[Dict]) -> str:\n        prompt = \"\"\"You are an expert at solving ARC (Abstraction and Reasoning Corpus) tasks with minimal Python code.\n\nGiven these input/output examples, write a Python function p(g) that transforms the input grid g to produce the output.\nThe function should be as SHORT as possible (code golf style).\n\nExamples:\n\"\"\"\n        \n        for i, ex in enumerate(examples[:3]):\n            prompt += f\"\\nExample {i+1}:\\n\"\n            prompt += f\"Input:\\n{self._grid_to_string(ex['input'])}\\n\"\n            prompt += f\"Output:\\n{self._grid_to_string(ex['output'])}\\n\"\n        \n        prompt += \"\"\"\nWrite ONLY the function definition. Make it as compact as possible.\nUse single letters for variables, no spaces where not needed.\nThe function signature must be: p=lambda g: ...\nExamples of compact style:\n- p=lambda g:g[::-1]  # reverse rows\n- p=lambda g:[r[::-1]for r in g]  # reverse each row\n- p=lambda g:[[c*2for c in r]for r in g]  # multiply each cell by 2\n\"\"\"\n        \n        return prompt\n    \n    def _grid_to_string(self, grid: List[List[int]]) -> str:\n        return '\\n'.join(' '.join(str(c) for c in row) for row in grid)\n    \n    def _extract_code(self, response: str) -> str:\n        if 'p=lambda' in response:\n            start = response.find('p=lambda')\n            end = response.find('\\n', start)\n            if end == -1:\n                end = len(response)\n            return response[start:end].strip()\n        \n        if 'def p(' in response:\n            start = response.find('def p(')\n            lines = response[start:].split('\\n')\n            func_lines = []\n            for line in lines:\n                if line.strip() and not line.strip().startswith(('\"\"\"', \"'''\", '#')):\n                    func_lines.append(line)\n                if line.strip().startswith('return'):\n                    break\n            return '\\n'.join(func_lines)\n        \n        return None\n    \n    def _verify_code(self, code: str, examples: List[Dict]) -> bool:\n        try:\n            namespace = {}\n            exec(code, namespace)\n            p = namespace['p']\n            \n            for ex in examples[:3]:\n                result = p([row[:] for row in ex['input']])\n                if result != ex['output']:\n                    return False\n            return True\n        except:\n            return False\n    \n    def _minify_code(self, code: str) -> str:\n        replacements = [\n            (' = ', '='), (', ', ','), (': ', ':'),\n            (' + ', '+'), (' - ', '-'), (' * ', '*'),\n            (' / ', '/'), (' % ', '%'), (' // ', '//'),\n            (' > ', '>'), (' < ', '<'), (' == ', '=='),\n            (' != ', '!='), (' >= ', '>='), (' <= ', '<='),\n            (' if ', 'if '), (' else ', 'else '),\n            (' for ', 'for '), (' in ', 'in '),\n            (' and ', 'and '), (' or ', 'or '),\n            (' not ', 'not '),\n        ]\n        \n        for old, new in replacements:\n            code = code.replace(old, new)\n            \n        code = '\\n'.join(line.rstrip() for line in code.split('\\n'))\n        \n        return code\n\nclass UltimateARCSolutionGenerator:\n    def __init__(self, enable_visuals=True, genetic_generations=10):\n        self.enable_visuals = enable_visuals\n        self.genetic_generations = genetic_generations\n        self.pattern_success_rates = defaultdict(lambda: {'attempts': 0, 'successes': 0})\n        self.solution_complexity_scores = {}\n        self.pattern_explanations = {}\n        self.population = []\n        self.fitness_history = []\n        self.llm_generator = LLMCodeGenerator() if LLM_AVAILABLE else None\n        \n        self.pattern_handlers = [\n            (self.handle_outline_only, \"Extract object outlines\", \"geometric\"),\n            (self.handle_center_object, \"Center objects in grid\", \"geometric\"),\n            (self.handle_diagonal_mirror, \"Diagonal mirror transformation\", \"geometric\"),\n            (self.handle_color_palette_row, \"Extract color palette\", \"color\"),\n            (self.handle_bounding_fill_by_color, \"Fill bounding boxes by color\", \"geometric\"),\n            (self.handle_row_col_propagation, \"Propagate colors along rows/columns\", \"propagation\"),\n            (self.handle_repeat_pattern, \"Tile pattern across grid\", \"tiling\"),\n            (self.handle_checkerboard, \"Create checkerboard pattern\", \"tiling\"),\n            (self.handle_vertical_stripes, \"Create vertical stripes\", \"tiling\"),\n            (self.handle_horizontal_stripes, \"Create horizontal stripes\", \"tiling\"),\n            (self.handle_majority_color_fill, \"Fill with majority color\", \"color\"),\n            (self.handle_cross_lines, \"Draw cross lines\", \"geometric\"),\n            (self.handle_color_swap, \"Swap two colors\", \"color\"),\n            (self.handle_crop_center, \"Crop to center region\", \"geometric\"),\n            (self.handle_single_color_output, \"Output single color\", \"color\"),\n            (self.handle_bounding_crop, \"Crop to bounding box\", \"geometric\"),\n            (self.handle_overlay_fill, \"Overlay fill pattern\", \"composite\"),\n            (self.handle_remove_color, \"Remove specific color\", \"color\"),\n            (self.handle_color_mapping, \"Map colors to new values\", \"color\"),\n            (self.handle_grid_operations, \"Grid rotation/flip\", \"geometric\"),\n            (self.handle_resize, \"Resize grid\", \"geometric\"),\n            (self.handle_object_operations, \"Object translations\", \"geometric\"),\n            (self.handle_pattern_replication, \"Replicate patterns\", \"tiling\"),\n            (self.handle_mirror_symmetry, \"Mirror symmetry\", \"geometric\"),\n            (self.handle_arithmetic_operations, \"Arithmetic operations\", \"arithmetic\"),\n            (self.handle_conditional_operations, \"Conditional operations\", \"logic\"),\n            (self.handle_upscale_nearest, \"Nearest neighbor upscaling\", \"scaling\"),\n            (self.handle_downscale_sample, \"Downscale by sampling\", \"scaling\"),\n            (self.handle_fill_gaps_rowwise, \"Fill gaps in rows\", \"propagation\"),\n            (self.handle_background_to_majority, \"Change background to majority\", \"color\"),\n            (self.handle_keep_dominant_color, \"Keep only dominant color\", \"color\"),\n            (self.handle_extract_first_nonempty_rowcol, \"Extract first non-empty row/col\", \"extraction\"),\n            (self.handle_sort_rows_by_density, \"Sort rows by density\", \"sorting\"),\n            (self.handle_sort_cols_by_density, \"Sort columns by density\", \"sorting\"),\n            (self.handle_complete_by_mirror_half, \"Complete by mirroring half\", \"geometric\"),\n            (self.handle_palette_column, \"Extract color palette column\", \"color\"),\n            (self.handle_uniform_row_fill, \"Fill rows uniformly\", \"propagation\"),\n            (self.handle_uniform_col_fill, \"Fill columns uniformly\", \"propagation\"),\n            (self.handle_draw_frame, \"Draw frame around grid\", \"geometric\"),\n            (self.handle_main_diag_line, \"Draw main diagonal\", \"geometric\"),\n            (self.handle_anti_diag_line, \"Draw anti-diagonal\", \"geometric\"),\n            (self.handle_border_to_zero, \"Clear border pixels\", \"geometric\"),\n            (self.handle_single_object_translate, \"Translate single object\", \"geometric\"),\n            (self.handle_object_count_row, \"Count objects in row\", \"counting\"),\n            (self.handle_color_shift_plus_one, \"Shift colors by +1\", \"arithmetic\"),\n            (self.handle_keep_colors, \"Keep only specific colors\", \"color\"),\n            (self.handle_remove_small_objects, \"Remove small objects\", \"filtering\"),\n            (self.handle_duplicate_quadrant, \"Duplicate quadrant\", \"tiling\"),\n        ]\n\n    def handle_uniform_row_fill(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        ok=True\n        for r1,r2 in zip(a,b):\n            maj = np.bincount(r1).argmax()\n            if not np.all(r2==maj): ok=False; break\n        if not ok: return None\n        return \"\"\"def p(g):\n return [[max(set(r), key=r.count)]*len(r) for r in g]\n\"\"\"\n\n    def handle_uniform_col_fill(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        H,W=a.shape\n        ok=True\n        for j in range(W):\n            col=a[:,j].tolist()\n            maj=max(set(col), key=col.count)\n            if not np.all(b[:,j]==maj): ok=False; break\n        if not ok: return None\n        return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n cols=[[g[i][j] for i in range(H)] for j in range(W)]\n res=[[0]*W for _ in range(H)]\n for j,col in enumerate(cols):\n  maj=max(set(col), key=col.count)\n  for i in range(H): res[i][j]=maj\n return res\n\"\"\"\n\n    def handle_draw_frame(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if b.shape[0]!=a.shape[0]+2 or b.shape[1]!=a.shape[1]+2: return None\n        frame_color = int(b[0,0])\n        inner = b[1:-1,1:-1]\n        if np.array_equal(inner,a) and \\\n           np.all(b[[0,-1],:]==frame_color) and np.all(b[:,[0,-1]]==frame_color):\n            return f\"\"\"def p(g):\n H,W=len(g),len(g[0])\n c={frame_color}\n out=[[c]*(W+2)]\n for r in g: out.append([c]+r+[c])\n out.append([c]*(W+2))\n return out\n\"\"\"\n        return None\n\n    def handle_main_diag_line(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        H,W=a.shape\n        if H!=W: return None\n        diff = np.where(a!=b)\n        if len(diff[0])==H and all(i==j for i,j in zip(*diff)):\n            c=int(b[0,0])\n            return f\"\"\"def p(g):\n n=len(g)\n return [[ {c} if i==j else g[i][j] for j in range(n)] for i in range(n)]\n\"\"\"\n        return None\n\n    def handle_anti_diag_line(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        H,W=a.shape\n        if H!=W: return None\n        diff = np.where(a!=b)\n        if len(diff[0])==H and all(i+j==H-1 for i,j in zip(*diff)):\n            c=int(b[0,W-1])\n            return f\"\"\"def p(g):\n n=len(g)\n return [[ {c} if i+j==n-1 else g[i][j] for j in range(n)] for i in range(n)]\n\"\"\"\n        return None\n\n    def handle_border_to_zero(self, task_data, analysis):\n        a,b = map(np.array,(task_data['train'][0]['input'],\n                            task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        inner = a[1:-1,1:-1]\n        if np.array_equal(b[1:-1,1:-1], inner) and np.all(b[[0,-1],:]==0) \\\n           and np.all(b[:,[0,-1]]==0):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n out=[r[:] for r in g]\n for i in range(H):\n  out[i][0]=out[i][-1]=0\n for j in range(W):\n  out[0][j]=out[-1][j]=0\n return out\n\"\"\"\n        return None\n\n    def handle_single_object_translate(self, task_data, analysis):\n        props = analysis['object_properties'][0][0]\n        mv = props['position_change']\n        if isinstance(mv, str):\n            return None\n        dy, dx = map(int, mv)\n        if (dy, dx) == (0, 0):\n            return None\n        return f'''def p(g):\n H, W = len(g), len(g[0])\n ys = [i for i,row in enumerate(g) for v in row if v]\n xs = [j for i,row in enumerate(g) for j,v in enumerate(row) if v]\n res = [[0]*W for _ in range(H)]\n for i,j in zip(ys, xs):\n     ni, nj = i+{dy}, j+{dx}\n     if 0 <= ni < H and 0 <= nj < W:\n         res[ni][nj] = g[i][j]\n return res\n'''\n\n    def handle_object_count_row(self, task_data, analysis):\n        a,b=(task_data['train'][0]['input'],task_data['train'][0]['output'])\n        if len(b)!=1: return None\n        flat=[x for r in a for x in r if x!=0]\n        if not flat: return None\n        maj=max(set(flat), key=flat.count)\n        cnt=flat.count(maj)\n        if b==[[maj]*cnt]:\n            return f\"\"\"def p(g):\n flat=[x for r in g for x in r if x!=0]\n maj=max(set(flat), key=flat.count)\n return [[maj]*flat.count(maj)]\n\"\"\"\n        return None\n\n    def handle_color_shift_plus_one(self, task_data, analysis):\n        a,b=(task_data['train'][0]['input'],task_data['train'][0]['output'])\n        if [[(x+1)%10 for x in r] for r in a]==b:\n            return \"\"\"def p(g):\n return [[(x+1)%10 for x in r] for r in g]\n\"\"\"\n        return None\n\n    def handle_keep_colors(self, task_data, analysis):\n        keep=set(x for ex in task_data['train']\n                   for r in ex['output'] for x in r)\n        if len(keep)>=10: return None\n        a,b=(task_data['train'][0]['input'],task_data['train'][0]['output'])\n        if [[x if x in keep else 0 for x in r] for r in a]==b:\n            keep_str=\",\".join(map(str,keep))\n            return f\"\"\"def p(g):\n keep={{ {keep_str} }}\n return [[x if x in keep else 0 for x in r] for r in g]\n\"\"\"\n        return None\n\n    def handle_remove_small_objects(self, task_data, analysis):\n        a,b=map(np.array,(task_data['train'][0]['input'],\n                          task_data['train'][0]['output']))\n        if a.shape!=b.shape: return None\n        diff=a!=b\n        if not diff.any(): return None\n        ys,xs=np.where(diff)\n        if len(ys)<=2 and np.all(b[ys,xs]==0):\n            coords=[(int(y),int(x)) for y,x in zip(ys,xs)]\n            return f\"\"\"def p(g):\n g=[r[:] for r in g]\n for y,x in {coords}: g[y][x]=0\n return g\n\"\"\"\n        return None\n\n    def handle_duplicate_quadrant(self, task_data, analysis):\n        a,b=map(np.array,(task_data['train'][0]['input'],\n                          task_data['train'][0]['output']))\n        h,w=a.shape\n        if h%2 or w%2 or b.shape!=(h,w): return None\n        q=a[:h//2,:w//2]\n        cand=np.block([[q,q],[q,q]])\n        if np.array_equal(cand,b):\n            return \"\"\"def p(g):\n h,w=len(g),len(g[0]);hh,ww=h//2,w//2\n q=[row[:ww] for row in g[:hh]]\n return [q_row+q_row for q_row in q]+[q_row+q_row for q_row in q]\n\"\"\"\n        return None\n\n    def handle_upscale_nearest(self, task_data, analysis):\n        a, b = map(np.array, (task_data['train'][0]['input'], task_data['train'][0]['output']))\n        hi, wi = a.shape; ho, wo = b.shape\n        if ho%hi or wo%wi: return None\n        ky, kx = ho//hi, wo//wi\n        ok = True\n        for i in range(hi):\n            for j in range(wi):\n                block = b[i*ky:(i+1)*ky, j*kx:(j+1)*kx]\n                if not np.all(block == a[i,j]): ok=False; break\n            if not ok: break\n        if not ok: return None\n        return f\"\"\"def p(g):\n ky, kx = {ky}, {kx}\n H, W = len(g), len(g[0])\n return [[ g[i//ky][j//kx] for j in range(W*kx) ] for i in range(H*ky)]\n\"\"\"\n\n    def handle_downscale_sample(self, task_data, analysis):\n        a, b = map(np.array, (task_data['train'][0]['input'], task_data['train'][0]['output']))\n        hi, wi = a.shape; ho, wo = b.shape\n        if hi%ho or wi%wo: return None\n        ky, kx = hi//ho, wi//wo\n        if not np.all(b == a[::ky, ::kx]): return None\n        return f\"\"\"def p(g):\n ky, kx = {ky}, {kx}\n return [[ g[i*ky][j*kx] for j in range(len(g[0])//kx) ] for i in range(len(g)//ky)]\n\"\"\"\n\n    def handle_fill_gaps_rowwise(self, task_data, analysis):\n        a, b = map(np.array, (task_data['train'][0]['input'], task_data['train'][0]['output']))\n        if a.shape != b.shape: return None\n        def fill_row(r):\n            r = r[:]\n            for c in set(x for x in r if x!=0):\n                idx = [j for j,x in enumerate(r) if x==c]\n                if idx:\n                    l, rgt = min(idx), max(idx)\n                    for j in range(l, rgt+1):\n                        if r[j]==0: r[j]=c\n            return r\n        cand = np.array([fill_row(list(row)) for row in a.tolist()])\n        if not np.array_equal(cand, b): return None\n        return \"\"\"def p(g):\n res=[]\n for r in g:\n  r=r[:]\n  s={x for x in r if x!=0}\n  for c in s:\n   idx=[j for j,x in enumerate(r) if x==c]\n   l,rgt=min(idx),max(idx)\n   for j in range(l,rgt+1):\n    if r[j]==0: r[j]=c\n  res.append(r)\n return res\n\"\"\"\n\n    def handle_background_to_majority(self, task_data, analysis):\n        a, b = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        def apply(g):\n            flat=[x for r in g for x in r if x!=0]\n            c=max(range(10), key=lambda v: flat.count(v)) if flat else 0\n            return [[x if x!=0 else c for x in r] for r in g]\n        if apply(a)==b:\n            return \"\"\"def p(g):\n flat=[x for r in g for x in r if x!=0]\n c=max(range(10), key=lambda v: flat.count(v)) if flat else 0\n return [[x if x!=0 else c for x in r] for r in g]\n\"\"\"\n        return None\n\n    def handle_keep_dominant_color(self, task_data, analysis):\n        a, b = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        def apply(g):\n            flat=[x for r in g for x in r if x!=0]\n            if not flat: return [[0 for _ in r] for r in g]\n            c=max(range(10), key=lambda v: flat.count(v))\n            return [[x if x==c else 0 for x in r] for r in g]\n        if apply(a)==b:\n            return \"\"\"def p(g):\n flat=[x for r in g for x in r if x!=0]\n if not flat: return [[0 for _ in r] for r in g]\n c=max(range(10), key=lambda v: flat.count(v))\n return [[x if x==c else 0 for x in r] for r in g]\n\"\"\"\n        return None\n\n    def handle_extract_first_nonempty_rowcol(self, task_data, analysis):\n        a, b = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        for i, r in enumerate(a):\n            if any(x!=0 for x in r):\n                if b == [r]:\n                    return \"\"\"def p(g):\n for r in g:\n  if any(x!=0 for x in r): return [r]\n return [g[0]]\n\"\"\"\n                break\n        H, W = len(a), len(a[0])\n        col = None\n        for j in range(W):\n            c = [a[i][j] for i in range(H)]\n            if any(x!=0 for x in c):\n                col = [[x] for x in c]; break\n        if col and b == col:\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n for j in range(W):\n  c=[g[i][j] for i in range(H)]\n  if any(x!=0 for x in c): return [[x] for x in c]\n return [[g[i][0]] for i in range(H)]\n\"\"\"\n        return None\n\n    def handle_sort_rows_by_density(self, task_data, analysis):\n        a, b = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        if len(a)!=len(b) or len(a[0])!=len(b[0]): return None\n        asc  = sorted(a, key=lambda r: sum(x!=0 for x in r))\n        desc = asc[::-1]\n        if b==asc:\n            return \"\"\"def p(g):\n return sorted(g, key=lambda r: sum(x!=0 for x in r))\n\"\"\"\n        if b==desc:\n            return \"\"\"def p(g):\n return sorted(g, key=lambda r: sum(x!=0 for x in r), reverse=True)\n\"\"\"\n        return None\n\n    def handle_sort_cols_by_density(self, task_data, analysis):\n        a, b = map(np.array, (task_data['train'][0]['input'], task_data['train'][0]['output']))\n        if a.shape != b.shape: return None\n        H,W=a.shape\n        def sort_cols(arr, rev=False):\n            cols=[arr[:,j].tolist() for j in range(W)]\n            cols=sorted(cols, key=lambda c: sum(x!=0 for x in c), reverse=rev)\n            return [[cols[j][i] for j in range(W)] for i in range(H)]\n        if np.array_equal(b, np.array(sort_cols(a, False))):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n cols=[[g[i][j] for i in range(H)] for j in range(W)]\n cols=sorted(cols, key=lambda c: sum(x!=0 for x in c))\n return [[cols[j][i] for j in range(W)] for i in range(H)]\n\"\"\"\n        if np.array_equal(b, np.array(sort_cols(a, True))):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n cols=[[g[i][j] for i in range(H)] for j in range(W)]\n cols=sorted(cols, key=lambda c: sum(x!=0 for x in c), reverse=True)\n return [[cols[j][i] for j in range(W)] for i in range(H)]\n\"\"\"\n        return None\n\n    def handle_complete_by_mirror_half(self, task_data, analysis):\n        a, b = map(np.array, (task_data['train'][0]['input'], task_data['train'][0]['output']))\n        if a.shape != b.shape: return None\n        H,W=a.shape\n        mid=W//2\n        left=a[:, :mid]\n        cand_lr=np.hstack([left, np.fliplr(left)])\n        if np.array_equal(b, cand_lr):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0]);m=W//2\n left=[r[:m] for r in g]\n return [ left[i]+left[i][::-1] for i in range(H) ]\n\"\"\"\n        mid=H//2\n        top=a[:mid, :]\n        cand_ud=np.vstack([top, np.flipud(top)])\n        if np.array_equal(b, cand_ud):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0]);m=H//2\n top=g[:m]\n return top + top[::-1]\n\"\"\"\n        return None\n\n    def handle_palette_column(self, task_data, analysis):\n        inp, out = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        colors = sorted({x for r in inp for x in r})\n        if out == [[c] for c in colors]:\n            return \"\"\"def p(g):\n c=sorted({x for r in g for x in r})\n return [[x] for x in c]\n\"\"\"\n        return None\n        \n    def handle_repeat_pattern(self, task_data, analysis):\n        inp = task_data['train'][0]['input']\n        out = task_data['train'][0]['output']\n        ry = len(out) // len(inp)\n        rx = len(out[0]) // len(inp[0])\n        tiled = [[inp[i % len(inp)][j % len(inp[0])]\n                  for j in range(len(inp[0]) * rx)]\n                 for i in range(len(inp) * ry)]\n        if tiled == out:\n            return f\"\"\"def p(g):\n ry, rx = {ry}, {rx}\n h, w = len(g), len(g[0])\n return [[g[i % h][j % w] for j in range(w * rx)]\n         for i in range(h * ry)]\n\"\"\"\n\n    def handle_checkerboard(self, task_data, analysis):\n        out = np.array(task_data['train'][0]['output'])\n        if np.all(out==out[0,0] ^ ((np.indices(out.shape).sum(0))%2)):\n            base=int(out[0,0]); other=int(list(set(out.ravel()))[1])\n            return f\"\"\"def p(g):\n return[[({base} if (i+j)%2==0 else {other}) for j in range(len(g[0]))] for i in range(len(g))]\n\"\"\"\n        return None\n\n    def handle_vertical_stripes(self, task_data, analysis):\n        inp,out=(np.array(task_data['train'][0][k]) for k in('input','output'))\n        if out.shape!=inp.shape: return None\n        for j in range(out.shape[1]):\n            if len(set(out[:,j]))==1 and not np.array_equal(inp[:,j],out[:,j]):\n                return f\"\"\"def p(g):\n H,W=len(g),len(g[0])\n return[[g[i][0] for _ in range(W)] for i in range(H)]\n\"\"\"\n        return None\n\n    def handle_horizontal_stripes(self, task_data, analysis):\n        inp,out=(np.array(task_data['train'][0][k]) for k in('input','output'))\n        if out.shape!=inp.shape: return None\n        for i in range(out.shape[0]):\n            if len(set(out[i]))==1 and not np.array_equal(inp[i],out[i]):\n                return \"\"\"def p(g):\n return[[x for _ in g[0]] for x in [r[0] for r in g]]\n\"\"\"\n        return None\n\n    def handle_majority_color_fill(self, task_data, analysis):\n        inp, out=(task_data['train'][0]['input'],task_data['train'][0]['output'])\n        flat=[x for r in inp for x in r]\n        maj=max(set(flat), key=flat.count)\n        if all(all(x==maj for x in r) for r in out):\n            return f\"\"\"def p(g):\n from collections import Counter\n flat=[x for r in g for x in r]\n maj=Counter(flat).most_common(1)[0][0]\n return[[maj]*len(g[0]) for _ in g]\n\"\"\"\n        return None\n\n    def handle_cross_lines(self, task_data, analysis):\n        inp,out=(np.array(task_data['train'][0][k]) for k in('input','output'))\n        if out.shape!=inp.shape: return None\n        H,W=out.shape; cy,cx=H//2,W//2\n        if np.all(out[cy,:]==out[cy,0]) and np.all(out[:,cx]==out[0,cx]):\n            c=int(out[cy,cx])\n            return f\"\"\"def p(g):\n H,W=len(g),len(g[0]);cy,cx=H//2,W//2\n return[[{c} if i==cy or j==cx else g[i][j] for j in range(W)] for i in range(H)]\n\"\"\"\n        return None\n\n    def handle_color_swap(self, task_data, analysis):\n        inp,out=(task_data['train'][0]['input'],task_data['train'][0]['output'])\n        in_set={x for r in inp for x in r}; out_set={x for r in out for x in r}\n        if len(in_set)==len(out_set)==2 and in_set==out_set:\n            a,b=in_set\n            return f\"\"\"def p(g):\n return[[{a} if x=={b} else ({b} if x=={a} else x) for x in r] for r in g]\n\"\"\"\n        return None\n\n    def handle_crop_center(self, task_data, analysis):\n        inp,out=(np.array(task_data['train'][0][k]) for k in('input','output'))\n        h,w=inp.shape; ho,wo=out.shape\n        cy,cx=(h-ho)//2,(w-wo)//2\n        if np.array_equal(out, inp[cy:cy+ho, cx:cx+wo]):\n            return f\"\"\"def p(g):\n h,w=len(g),len(g[0]);ho,wo={ho},{wo}\n cy,cx=(h-ho)//2,(w-wo)//2\n return[g[i][cx:cx+wo] for i in range(cy,cy+ho)]\n\"\"\"\n        return None\n\n    def handle_outline_only(self, task_data, analysis):\n        inp, out = (np.array(task_data['train'][0][k]) for k in ('input','output'))\n        if inp.shape != out.shape: return None\n        h, w = inp.shape\n        edge = np.zeros_like(inp)\n        for i in range(h):\n            for j in range(w):\n                if inp[i,j]==0: continue\n                nbor = [(i+1,j),(i-1,j),(i,j+1),(i,j-1)]\n                if any(not(0<=y<h and 0<=x<w) or inp[y,x]==0 for y,x in nbor):\n                    edge[i,j] = inp[i,j]\n        if np.array_equal(out, edge):\n            return \"\"\"def p(g):\n h,w=len(g),len(g[0])\n return[[ g[i][j] if g[i][j] and any( (y<0 or y>=h or x<0 or x>=w or g[y][x]==0)\n        for y,x in ((i+1,j),(i-1,j),(i,j+1),(i,j-1)) )\n        else 0 for j in range(w)] for i in range(h)]\n\"\"\"\n        return None\n\n    def handle_center_object(self, task_data, analysis):\n        inp = task_data['train'][0]['input']\n        out = task_data['train'][0]['output']\n        if len(inp) != len(out) or len(inp[0]) != len(out[0]):\n            return None\n        ys = [i for i, row in enumerate(inp) for v in row if v]\n        xs = [j for row in inp for j, v in enumerate(row) if v]\n        if not ys:\n            return None\n        y0, y1 = min(ys), max(ys) + 1\n        x0, x1 = min(xs), max(xs) + 1\n        crop = [r[x0:x1] for r in inp[y0:y1]]\n        H, W = len(inp), len(inp[0])\n        h, w = len(crop), len(crop[0])\n        cy, cx = (H - h) // 2, (W - w) // 2\n        canvas = [[0] * W for _ in range(H)]\n        for i in range(h):\n            canvas[cy + i][cx:cx + w] = crop[i][:]\n        if canvas == out:\n            return f'''def p(g):\n H, W = len(g), len(g[0])\n ys = [i for i,row in enumerate(g) for v in row if v]\n xs = [j for row in g for j,v in enumerate(row) if v]\n y0, y1 = min(ys), max(ys)+1\n x0, x1 = min(xs), max(xs)+1\n crop = [r[x0:x1] for r in g[y0:y1]]\n h, w = len(crop), len(crop[0])\n cy, cx = (H-h)//2, (W-w)//2\n res = [[0]*W for _ in range(H)]\n for i in range(h):\n     res[cy+i][cx:cx+w] = crop[i][:]\n return res\n'''\n\n    def handle_diagonal_mirror(self, task_data, analysis):\n        inp, out = (np.array(task_data['train'][0][k]) for k in ('input','output'))\n        if inp.shape[0]!=inp.shape[1]: return None\n        if np.array_equal(out, inp.T):\n            return \"\"\"def p(g):\n return[list(r) for r in zip(*g)]\n\"\"\"\n        return None\n\n    def handle_color_palette_row(self, task_data, analysis):\n        inp, out = (task_data['train'][0]['input'], task_data['train'][0]['output'])\n        colors = sorted({x for row in inp for x in row})\n        if len(out)==1 and out[0]==colors:\n            return \"\"\"def p(g):\n c=sorted({x for r in g for x in r})\n return [c]\n\"\"\"\n        return None\n\n    def handle_bounding_fill_by_color(self, task_data, analysis):\n        inp = task_data['train'][0]['input']\n        out = task_data['train'][0]['output']\n        if len(inp) != len(out) or len(inp[0]) != len(out[0]):\n            return None\n        H, W = len(inp), len(inp[0])\n        res = [[0] * W for _ in range(H)]\n        colors = {v for row in inp for v in row if v}\n        for c in colors:\n            ys = [i for i, row in enumerate(inp) for v in row if v == c]\n            xs = [j for i, row in enumerate(inp) for j, v in enumerate(row) if v == c]\n            y0, y1 = min(ys), max(ys) + 1\n            x0, x1 = min(xs), max(xs) + 1\n            for i in range(y0, y1):\n                for j in range(x0, x1):\n                    res[i][j] = c\n        if res == out:\n            return '''def p(g):\n H, W = len(g), len(g[0])\n res = [[0]*W for _ in range(H)]\n colors = {v for row in g for v in row if v}\n for c in colors:\n     ys = [i for i,row in enumerate(g) for v in row if v==c]\n     xs = [j for i,row in enumerate(g) for j,v in enumerate(row) if v==c]\n     y0,y1 = min(ys), max(ys)+1\n     x0,x1 = min(xs), max(xs)+1\n     for i in range(y0,y1):\n         for j in range(x0,x1):\n             res[i][j] = c\n return res\n'''\n\n    def handle_row_col_propagation(self, task_data, analysis):\n        inp, out = (np.array(task_data['train'][0][k]) for k in ('input','output'))\n        if inp.shape!=out.shape: return None\n        if all((np.unique(o).size==1) or np.array_equal(o, i)\n               for i,o in zip(inp, out)):\n            return \"\"\"def p(g):\n return[[g[r][0] if len(set(g[r]))>1 else g[r][c]\n         for c in range(len(g[0]))] for r in range(len(g))]\n\"\"\"\n        if all((np.unique(out[:,c]).size==1) or np.array_equal(out[:,c], inp[:,c])\n               for c in range(inp.shape[1])):\n            return \"\"\"def p(g):\n H,W=len(g),len(g[0])\n cols=[[g[r][c] for r in range(H)] for c in range(W)]\n for c in range(W):\n     if len(set(cols[c]))>1:\n         cols[c]=[cols[c][0]]*H\n return[[cols[c][r] for c in range(W)] for r in range(H)]\n\"\"\"\n        return None\n\n    def handle_single_color_output(self, task_data, analysis):\n        cands = {int(x) for ex in task_data['train']\n                       for row in ex['output'] for x in row}\n        if len(cands) == 1:\n            c = cands.pop()\n            return f\"\"\"def p(g):\n return[[{c}]*len(g[0]) for _ in g]\n\"\"\"\n        return None\n\n    def handle_bounding_crop(self, task_data, analysis):\n        inp, out = map(np.array, (task_data['train'][0]['input'],\n                                  task_data['train'][0]['output']))\n        nz = np.argwhere(inp)\n        if not len(nz):\n            return None\n        (y0,x0),(y1,x1) = nz.min(0), nz.max(0)+1\n        if np.array_equal(out, inp[y0:y1, x0:x1]):\n            return f\"\"\"def p(g):\n c=[(i,j)for i,r in enumerate(g)for j,x in enumerate(r)if x]\n y0=min(i for i,_ in c); y1=max(i for i,_ in c)+1\n x0=min(j for _,j in c); x1=max(j for _,j in c)+1\n return[g[i][x0:x1] for i in range(y0,y1)]\n\"\"\"\n        return None\n\n    def handle_overlay_fill(self, task_data, analysis):\n        inp, out = map(np.array, (task_data['train'][0]['input'],\n                                  task_data['train'][0]['output']))\n        if inp.shape != out.shape:\n            return None\n        if np.all(np.where(out==0, inp, out) == out):\n            return \"\"\"def p(g):\n return[[g[i][j] if r[j]==0 else r[j]\n         for j in range(len(r))]\n        for i,r in enumerate(g)]\n\"\"\"\n        return None\n\n    def handle_remove_color(self, task_data, analysis):\n        inp, out = map(np.array, (task_data['train'][0]['input'],\n                                  task_data['train'][0]['output']))\n        if inp.shape != out.shape:\n            return None\n        diff = inp != out\n        removed = {int(inp[i,j]) for (i,j) in zip(*np.where(diff))}\n        if len(removed)==1 and np.all(out[diff]==0):\n            c = removed.pop()\n            return f\"\"\"def p(g):\n return[[0 if x=={c} else x for x in r] for r in g]\n\"\"\"\n        return None\n        \n    def handle_color_mapping(self, task_data, analysis):\n        color_map = {}\n        for pos, changes in analysis['color_changes'].items():\n            if len(changes) == 1:\n                src, dest = next(iter(changes))\n                if src not in color_map:\n                    color_map[src] = dest\n                elif color_map[src] != dest:\n                    return None\n        \n        if not color_map:\n            return None\n            \n        cases = \"\\n\".join([f\"    if x=={int(src)}: return {int(dest)}\" for src, dest in color_map.items()])\n        return f\"\"\"def p(g):\n return[[(lambda x:\n{cases}\n    else x)(x)for x in r]for r in g]\n\"\"\"\n    \n    def handle_grid_operations(self, task_data, analysis):\n        if not analysis['symmetry']:\n            return None\n        sym_type = analysis['symmetry']\n        if sym_type == 'rotate_90':\n            return \"\"\"def p(g):\n return[list(r)for r in zip(*g[::-1])]\n\"\"\"\n        if sym_type == 'rotate_180':\n            return \"\"\"def p(g):\n return[r[::-1]for r in g[::-1]]\n\"\"\"\n        if sym_type == 'rotate_270':\n            return \"\"\"def p(g):\n return[list(r)for r in zip(*g)][::-1]\n\"\"\"\n        if sym_type == 'flip_vertical':\n            return \"\"\"def p(g):\n return g[::-1]\n\"\"\"\n        if sym_type == 'flip_horizontal':\n            return \"\"\"def p(g):\n return[r[::-1]for r in g]\n\"\"\"\n        return None\n\n    def handle_resize(self, task_data, analysis):\n        first_pair = task_data['train'][0]\n        in_grid = np.array(first_pair['input'])\n        out_grid = np.array(first_pair['output'])\n        h_out, w_out = out_grid.shape\n        h_in, w_in = in_grid.shape\n        \n        if (h_out < h_in) or (w_out < w_in):\n            return f\"\"\"def p(g):\n return [row[:{w_out}] for row in g[:{h_out}]]\n\"\"\"\n        if (h_out > h_in) or (w_out > w_in):\n            return f\"\"\"def p(g):\n return [row+[0]*({w_out}-len(row)) for row in g]+[[0]*{w_out} for _ in range({h_out}-len(g))]\n\"\"\"\n        return None\n\n    def handle_object_operations(self, task_data, analysis):\n        obj_changes = [obj for example in analysis['object_properties'] for obj in example]\n        if not obj_changes:\n            return None\n        if len(obj_changes) == 1 and isinstance(obj_changes[0]['position_change'], np.ndarray):\n            dy, dx = map(int, obj_changes[0]['position_change'])\n            return f\"\"\"def p(g):\n h,w=len(g),len(g[0])\n return[[g[i-{dy}][j-{dx}] if 0<=i-{dy}<h and 0<=j-{dx}<w else 0 for j in range(w)]for i in range(h)]\n\"\"\"\n        return None\n    \n    def handle_pattern_replication(self, task_data, analysis):\n        first_pair = task_data['train'][0]\n        in_grid = np.array(first_pair['input'])\n        out_grid = np.array(first_pair['output'])\n        if (out_grid.shape[0] % in_grid.shape[0] == 0 and \n            out_grid.shape[1] % in_grid.shape[1] == 0):\n            tiles_y = out_grid.shape[0] // in_grid.shape[0]\n            tiles_x = out_grid.shape[1] // in_grid.shape[1]\n            return f\"\"\"def p(g):\n return[[g[i%{in_grid.shape[0]}][j%{in_grid.shape[1]}]for j in range(len(g[0])*{tiles_x})]for i in range(len(g)*{tiles_y})]\n\"\"\"\n        return None\n    \n    def handle_mirror_symmetry(self, task_data, analysis):\n        first_pair = task_data['train'][0]\n        in_grid = np.array(first_pair['input'])\n        out_grid = np.array(first_pair['output'])\n        if np.array_equal(out_grid, in_grid[:, ::-1]) and in_grid.shape == out_grid.shape:\n            return \"\"\"def p(g):\n return[r[::-1]for r in g]\n\"\"\"\n        return None\n    \n    def handle_arithmetic_operations(self, task_data, analysis):\n        if not analysis['arithmetic']:\n            return None\n        op, val = analysis['arithmetic']\n        if op == 'add':\n            return f\"\"\"def p(g):\n return[[x+{val}for x in r]for r in g]\n\"\"\"\n        if op == 'multiply':\n            return f\"\"\"def p(g):\n return[[x*{val}for x in r]for r in g]\n\"\"\"\n        return None\n    \n    def handle_conditional_operations(self, task_data, analysis):\n        first_pair = task_data['train'][0]\n        in_grid = np.array(first_pair['input'])\n        out_grid = np.array(first_pair['output'])\n        if in_grid.shape != out_grid.shape:\n            return None\n        diff = out_grid - in_grid\n        changed = np.where(diff != 0)\n        for i,j in zip(*changed):\n            if i > 0 and in_grid[i-1,j] == out_grid[i,j]:\n                return \"\"\"def p(g):\n return[[g[i-1][j]if i>0 and g[i][j]!=0 else g[i][j]for j in range(len(g[0]))]for i in range(len(g))]\n\"\"\"\n            if j > 0 and in_grid[i,j-1] == out_grid[i,j]:\n                return \"\"\"def p(g):\n return[[g[i][j-1]if j>0 and g[i][j]!=0 else g[i][j]for j in range(len(g[0]))]for i in range(len(g))]\n\"\"\"\n        return None\n\n    def visualize_grid(self, grid: np.ndarray, title: str = \"Grid\", ax=None) -> None:\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n        \n        color_matrix = np.zeros((*grid.shape, 3))\n        for i in range(grid.shape[0]):\n            for j in range(grid.shape[1]):\n                color_hex = ARC_COLORS[int(grid[i, j]) % 10]\n                color_matrix[i, j] = [int(color_hex[i:i+2], 16)/255 for i in (1, 3, 5)]\n        \n        ax.imshow(color_matrix, interpolation='nearest')\n        ax.set_title(title)\n        ax.grid(True, which='both', color='gray', linewidth=0.5)\n        ax.set_xticks(np.arange(-0.5, grid.shape[1], 1), minor=True)\n        ax.set_yticks(np.arange(-0.5, grid.shape[0], 1), minor=True)\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        \n    def visualize_transformation(self, input_grid: np.ndarray, output_grid: np.ndarray, \n                               predicted_grid: Optional[np.ndarray] = None,\n                               pattern_name: str = \"\") -> None:\n        if not self.enable_visuals:\n            return\n            \n        n_plots = 3 if predicted_grid is not None else 2\n        fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n        \n        self.visualize_grid(input_grid, \"Input\", axes[0])\n        self.visualize_grid(output_grid, \"Expected Output\", axes[1])\n        \n        if predicted_grid is not None:\n            self.visualize_grid(predicted_grid, \"Predicted Output\", axes[2])\n            match_pct = np.mean(output_grid == predicted_grid) * 100\n            axes[2].text(0.5, -0.1, f\"Match: {match_pct:.1f}%\", \n                        transform=axes[2].transAxes, ha='center')\n        \n        if pattern_name:\n            fig.suptitle(f\"Pattern: {pattern_name}\", fontsize=14)\n        \n        plt.tight_layout()\n        plt.show()\n        \n    def analyze_pattern_distribution(self, task_data: Dict) -> Dict[str, Any]:\n        pattern_counts = defaultdict(int)\n        pattern_categories = defaultdict(int)\n        \n        for handler, name, category in self.pattern_handlers:\n            analysis = self.analyze_task(task_data)\n            solution = handler(task_data, analysis)\n            if solution and self.verify_solution(solution, task_data):\n                pattern_counts[name] += 1\n                pattern_categories[category] += 1\n        \n        return {\n            'pattern_counts': dict(pattern_counts),\n            'category_counts': dict(pattern_categories),\n            'total_patterns': sum(pattern_counts.values())\n        }\n        \n    def visualize_pattern_analysis(self, analysis_results: List[Dict]) -> None:\n        if not self.enable_visuals:\n            return\n            \n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        \n        ax1 = axes[0, 0]\n        patterns = list(self.pattern_success_rates.keys())[:20]\n        success_rates = [self.pattern_success_rates[p]['successes'] / \n                        max(self.pattern_success_rates[p]['attempts'], 1) * 100 \n                        for p in patterns]\n        \n        ax1.barh(patterns, success_rates)\n        ax1.set_xlabel('Success Rate (%)')\n        ax1.set_title('Pattern Success Rates')\n        ax1.grid(True, alpha=0.3)\n        \n        ax2 = axes[0, 1]\n        categories = defaultdict(int)\n        for results in analysis_results:\n            for cat, count in results.get('category_counts', {}).items():\n                categories[cat] += count\n                \n        if categories:\n            ax2.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%')\n            ax2.set_title('Pattern Category Distribution')\n        \n        ax3 = axes[1, 0]\n        if self.solution_complexity_scores:\n            complexities = list(self.solution_complexity_scores.values())\n            ax3.hist(complexities, bins=30, edgecolor='black')\n            ax3.set_xlabel('Complexity Score')\n            ax3.set_ylabel('Frequency')\n            ax3.set_title('Solution Complexity Distribution')\n            ax3.grid(True, alpha=0.3)\n        \n        ax4 = axes[1, 1]\n        if self.fitness_history:\n            generations = range(len(self.fitness_history))\n            best_fitness = [gen['best'] for gen in self.fitness_history]\n            avg_fitness = [gen['average'] for gen in self.fitness_history]\n            \n            ax4.plot(generations, best_fitness, 'b-', label='Best Fitness')\n            ax4.plot(generations, avg_fitness, 'r--', label='Average Fitness')\n            ax4.set_xlabel('Generation')\n            ax4.set_ylabel('Fitness Score')\n            ax4.set_title('Genetic Algorithm Progress')\n            ax4.legend()\n            ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n    def calculate_solution_complexity(self, solution_code: str) -> float:\n        lines = solution_code.strip().split('\\n')\n        char_count = len(solution_code)\n        line_count = len(lines)\n        \n        loops = sum(1 for line in lines if 'for' in line or 'while' in line)\n        conditionals = sum(1 for line in lines if 'if' in line)\n        comprehensions = sum(1 for line in lines if '[' in line and 'for' in line)\n        \n        complexity = (char_count * 0.01 + \n                     line_count * 2 + \n                     loops * 5 + \n                     conditionals * 3 + \n                     comprehensions * 4)\n        \n        return complexity\n        \n    def genetic_optimize_solution(self, task_data: Dict, initial_solution: str) -> str:\n        self.population = [initial_solution]\n        \n        for _ in range(19):\n            variant = self.mutate_solution(initial_solution)\n            self.population.append(variant)\n        \n        for generation in range(self.genetic_generations):\n            fitness_scores = []\n            for solution in self.population:\n                fitness = self.evaluate_solution_fitness(solution, task_data)\n                fitness_scores.append(fitness)\n            \n            self.fitness_history.append({\n                'best': max(fitness_scores),\n                'average': np.mean(fitness_scores),\n                'worst': min(fitness_scores)\n            })\n            \n            new_population = []\n            \n            elite_count = len(self.population) // 5\n            elite_indices = np.argsort(fitness_scores)[-elite_count:]\n            for idx in elite_indices:\n                new_population.append(self.population[idx])\n            \n            while len(new_population) < len(self.population):\n                parent1 = self.tournament_select(self.population, fitness_scores)\n                parent2 = self.tournament_select(self.population, fitness_scores)\n                \n                if random.random() < 0.7:\n                    child = self.crossover_solutions(parent1, parent2)\n                else:\n                    child = parent1\n                    \n                if random.random() < 0.3:\n                    child = self.mutate_solution(child)\n                    \n                new_population.append(child)\n            \n            self.population = new_population\n        \n        final_fitness = [self.evaluate_solution_fitness(s, task_data) for s in self.population]\n        best_idx = np.argmax(final_fitness)\n        return self.population[best_idx]\n        \n    def evaluate_solution_fitness(self, solution: str, task_data: Dict) -> float:\n        fitness = 0.0\n        \n        if self.verify_solution(solution, task_data):\n            fitness += 100.0\n        \n        char_count = len(solution.replace(' ', '').replace('\\n', ''))\n        fitness += max(0, 50 - char_count * 0.1)\n        \n        complexity = self.calculate_solution_complexity(solution)\n        fitness -= complexity * 0.01\n        \n        generalization_score = self.test_generalization(solution, task_data)\n        fitness += generalization_score * 20\n        \n        return max(0, fitness)\n        \n    def test_generalization(self, solution: str, task_data: Dict) -> float:\n        try:\n            namespace = {}\n            exec(solution, namespace)\n            p = namespace['p']\n            \n            correct = 0\n            total = 0\n            \n            for example in task_data['train']:\n                input_grid = example['input']\n                expected = example['output']\n                actual = p(input_grid)\n                if actual == expected:\n                    correct += 1\n                total += 1\n                \n                noisy_input = self.add_minor_noise(input_grid)\n                try:\n                    noisy_output = p(noisy_input)\n                    if isinstance(noisy_output, list) and len(noisy_output) > 0:\n                        correct += 0.5\n                    total += 1\n                except:\n                    pass\n                    \n            return correct / max(total, 1)\n        except:\n            return 0.0\n            \n    def add_minor_noise(self, grid: List[List[int]]) -> List[List[int]]:\n        noisy = [row[:] for row in grid]\n        h, w = len(grid), len(grid[0]) if grid else 0\n        \n        changes = random.randint(0, min(2, h*w//10))\n        for _ in range(changes):\n            i = random.randint(0, h-1)\n            j = random.randint(0, w-1)\n            colors = {grid[y][x] for y in range(h) for x in range(w)}\n            if len(colors) > 1:\n                noisy[i][j] = random.choice(list(colors - {grid[i][j]}))\n                \n        return noisy\n        \n    def tournament_select(self, population: List[str], fitness_scores: List[float]) -> str:\n        tournament_size = 3\n        indices = random.sample(range(len(population)), tournament_size)\n        tournament_fitness = [fitness_scores[i] for i in indices]\n        winner_idx = indices[np.argmax(tournament_fitness)]\n        return population[winner_idx]\n        \n    def crossover_solutions(self, parent1: str, parent2: str) -> str:\n        lines1 = parent1.strip().split('\\n')\n        lines2 = parent2.strip().split('\\n')\n        \n        if len(lines1) < 3 or len(lines2) < 3:\n            return parent1\n            \n        child_lines = [lines1[0]]\n        \n        body1 = lines1[1:]\n        body2 = lines2[1:]\n        \n        for i in range(max(len(body1), len(body2))):\n            if random.random() < 0.5:\n                if i < len(body1):\n                    child_lines.append(body1[i])\n            else:\n                if i < len(body2):\n                    child_lines.append(body2[i])\n                    \n        return '\\n'.join(child_lines)\n        \n    def mutate_solution(self, solution: str) -> str:\n        lines = solution.strip().split('\\n')\n        mutation_type = random.choice(['simplify', 'compress', 'reorder'])\n        \n        if mutation_type == 'simplify':\n            for i, line in enumerate(lines):\n                if 'for i in range(len(' in line:\n                    lines[i] = line.replace('for i in range(len(', 'for i,_ in enumerate(')\n                elif 'lambda' not in line and ':' in line and '=' in line:\n                    parts = line.split('=', 1)\n                    if len(parts) == 2 and ';' not in line:\n                        lines[i] = parts[0] + '=' + parts[1].strip()\n                        \n        elif mutation_type == 'compress':\n            for i, line in enumerate(lines):\n                if i > 0:\n                    lines[i] = line.replace(' ', '', 1)\n                    \n        elif mutation_type == 'reorder':\n            if len(lines) > 3:\n                idx1, idx2 = random.sample(range(1, len(lines)-1), 2)\n                lines[idx1], lines[idx2] = lines[idx2], lines[idx1]\n                \n        return '\\n'.join(lines)\n        \n    def explain_pattern(self, pattern_name: str, task_data: Dict, \n                       solution: str) -> Dict[str, Any]:\n        explanation = {\n            'pattern_name': pattern_name,\n            'description': '',\n            'key_features': [],\n            'transformation_steps': [],\n            'code_explanation': [],\n            'visual_example': None\n        }\n        \n        for handler, name, category in self.pattern_handlers:\n            if name == pattern_name:\n                explanation['description'] = f\"{name} ({category} pattern)\"\n                explanation['category'] = category\n                break\n                \n        example = task_data['train'][0]\n        input_grid = np.array(example['input'])\n        output_grid = np.array(example['output'])\n        \n        if input_grid.shape != output_grid.shape:\n            explanation['key_features'].append(\n                f\"Shape change: {input_grid.shape} → {output_grid.shape}\"\n            )\n            \n        input_colors = set(input_grid.flatten())\n        output_colors = set(output_grid.flatten())\n        if input_colors != output_colors:\n            explanation['key_features'].append(\n                f\"Color change: {input_colors} → {output_colors}\"\n            )\n            \n        lines = solution.strip().split('\\n')\n        for line in lines:\n            if 'for' in line:\n                explanation['code_explanation'].append(\"Uses iteration\")\n            if 'if' in line:\n                explanation['code_explanation'].append(\"Contains conditional logic\")\n            if 'zip' in line:\n                explanation['code_explanation'].append(\"Combines multiple sequences\")\n            if '[::-1]' in line:\n                explanation['code_explanation'].append(\"Reverses sequences\")\n                \n        return explanation\n        \n    def generate_detailed_report(self, task_id: str, task_data: Dict, \n                               solution: str, pattern_name: str) -> Dict[str, Any]:\n        report = {\n            'task_id': task_id,\n            'pattern': pattern_name,\n            'solution_length': len(solution),\n            'complexity_score': self.calculate_solution_complexity(solution),\n            'verification_status': 'PASSED' if self.verify_solution(solution, task_data) else 'FAILED',\n            'generalization_score': self.test_generalization(solution, task_data),\n            'explanation': self.explain_pattern(pattern_name, task_data, solution)\n        }\n        \n        if pattern_name in self.pattern_success_rates:\n            stats = self.pattern_success_rates[pattern_name]\n            report['pattern_success_rate'] = stats['successes'] / max(stats['attempts'], 1)\n            \n        return report\n    \n    def analyze_task(self, task_data):\n        all_examples = task_data['train'] + task_data['test'] + task_data['arc-gen']\n        analysis = {\n            'color_changes': defaultdict(set),\n            'shape_changes': set(),\n            'object_properties': [],\n            'symmetry': None,\n            'arithmetic': None,\n            'pattern_metrics': {},\n            'statistical_features': {}\n        }\n        \n        for example in all_examples:\n            in_grid = np.array(example['input'])\n            out_grid = np.array(example['output'])\n            \n            analysis['shape_changes'].add((in_grid.shape, out_grid.shape))\n            \n            for (i,j), val in np.ndenumerate(in_grid):\n                if i < out_grid.shape[0] and j < out_grid.shape[1]:\n                    if in_grid[i,j] != out_grid[i,j]:\n                        analysis['color_changes'][(i,j)].add((int(in_grid[i,j]), int(out_grid[i,j])))\n            \n            analysis['statistical_features']['input_colors'] = len(np.unique(in_grid))\n            analysis['statistical_features']['output_colors'] = len(np.unique(out_grid))\n            analysis['statistical_features']['input_density'] = np.mean(in_grid > 0)\n            analysis['statistical_features']['output_density'] = np.mean(out_grid > 0)\n            \n            analysis['object_properties'].append(self.analyze_objects(in_grid, out_grid))\n            \n            if analysis['symmetry'] is None:\n                analysis['symmetry'] = self.detect_symmetry(in_grid, out_grid)\n            \n            if analysis['arithmetic'] is None:\n                analysis['arithmetic'] = self.detect_arithmetic(in_grid, out_grid)\n        \n        return analysis\n    \n    def analyze_objects(self, in_grid, out_grid):\n        structure = np.ones((3,3), dtype=int)\n        labeled_in, n_in = label(in_grid > 0, structure)\n        labeled_out, n_out = label(out_grid > 0, structure)\n        \n        objects = []\n        for i in range(1, n_in+1):\n            in_obj = (labeled_in == i)\n            out_obj = None\n            if i <= n_out:\n                out_obj = (labeled_out == i)\n            objects.append({\n                'in_size': int(in_obj.sum()),\n                'out_size': int(out_obj.sum()) if out_obj is not None else 0,\n                'position_change': self.detect_position_change(in_obj, out_obj)\n            })\n        return objects\n    \n    def detect_position_change(self, in_obj, out_obj):\n        if out_obj is None:\n            return \"removed\"\n        in_pos = np.argwhere(in_obj).mean(axis=0)\n        out_pos = np.argwhere(out_obj).mean(axis=0)\n        return out_pos - in_pos\n    \n    def detect_symmetry(self, in_grid, out_grid):\n        if np.array_equal(out_grid, np.rot90(in_grid, 1)):\n            return 'rotate_90'\n        if np.array_equal(out_grid, np.rot90(in_grid, 2)):\n            return 'rotate_180'\n        if np.array_equal(out_grid, np.rot90(in_grid, 3)):\n            return 'rotate_270'\n        if np.array_equal(out_grid, np.flipud(in_grid)):\n            return 'flip_vertical'\n        if np.array_equal(out_grid, np.fliplr(in_grid)):\n            return 'flip_horizontal'\n        return None\n    \n    def detect_arithmetic(self, in_grid, out_grid):\n        if in_grid.shape != out_grid.shape:\n            return None\n            \n        diff = out_grid - in_grid\n        if np.all(diff == diff[0,0]):\n            return ('add', int(diff[0,0]))\n        \n        with np.errstate(divide='ignore', invalid='ignore'):\n            safe_in = np.where(in_grid==0, 1, in_grid)\n            ratio = out_grid / safe_in\n            if np.all(ratio[in_grid > 0] == ratio[in_grid > 0][0]):\n                return ('multiply', float(ratio[in_grid > 0][0]))\n        return None\n    \n    def generate_solution(self, task_data):\n        if self.llm_generator:\n            try:\n                llm_code = self.llm_generator.generate_code(task_data['train'])\n                if llm_code and self.verify_solution(llm_code, task_data):\n                    return llm_code, \"LLM Generated\"\n            except:\n                pass\n        \n        analysis = self.analyze_task(task_data)\n        \n        for handler, name, category in self.pattern_handlers:\n            self.pattern_success_rates[name]['attempts'] += 1\n            \n            try:\n                solution = handler(task_data, analysis)\n                if solution and self.verify_solution(solution, task_data):\n                    self.pattern_success_rates[name]['successes'] += 1\n                    \n                    complexity = self.calculate_solution_complexity(solution)\n                    self.solution_complexity_scores[name] = complexity\n                    \n                    if self.genetic_generations > 0:\n                        solution = self.genetic_optimize_solution(task_data, solution)\n                    \n                    return solution, name\n            except Exception as e:\n                print(f\"Error in {name}: {str(e)}\")\n                continue\n        \n        return \"\"\"def p(g):\n return [row[:] for row in g]\n\"\"\", \"Identity (Fallback)\"\n    \n    def verify_solution(self, solution_code, task_data):\n        try:\n            namespace = {}\n            exec(solution_code, namespace)\n            p = namespace['p']\n            \n            examples = task_data['train'] + task_data['test'] + task_data['arc-gen']\n            \n            if len(examples) >= 3:\n                test_indices = random.sample(range(len(examples)), min(3, len(examples)))\n                for idx in test_indices:\n                    example = examples[idx]\n                    input_grid = example['input']\n                    expected = example['output']\n                    actual = p(input_grid)\n                    if actual != expected:\n                        return False\n            else:\n                for example in examples:\n                    input_grid = example['input']\n                    expected = example['output']\n                    actual = p(input_grid)\n                    if actual != expected:\n                        return False\n                        \n            return True\n        except Exception as e:\n            return False\n\ndef create_ultimate_arc_solutions():\n    generator = UltimateARCSolutionGenerator(enable_visuals=False, genetic_generations=5)\n    solutions = {}\n    reports = []\n    analysis_results = []\n    \n    print(\"🚀 Starting Ultimate ARC Solution Generation with LLM...\\n\")\n    \n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        try:\n            with open(f\"/kaggle/input/google-code-golf-2025/task{task_id}.json\") as f:\n                task_data = json.load(f)\n            \n            pattern_analysis = generator.analyze_pattern_distribution(task_data)\n            analysis_results.append(pattern_analysis)\n            \n            solution, pattern_name = generator.generate_solution(task_data)\n            solutions[task_id] = solution\n            \n            report = generator.generate_detailed_report(task_id, task_data, solution, pattern_name)\n            reports.append(report)\n            \n            if report['verification_status'] == 'PASSED':\n                print(f\"Task {task_id} ✅ - Pattern: {pattern_name} - \"\n                      f\"Complexity: {report['complexity_score']:.1f}\")\n            else:\n                print(f\"Task {task_id} ❌ - Used fallback\")\n                \n        except Exception as e:\n            print(f\"Error processing task {task_id}: {str(e)}\")\n            solutions[task_id] = \"\"\"def p(g):\n return [row[:] for row in g]\n\"\"\"\n    \n    os.makedirs(\"/kaggle/working/submission\", exist_ok=True)\n    for task_id, code in solutions.items():\n        with open(f\"/kaggle/working/submission/task{task_id}.py\", \"w\") as f:\n            f.write(code)\n    \n    with zipfile.ZipFile(\"/kaggle/working/submission.zip\", \"w\") as zipf:\n        for task_id in solutions:\n            zipf.write(f\"/kaggle/working/submission/task{task_id}.py\", \n                       f\"task{task_id}.py\")\n    \n    with open(\"/kaggle/working/detailed_reports.json\", \"w\") as f:\n        json.dump(reports, f, indent=2)\n    \n    print(\"\\n📈 Summary Statistics:\")\n    print(f\"Total tasks processed: {len(solutions)}\")\n    print(f\"Successful patterns: {sum(1 for r in reports if r['verification_status'] == 'PASSED')}\")\n    print(f\"Average complexity: {np.mean([r['complexity_score'] for r in reports]):.2f}\")\n    print(f\"Average generalization: {np.mean([r['generalization_score'] for r in reports]):.2%}\")\n    \n    print(\"\\n🎯 Top 10 Most Successful Patterns:\")\n    pattern_stats = [(name, stats['successes'], stats['attempts']) \n                     for name, stats in generator.pattern_success_rates.items()\n                     if stats['attempts'] > 0]\n    pattern_stats.sort(key=lambda x: x[1], reverse=True)\n    \n    for i, (name, successes, attempts) in enumerate(pattern_stats[:10]):\n        success_rate = successes / attempts * 100\n        print(f\"{i+1}. {name}: {successes}/{attempts} ({success_rate:.1f}%)\")\n    \n    print(\"\\n✨ Ultimate ARC solutions created: submission.zip\")\n    print(\"📄 Detailed reports saved: detailed_reports.json\")\n\nif __name__ == \"__main__\":\n    create_ultimate_arc_solutions()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}