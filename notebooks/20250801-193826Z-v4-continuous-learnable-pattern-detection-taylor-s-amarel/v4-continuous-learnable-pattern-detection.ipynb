{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13245791,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport zipfile\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom scipy.ndimage import convolve, label, distance_transform_edt, binary_erosion, binary_dilation, gaussian_filter, zoom\nfrom scipy.spatial import distance_matrix, ConvexHull\nfrom scipy.stats import mode, entropy, skew, kurtosis\nfrom scipy.signal import find_peaks, correlate2d\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                            RandomForestClassifier, GradientBoostingClassifier, \n                            ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier,\n                            BaggingClassifier, HistGradientBoostingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA, NMF, FastICA, TruncatedSVD\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, SpectralClustering\nfrom sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.svm import SVC, SVR, NuSVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, SGDClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport hashlib\nimport itertools\nimport random\nfrom functools import lru_cache\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ExhaustiveCombinatorics:\n    \"\"\"Extract ALL possible combinatorial features from grids\"\"\"\n    \n    def __init__(self):\n        self.feature_cache = {}\n        self.relationship_cache = {}\n        \n    def extract_all_cell_combinations(self, grid):\n        \"\"\"Extract features from ALL possible cell combinations\"\"\"\n        grid = np.array(grid)\n        h, w = grid.shape\n        features = []\n        \n        # 1. Single cell features (position + value)\n        for i in range(h):\n            for j in range(w):\n                features.extend([\n                    grid[i,j],  # Value\n                    i, j,  # Position\n                    i/(h+1e-10), j/(w+1e-10),  # Normalized position\n                    i*w + j,  # Raster position\n                    (i+j) % 2,  # Checkerboard position\n                    min(i, j, h-i-1, w-j-1),  # Distance to border\n                    np.sqrt(i**2 + j**2),  # Distance from origin\n                    np.sqrt((i-h/2)**2 + (j-w/2)**2),  # Distance from center\n                ])\n        \n        # 2. Pairwise cell relationships (ALL pairs)\n        cell_pairs = list(itertools.combinations(range(h*w), 2))\n        \n        for idx1, idx2 in cell_pairs[:1000]:  # Limit to prevent memory explosion\n            i1, j1 = idx1 // w, idx1 % w\n            i2, j2 = idx2 // w, idx2 % w\n            \n            features.extend([\n                grid[i1,j1] - grid[i2,j2],  # Value difference\n                abs(grid[i1,j1] - grid[i2,j2]),  # Absolute difference\n                grid[i1,j1] * grid[i2,j2],  # Product\n                max(grid[i1,j1], grid[i2,j2]),  # Max\n                min(grid[i1,j1], grid[i2,j2]),  # Min\n                1 if grid[i1,j1] == grid[i2,j2] else 0,  # Equality\n                i2 - i1,  # Row distance\n                j2 - j1,  # Column distance\n                np.sqrt((i2-i1)**2 + (j2-j1)**2),  # Euclidean distance\n                abs(i2-i1) + abs(j2-j1),  # Manhattan distance\n            ])\n        \n        # 3. Triple cell relationships\n        cell_triples = list(itertools.combinations(range(min(h*w, 20)), 3))\n        \n        for idx1, idx2, idx3 in cell_triples[:100]:  # Limit\n            i1, j1 = idx1 // w, idx1 % w\n            i2, j2 = idx2 // w, idx2 % w\n            i3, j3 = idx3 // w, idx3 % w\n            \n            v1, v2, v3 = grid[i1,j1], grid[i2,j2], grid[i3,j3]\n            \n            features.extend([\n                v1 + v2 + v3,  # Sum\n                v1 * v2 * v3,  # Product\n                max(v1, v2, v3) - min(v1, v2, v3),  # Range\n                1 if v1 == v2 == v3 else 0,  # All equal\n                1 if v1 < v2 < v3 else 0,  # Monotonic increasing\n                1 if v1 > v2 > v3 else 0,  # Monotonic decreasing\n                # Check if they form a line\n                1 if (j2-j1)*(i3-i1) == (j3-j1)*(i2-i1) else 0,\n            ])\n        \n        # 4. Row-wise combinations\n        for i in range(h):\n            row = grid[i, :]\n            \n            # All pairs in row\n            for j1, j2 in itertools.combinations(range(w), 2):\n                features.extend([\n                    row[j1] - row[j2],\n                    abs(row[j1] - row[j2]),\n                    1 if row[j1] == row[j2] else 0,\n                ])\n            \n            # Row statistics\n            try:\n                features.extend([\n                    row.mean(), row.std(), row.min(), row.max(),\n                    mode(row)[0][0] if len(row) > 0 else 0,\n                    entropy(np.bincount(row.astype(int))) if len(row) > 0 else 0,\n                    len(np.unique(row)), np.median(row),\n                    skew(row) if len(row) > 1 else 0, \n                    kurtosis(row) if len(row) > 1 else 0,\n                ])\n            except:\n                features.extend([0] * 10)\n        \n        # 5. Column-wise combinations\n        for j in range(w):\n            col = grid[:, j]\n            \n            # All pairs in column\n            for i1, i2 in itertools.combinations(range(h), 2):\n                features.extend([\n                    col[i1] - col[i2],\n                    abs(col[i1] - col[i2]),\n                    1 if col[i1] == col[i2] else 0,\n                ])\n            \n            # Column statistics\n            try:\n                features.extend([\n                    col.mean(), col.std(), col.min(), col.max(),\n                    mode(col)[0][0] if len(col) > 0 else 0,\n                    entropy(np.bincount(col.astype(int))) if len(col) > 0 else 0,\n                    len(np.unique(col)), np.median(col),\n                    skew(col) if len(col) > 1 else 0,\n                    kurtosis(col) if len(col) > 1 else 0,\n                ])\n            except:\n                features.extend([0] * 10)\n        \n        # 6. Diagonal combinations\n        # Main diagonal\n        main_diag = np.diag(grid)\n        if len(main_diag) > 0:\n            features.extend([\n                main_diag.mean(), main_diag.std(),\n                len(np.unique(main_diag)), \n                entropy(np.bincount(main_diag.astype(int))) if len(main_diag) > 0 else 0,\n            ])\n        else:\n            features.extend([0, 0, 0, 0])\n        \n        # Anti-diagonal\n        anti_diag = np.diag(np.fliplr(grid))\n        if len(anti_diag) > 0:\n            features.extend([\n                anti_diag.mean(), anti_diag.std(),\n                len(np.unique(anti_diag)),\n                entropy(np.bincount(anti_diag.astype(int))) if len(anti_diag) > 0 else 0,\n            ])\n        else:\n            features.extend([0, 0, 0, 0])\n        \n        # 7. Block-wise combinations (2x2, 3x3, etc.)\n        for block_size in [2, 3]:\n            for i in range(h - block_size + 1):\n                for j in range(w - block_size + 1):\n                    block = grid[i:i+block_size, j:j+block_size]\n                    \n                    # All pairs in block\n                    block_flat = block.flatten()\n                    for idx1, idx2 in itertools.combinations(range(len(block_flat)), 2):\n                        features.append(block_flat[idx1] - block_flat[idx2])\n                    \n                    # Block statistics\n                    features.extend([\n                        block.mean(), block.std(), block.min(), block.max(),\n                        len(np.unique(block)), \n                        entropy(np.bincount(block_flat.astype(int))) if len(block_flat) > 0 else 0,\n                    ])\n        \n        return np.array(features[:10000])  # Limit total features\n    \n    def extract_all_raster_combinations(self, grid):\n        \"\"\"Extract features from ALL possible rasterization orders\"\"\"\n        grid = np.array(grid)\n        h, w = grid.shape\n        features = []\n        \n        # Different rasterization methods\n        rasterizations = {\n            'row_major': grid.flatten(),\n            'col_major': grid.T.flatten(),\n            'spiral': self.spiral_raster(grid),\n            'zigzag': self.zigzag_raster(grid),\n            'diagonal': self.diagonal_raster(grid),\n            'reverse': grid.flatten()[::-1],\n            'random': np.random.permutation(grid.flatten()),\n            'hilbert': self.hilbert_raster(grid),\n            'morton': self.morton_raster(grid),\n        }\n        \n        for name, raster in rasterizations.items():\n            # Single element features\n            for i, val in enumerate(raster[:100]):  # Limit\n                features.extend([\n                    val,  # Value\n                    i,  # Position in raster\n                    i / (len(raster) + 1e-10),  # Normalized position\n                ])\n            \n            # Consecutive pairs\n            for i in range(min(len(raster) - 1, 100)):\n                features.extend([\n                    raster[i+1] - raster[i],  # Difference\n                    abs(raster[i+1] - raster[i]),  # Absolute difference\n                    1 if raster[i+1] > raster[i] else 0,  # Increasing\n                    1 if raster[i+1] == raster[i] else 0,  # Equal\n                ])\n            \n            # All pairs (limited)\n            for i, j in itertools.combinations(range(min(len(raster), 50)), 2):\n                features.extend([\n                    raster[j] - raster[i],\n                    abs(j - i),  # Distance in raster\n                    1 if raster[i] == raster[j] else 0,\n                ])\n            \n            # Run-length encoding features\n            runs = self.run_length_encode(raster)\n            if runs:\n                features.extend([\n                    len(runs),  # Number of runs\n                    np.mean([r[1] for r in runs]),  # Average run length\n                    np.max([r[1] for r in runs]),  # Max run length\n                    np.std([r[1] for r in runs]) if len(runs) > 1 else 0,\n                ])\n            else:\n                features.extend([0, 0, 0, 0])\n            \n            # Pattern detection in raster\n            for pattern_len in [2, 3, 4]:\n                patterns = defaultdict(int)\n                for i in range(len(raster) - pattern_len + 1):\n                    pattern = tuple(raster[i:i+pattern_len])\n                    patterns[pattern] += 1\n                \n                if patterns:\n                    features.extend([\n                        len(patterns),  # Number of unique patterns\n                        max(patterns.values()),  # Most common pattern count\n                        entropy(list(patterns.values())),\n                    ])\n                else:\n                    features.extend([0, 0, 0])\n        \n        return np.array(features[:5000])  # Limit\n    \n    def spiral_raster(self, grid):\n        \"\"\"Convert grid to spiral order\"\"\"\n        result = []\n        h, w = grid.shape\n        top, bottom, left, right = 0, h-1, 0, w-1\n        \n        while top <= bottom and left <= right:\n            # Right\n            for j in range(left, right+1):\n                result.append(grid[top, j])\n            top += 1\n            \n            # Down\n            for i in range(top, bottom+1):\n                result.append(grid[i, right])\n            right -= 1\n            \n            # Left\n            if top <= bottom:\n                for j in range(right, left-1, -1):\n                    result.append(grid[bottom, j])\n                bottom -= 1\n            \n            # Up\n            if left <= right:\n                for i in range(bottom, top-1, -1):\n                    result.append(grid[i, left])\n                left += 1\n                \n        return np.array(result)\n    \n    def zigzag_raster(self, grid):\n        \"\"\"Convert grid to zigzag order\"\"\"\n        result = []\n        h, w = grid.shape\n        \n        for i in range(h):\n            if i % 2 == 0:\n                result.extend(grid[i, :])\n            else:\n                result.extend(grid[i, ::-1])\n                \n        return np.array(result)\n    \n    def diagonal_raster(self, grid):\n        \"\"\"Convert grid to diagonal order\"\"\"\n        result = []\n        h, w = grid.shape\n        \n        # Upper diagonals\n        for d in range(w):\n            i, j = 0, d\n            while i < h and j >= 0:\n                result.append(grid[i, j])\n                i += 1\n                j -= 1\n        \n        # Lower diagonals\n        for d in range(1, h):\n            i, j = d, w-1\n            while i < h and j >= 0:\n                result.append(grid[i, j])\n                i += 1\n                j -= 1\n                \n        return np.array(result)\n    \n    def hilbert_raster(self, grid):\n        \"\"\"Hilbert curve ordering (simplified)\"\"\"\n        # Simplified version - just returns row-major for now\n        return grid.flatten()\n    \n    def morton_raster(self, grid):\n        \"\"\"Morton (Z-order) curve ordering\"\"\"\n        result = []\n        h, w = grid.shape\n        \n        # Interleave bits for Morton ordering\n        coords = []\n        for i in range(h):\n            for j in range(w):\n                # Interleave bits of i and j\n                morton = 0\n                for k in range(8):\n                    morton |= (i & (1 << k)) << k | (j & (1 << k)) << (k + 1)\n                coords.append((morton, i, j))\n        \n        # Sort by Morton code\n        coords.sort()\n        \n        for _, i, j in coords:\n            result.append(grid[i, j])\n            \n        return np.array(result)\n    \n    def run_length_encode(self, array):\n        \"\"\"Run-length encoding of array\"\"\"\n        if len(array) == 0:\n            return []\n            \n        runs = []\n        current_val = array[0]\n        current_len = 1\n        \n        for val in array[1:]:\n            if val == current_val:\n                current_len += 1\n            else:\n                runs.append((current_val, current_len))\n                current_val = val\n                current_len = 1\n                \n        runs.append((current_val, current_len))\n        return runs\n    \n    def extract_transformation_relationships(self, input_grid, output_grid):\n        \"\"\"Extract ALL possible relationships between input and output\"\"\"\n        inp = np.array(input_grid)\n        out = np.array(output_grid)\n        features = []\n        \n        # Shape relationships\n        features.extend([\n            out.shape[0] / (inp.shape[0] + 1e-10),\n            out.shape[1] / (inp.shape[1] + 1e-10),\n            (out.shape[0] * out.shape[1]) / (inp.shape[0] * inp.shape[1] + 1e-10),\n            1 if out.shape == inp.shape else 0,\n        ])\n        \n        if inp.shape == out.shape:\n            h, w = inp.shape\n            \n            # Cell-to-cell mappings (ALL cells)\n            for i in range(min(h, 10)):  # Limit\n                for j in range(min(w, 10)):\n                    features.extend([\n                        out[i,j] - inp[i,j],  # Direct difference\n                        abs(out[i,j] - inp[i,j]),  # Absolute difference\n                        1 if out[i,j] == inp[i,j] else 0,  # Unchanged\n                        out[i,j] * inp[i,j],  # Product\n                        out[i,j] / (inp[i,j] + 1e-10),  # Ratio\n                    ])\n                    \n                    # Relationship to neighbors in input\n                    for di, dj in [(0,1), (1,0), (0,-1), (-1,0)]:\n                        ni, nj = i + di, j + dj\n                        if 0 <= ni < h and 0 <= nj < w:\n                            features.extend([\n                                out[i,j] - inp[ni,nj],\n                                1 if out[i,j] == inp[ni,nj] else 0,\n                            ])\n            \n            # All pairs of cells - input to output mapping\n            for idx1 in range(min(h*w, 50)):\n                for idx2 in range(min(h*w, 50)):\n                    i1, j1 = idx1 // w, idx1 % w\n                    i2, j2 = idx2 // w, idx2 % w\n                    \n                    features.extend([\n                        1 if inp[i1,j1] == inp[i2,j2] and out[i1,j1] == out[i2,j2] else 0,  # Preserves equality\n                        1 if inp[i1,j1] < inp[i2,j2] and out[i1,j1] < out[i2,j2] else 0,  # Preserves order\n                        1 if inp[i1,j1] == out[i2,j2] else 0,  # Cross mapping\n                    ])\n        \n        # Color mapping analysis\n        color_map = {}\n        if inp.shape == out.shape:\n            for i in range(inp.shape[0]):\n                for j in range(inp.shape[1]):\n                    in_val = int(inp[i,j])\n                    out_val = int(out[i,j])\n                    if in_val not in color_map:\n                        color_map[in_val] = out_val\n                    elif color_map[in_val] != out_val:\n                        color_map = None  # Not a simple color mapping\n                        break\n                if color_map is None:\n                    break\n        \n        features.append(1 if color_map is not None else 0)\n        \n        # Pattern preservation\n        try:\n            features.extend([\n                1 if np.array_equal(inp, out) else 0,  # Identity\n                1 if inp.shape == out.shape and np.array_equal(np.rot90(inp), out) else 0,  # Rotation 90\n                1 if inp.shape == out.shape and np.array_equal(np.rot90(inp, 2), out) else 0,  # Rotation 180\n                1 if inp.shape == out.shape and np.array_equal(np.rot90(inp, 3), out) else 0,  # Rotation 270\n                1 if inp.shape == out.shape and np.array_equal(np.fliplr(inp), out) else 0,  # Horizontal flip\n                1 if inp.shape == out.shape and np.array_equal(np.flipud(inp), out) else 0,  # Vertical flip\n                1 if inp.shape == out.shape and np.array_equal(inp.T, out) else 0,  # Transpose\n            ])\n        except:\n            features.extend([0] * 7)\n        \n        return np.array(features[:5000])\n\n\nclass UltimateMegaEnsemble:\n    \"\"\"Massive ensemble of diverse models\"\"\"\n    \n    def __init__(self):\n        self.models = self._create_all_models()\n        self.model_weights = defaultdict(lambda: 1.0)\n        self.model_performance = defaultdict(list)\n        \n    def _create_all_models(self):\n        \"\"\"Create ALL available model types\"\"\"\n        models = {\n            # Tree-based\n            'rf': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n            'rf_deep': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n            'et': ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1),\n            'gb': GradientBoostingClassifier(n_estimators=50, random_state=42),\n            'xgb': xgb.XGBClassifier(n_estimators=50, random_state=42, verbosity=0),\n            'lgb': lgb.LGBMClassifier(n_estimators=50, random_state=42, verbose=-1),\n            'catboost': CatBoostClassifier(iterations=50, random_state=42, verbose=False),\n            'ada': AdaBoostClassifier(n_estimators=50, random_state=42),\n            'hist_gb': HistGradientBoostingClassifier(max_iter=50, random_state=42),\n            \n            # Neural Networks\n            'mlp': MLPClassifier(hidden_layer_sizes=(50,), random_state=42, max_iter=500),\n            'mlp_deep': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500),\n            \n            # SVM variants\n            'svc_rbf': SVC(kernel='rbf', probability=True, random_state=42),\n            'svc_linear': LinearSVC(random_state=42, max_iter=1000),\n            \n            # Linear models\n            'lr': LogisticRegression(random_state=42, max_iter=1000),\n            'sgd': SGDClassifier(random_state=42, max_iter=1000),\n            \n            # Neighbors\n            'knn5': KNeighborsClassifier(n_neighbors=5),\n            \n            # Naive Bayes\n            'gnb': GaussianNB(),\n            \n            # Discriminant Analysis\n            'lda': LinearDiscriminantAnalysis(),\n        }\n        \n        return models\n    \n    def _create_clustering_classifier(self, clusterer):\n        \"\"\"Wrapper to use clustering as classification\"\"\"\n        class ClusteringClassifier:\n            def __init__(self, clusterer):\n                self.clusterer = clusterer\n                self.cluster_to_class = {}\n                \n            def fit(self, X, y):\n                try:\n                    clusters = self.clusterer.fit_predict(X)\n                    # Map clusters to most common class\n                    for cluster in np.unique(clusters):\n                        mask = clusters == cluster\n                        if mask.sum() > 0:\n                            self.cluster_to_class[cluster] = mode(y[mask])[0][0]\n                except:\n                    pass\n                return self\n                \n            def predict(self, X):\n                try:\n                    clusters = self.clusterer.predict(X) if hasattr(self.clusterer, 'predict') else self.clusterer.fit_predict(X)\n                    return np.array([self.cluster_to_class.get(c, 0) for c in clusters])\n                except:\n                    return np.zeros(len(X))\n                \n            def predict_proba(self, X):\n                # Simple probability based on cluster assignment\n                preds = self.predict(X)\n                n_classes = max(2, len(np.unique(list(self.cluster_to_class.values()))))\n                proba = np.zeros((len(X), n_classes))\n                for i, pred in enumerate(preds):\n                    if pred < n_classes:\n                        proba[i, pred] = 1.0\n                return proba\n                \n        return ClusteringClassifier(clusterer)\n    \n    def train_all_models(self, X, y):\n        \"\"\"Train all models in the ensemble\"\"\"\n        trained_models = {}\n        \n        # Ensure we have valid data\n        if len(X) == 0 or len(y) == 0:\n            return trained_models\n            \n        # Ensure we have at least 2 classes\n        unique_classes = np.unique(y)\n        if len(unique_classes) < 2:\n            return trained_models\n        \n        for name, model in self.models.items():\n            try:\n                # Handle special cases\n                if name.startswith('mnb'):\n                    # MultinomialNB needs non-negative features\n                    X_scaled = MinMaxScaler().fit_transform(X)\n                    model.fit(X_scaled, y)\n                else:\n                    model.fit(X, y)\n                    \n                trained_models[name] = model\n            except Exception as e:\n                # Skip models that fail\n                pass\n                \n        return trained_models\n    \n    def predict_ensemble(self, models, X):\n        \"\"\"Get weighted ensemble predictions\"\"\"\n        if not models or len(X) == 0:\n            return np.zeros(len(X))\n            \n        all_predictions = []\n        all_weights = []\n        \n        for name, model in models.items():\n            try:\n                if hasattr(model, 'predict_proba'):\n                    proba = model.predict_proba(X)\n                    pred = np.argmax(proba, axis=1)\n                else:\n                    pred = model.predict(X)\n                    \n                all_predictions.append(pred)\n                all_weights.append(self.model_weights[name])\n            except:\n                pass\n                \n        if not all_predictions:\n            return np.zeros(len(X))\n            \n        # Weighted voting\n        all_predictions = np.array(all_predictions)\n        all_weights = np.array(all_weights)\n        \n        # Normalize weights\n        all_weights = all_weights / (all_weights.sum() + 1e-10)\n        \n        # Weighted mode\n        final_predictions = []\n        for i in range(len(X)):\n            votes = all_predictions[:, i]\n            weighted_votes = defaultdict(float)\n            \n            for vote, weight in zip(votes, all_weights):\n                weighted_votes[vote] += weight\n                \n            final_predictions.append(max(weighted_votes.items(), key=lambda x: x[1])[0])\n            \n        return np.array(final_predictions)\n    \n    def update_model_weights(self, model_name, performance):\n        \"\"\"Update model weights based on performance\"\"\"\n        self.model_performance[model_name].append(performance)\n        \n        # Update weight based on recent performance\n        if len(self.model_performance[model_name]) >= 5:\n            recent_perf = self.model_performance[model_name][-10:]\n            avg_perf = np.mean(recent_perf)\n            \n            # Weight between 0.5 and 2.0\n            self.model_weights[model_name] = 0.5 + 1.5 * avg_perf\n\n\nclass AdvancedGeneticEvolver:\n    \"\"\"Advanced genetic algorithm with multiple evolution strategies\"\"\"\n    \n    def __init__(self, population_size=100):\n        self.population_size = population_size\n        self.mutation_rate = 0.3\n        self.crossover_rate = 0.7\n        self.elite_ratio = 0.1\n        self.generation = 0\n        \n        # Expanded gene pool\n        self.gene_pool = self._create_comprehensive_gene_pool()\n        \n        # Evolution strategies\n        self.mutation_strategies = [\n            self.point_mutation,\n            self.swap_mutation,\n            self.creep_mutation,\n            self.boundary_mutation,\n        ]\n        \n        self.crossover_strategies = [\n            self.single_point_crossover,\n            self.two_point_crossover,\n        ]\n        \n    def _create_comprehensive_gene_pool(self):\n        \"\"\"Create comprehensive gene pool with all possible operations\"\"\"\n        gene_pool = {}\n        \n        # Basic transformations - Fixed f-string syntax\n        gene_pool['identity'] = \"g\"\n        gene_pool['rot90'] = \"[list(r)for r in zip(*g[::-1])]\"\n        gene_pool['rot180'] = \"[r[::-1]for r in g[::-1]]\"\n        gene_pool['rot270'] = \"[list(r)for r in zip(*g)][::-1]\"\n        \n        # Flips\n        gene_pool['fliph'] = \"[r[::-1]for r in g]\"\n        gene_pool['flipv'] = \"g[::-1]\"\n        gene_pool['transpose'] = \"[list(r)for r in zip(*g)]\"\n        \n        # Scaling\n        for s in [2, 3]:\n            gene_pool[f'scale{s}x'] = f\"[[g[i//{s}][j//{s}]for j in range(len(g[0])*{s})]for i in range(len(g)*{s})]\"\n            gene_pool[f'down{s}x'] = f\"[r[::{s}]for r in g[::{s}]]\"\n            \n        # Color operations\n        for c1 in range(5):\n            for c2 in range(5):\n                if c1 != c2:\n                    gene_pool[f'swap{c1}{c2}'] = f\"[[{c2}if x=={c1}else{c1}if x=={c2}else x for x in r]for r in g]\"\n                    \n        # Position-based\n        gene_pool['pos_sum'] = \"[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n        gene_pool['pos_mul'] = \"[[(i*j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n        gene_pool['pos_max'] = \"[[max(i,j)%10 for j in range(len(g[0]))]for i in range(len(g))]\"\n        \n        # Complex operations\n        gene_pool['fill_black'] = \"[[0 for x in r]for r in g]\"\n        gene_pool['fill_ones'] = \"[[1 for x in r]for r in g]\"\n        \n        # Conditional operations\n        for threshold in [1, 2, 5]:\n            gene_pool[f'threshold{threshold}'] = f\"[[1 if x>={threshold}else 0 for x in r]for r in g]\"\n            \n        return gene_pool\n    \n    def create_random_individual(self):\n        \"\"\"Create a random individual with multiple genes\"\"\"\n        n_genes = random.randint(1, 2)\n        genes = random.sample(list(self.gene_pool.values()), n_genes)\n        \n        if n_genes == 1:\n            return f\"def p(g):return {genes[0]}\"\n        else:\n            return f\"def p(g):g={genes[0]};return {genes[1]}\"\n            \n    def point_mutation(self, individual):\n        \"\"\"Point mutation - change a single gene\"\"\"\n        import re\n        \n        # Find all gene expressions\n        genes = re.findall(r'(g=|return )([^;]+)', individual)\n        if genes:\n            idx = random.randint(0, len(genes)-1)\n            old_gene = genes[idx][1]\n            new_gene = random.choice(list(self.gene_pool.values()))\n            individual = individual.replace(old_gene, new_gene, 1)\n            \n        return individual\n    \n    def swap_mutation(self, individual):\n        \"\"\"Swap two genes\"\"\"\n        # Simplified - just do point mutation\n        return self.point_mutation(individual)\n    \n    def creep_mutation(self, individual):\n        \"\"\"Small change to numeric values\"\"\"\n        import re\n        \n        numbers = re.findall(r'\\b\\d+\\b', individual)\n        if numbers:\n            old_num = random.choice(numbers)\n            new_num = str(max(0, min(9, int(old_num) + random.randint(-2, 2))))\n            individual = individual.replace(old_num, new_num, 1)\n            \n        return individual\n    \n    def boundary_mutation(self, individual):\n        \"\"\"Change values to boundary conditions\"\"\"\n        import re\n        \n        numbers = re.findall(r'\\b\\d+\\b', individual)\n        if numbers:\n            old_num = random.choice(numbers)\n            new_num = random.choice(['0', '9', '1'])\n            individual = individual.replace(old_num, new_num, 1)\n            \n        return individual\n    \n    def single_point_crossover(self, parent1, parent2):\n        \"\"\"Single point crossover\"\"\"\n        # Simplified implementation\n        if random.random() < 0.5:\n            return parent1, parent2\n        else:\n            return parent2, parent1\n    \n    def two_point_crossover(self, parent1, parent2):\n        \"\"\"Two point crossover\"\"\"\n        # Simplified implementation\n        return self.single_point_crossover(parent1, parent2)\n    \n    def evolve_population(self, population, fitness_scores):\n        \"\"\"Evolve population with multiple strategies\"\"\"\n        if not population or not fitness_scores:\n            return population\n            \n        # Sort by fitness\n        sorted_pop = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n        \n        # Elite selection\n        elite_size = max(1, int(self.population_size * self.elite_ratio))\n        new_population = [ind for ind, _ in sorted_pop[:elite_size]]\n        \n        # Fill rest of population\n        while len(new_population) < self.population_size:\n            # Tournament selection\n            tournament_size = min(7, len(sorted_pop))\n            tournament = random.sample(sorted_pop, tournament_size)\n            parent1 = max(tournament, key=lambda x: x[1])[0]\n            \n            tournament = random.sample(sorted_pop, tournament_size)\n            parent2 = max(tournament, key=lambda x: x[1])[0]\n            \n            # Crossover with random strategy\n            if random.random() < self.crossover_rate:\n                crossover_func = random.choice(self.crossover_strategies)\n                child1, child2 = crossover_func(parent1, parent2)\n            else:\n                child1, child2 = parent1, parent2\n                \n            # Mutation with random strategy\n            if random.random() < self.mutation_rate:\n                mutation_func = random.choice(self.mutation_strategies)\n                child1 = mutation_func(child1)\n                \n            if random.random() < self.mutation_rate:\n                mutation_func = random.choice(self.mutation_strategies)\n                child2 = mutation_func(child2)\n                \n            new_population.extend([child1, child2])\n            \n        self.generation += 1\n        return new_population[:self.population_size]\n\n\nclass ContinuousLearningOptimizer:\n    \"\"\"Continuous learning system that improves over time\"\"\"\n    \n    def __init__(self):\n        self.task_history = []\n        self.solution_history = []\n        self.feature_importance_history = []\n        self.pattern_success_rate = defaultdict(lambda: {'success': 0, 'total': 0})\n        self.online_models = {}\n        self.meta_features_cache = {}\n        \n    def record_task_result(self, task_id, features, solution, success):\n        \"\"\"Record task result for continuous learning\"\"\"\n        self.task_history.append({\n            'task_id': task_id,\n            'features': features,\n            'solution': solution,\n            'success': success,\n            'timestamp': len(self.task_history)\n        })\n        \n        # Update pattern success rates\n        pattern_key = self._extract_pattern_key(solution)\n        self.pattern_success_rate[pattern_key]['total'] += 1\n        if success:\n            self.pattern_success_rate[pattern_key]['success'] += 1\n            \n        # Update online models periodically\n        if len(self.task_history) % 20 == 0:\n            self._update_online_models()\n            \n    def _extract_pattern_key(self, solution):\n        \"\"\"Extract key pattern from solution\"\"\"\n        # Simple extraction\n        if 'rot90' in solution or 'zip(*g[::-1])' in solution:\n            return 'rotation'\n        elif 'r[::-1]' in solution and 'for r in g]' in solution:\n            return 'flip_horizontal'\n        elif 'g[::-1]' in solution:\n            return 'flip_vertical'\n        elif '//' in solution:\n            return 'scaling'\n        elif 'if' in solution:\n            return 'conditional'\n        else:\n            return 'other'\n            \n    def _update_online_models(self):\n        \"\"\"Update online learning models\"\"\"\n        if len(self.task_history) < 10:\n            return\n            \n        # Prepare training data\n        recent_tasks = self.task_history[-100:]\n        X = []\n        y = []\n        \n        for task in recent_tasks:\n            if task['features'] is not None and len(task['features']) > 0:\n                X.append(task['features'])\n                y.append(task['success'])\n                \n        if len(X) < 10 or len(set(y)) < 2:\n            return\n            \n        X = np.array(X)\n        y = np.array(y)\n        \n        # Update various online models\n        try:\n            if 'sgd' not in self.online_models:\n                self.online_models['sgd'] = SGDClassifier(loss='log', random_state=42)\n                self.online_models['nb'] = GaussianNB()\n                \n            # Partial fit for online learning\n            self.online_models['sgd'].partial_fit(X, y, classes=[0, 1])\n            self.online_models['nb'].partial_fit(X, y, classes=[0, 1])\n        except:\n            pass\n        \n    def get_recommendations(self, features):\n        \"\"\"Get recommendations based on learned patterns\"\"\"\n        recommendations = []\n        \n        # Pattern-based recommendations\n        sorted_patterns = sorted(\n            self.pattern_success_rate.items(),\n            key=lambda x: x[1]['success'] / (x[1]['total'] + 1e-10),\n            reverse=True\n        )\n        \n        for pattern, stats in sorted_patterns[:5]:\n            if stats['total'] > 5:\n                success_rate = stats['success'] / stats['total']\n                recommendations.append({\n                    'pattern': pattern,\n                    'confidence': success_rate,\n                    'support': stats['total']\n                })\n                \n        # Model-based recommendations\n        if self.online_models and features is not None and len(features) > 0:\n            for name, model in self.online_models.items():\n                try:\n                    pred = model.predict_proba([features])[0]\n                    recommendations.append({\n                        'model': name,\n                        'success_probability': pred[1] if len(pred) > 1 else 0.5\n                    })\n                except:\n                    pass\n                    \n        return recommendations\n\n\nclass UltimateCombinationalARCSolver:\n    \"\"\"Ultimate solver using all combinatorial techniques\"\"\"\n    \n    def __init__(self):\n        # Initialize all components\n        self.combinatorics = ExhaustiveCombinatorics()\n        self.mega_ensemble = UltimateMegaEnsemble()\n        self.genetic_evolver = AdvancedGeneticEvolver()\n        self.continuous_learner = ContinuousLearningOptimizer()\n        \n        # Caching for efficiency\n        self.feature_cache = {}\n        self.solution_cache = {}\n        \n        # Success tracking\n        self.task_results = {}\n        \n    def extract_comprehensive_features(self, examples):\n        \"\"\"Extract ALL possible features from examples\"\"\"\n        if not examples:\n            return np.zeros(1000)\n            \n        all_features = []\n        \n        for ex in examples[:3]:  # Limit examples\n            inp = np.array(ex['input'])\n            out = np.array(ex['output'])\n            \n            # Get cache key\n            cache_key = (inp.tobytes(), out.tobytes())\n            \n            if cache_key in self.feature_cache:\n                features = self.feature_cache[cache_key]\n            else:\n                try:\n                    # Extract all combinatorial features\n                    inp_cell_features = self.combinatorics.extract_all_cell_combinations(inp)\n                    out_cell_features = self.combinatorics.extract_all_cell_combinations(out)\n                    \n                    inp_raster_features = self.combinatorics.extract_all_raster_combinations(inp)\n                    out_raster_features = self.combinatorics.extract_all_raster_combinations(out)\n                    \n                    transformation_features = self.combinatorics.extract_transformation_relationships(inp, out)\n                    \n                    # Combine all features\n                    features = np.concatenate([\n                        inp_cell_features[:1000],\n                        out_cell_features[:1000],\n                        inp_raster_features[:500],\n                        out_raster_features[:500],\n                        transformation_features[:1000]\n                    ])\n                    \n                    self.feature_cache[cache_key] = features\n                except:\n                    features = np.zeros(4000)\n                    \n            all_features.append(features)\n            \n        if not all_features:\n            return np.zeros(4000)\n            \n        # Aggregate features across examples\n        all_features = np.array(all_features)\n        \n        # Statistical aggregation\n        try:\n            aggregated = np.concatenate([\n                all_features.mean(axis=0)[:1000],\n                all_features.std(axis=0)[:1000],\n                all_features.min(axis=0)[:1000],\n                all_features.max(axis=0)[:1000]\n            ])\n        except:\n            aggregated = np.zeros(4000)\n        \n        return aggregated[:4000]  # Limit total features\n        \n    def generate_all_candidates(self, examples):\n        \"\"\"Generate candidates using ALL methods\"\"\"\n        candidates = []\n        \n        # 1. Genetic evolution\n        population = [self.genetic_evolver.create_random_individual() \n                     for _ in range(50)]\n        \n        for gen in range(10):  # Reduced generations\n            fitness_scores = []\n            for individual in population:\n                score = self._evaluate_candidate(individual, examples)\n                fitness_scores.append(score)\n                \n            # Add best to candidates\n            if fitness_scores:\n                best_idx = np.argmax(fitness_scores)\n                if fitness_scores[best_idx] > 0.5:\n                    candidates.append(population[best_idx])\n                    \n                # Evolve\n                population = self.genetic_evolver.evolve_population(population, fitness_scores)\n                \n                # Early stopping\n                if max(fitness_scores) == 1.0:\n                    break\n                    \n        # 2. Pattern library with variations\n        base_patterns = [\n            \"def p(g):return g\",\n            \"def p(g):return[list(r)for r in zip(*g[::-1])]\",\n            \"def p(g):return[r[::-1]for r in g[::-1]]\",\n            \"def p(g):return[list(r)for r in zip(*g)][::-1]\",\n            \"def p(g):return[r[::-1]for r in g]\",\n            \"def p(g):return g[::-1]\",\n            \"def p(g):return[list(r)for r in zip(*g)]\",\n        ]\n        \n        candidates.extend(base_patterns)\n        \n        # Chain operations\n        for i, pattern1 in enumerate(base_patterns[:3]):\n            for j, pattern2 in enumerate(base_patterns[:3]):\n                if i != j:\n                    try:\n                        operation1 = pattern1.split('return ')[1]\n                        operation2 = pattern2.split('return ')[1]\n                        candidates.append(f\"def p(g):g={operation1};return {operation2}\")\n                    except:\n                        pass\n                    \n        # 3. Position-based patterns\n        candidates.extend([\n            \"def p(g):return[[(i+j)%10 for j in range(len(g[0]))]for i in range(len(g))]\",\n            \"def p(g):return[[(i*j)%10 for j in range(len(g[0]))]for i in range(len(g))]\",\n            \"def p(g):return[[max(i,j)%10 for j in range(len(g[0]))]for i in range(len(g))]\",\n            \"def p(g):return[[min(i,j)%10 for j in range(len(g[0]))]for i in range(len(g))]\",\n        ])\n            \n        # 4. Color-based patterns\n        for c in range(5):\n            candidates.append(f\"def p(g):return[[{c}]*len(g[0])for _ in g]\")\n            candidates.append(f\"def p(g):return[[x if x=={c}else 0 for x in r]for r in g]\")\n            \n        # 5. Scale patterns\n        for s in [2, 3]:\n            candidates.append(f\"def p(g):return[[g[i//{s}][j//{s}]for j in range(len(g[0])*{s})]for i in range(len(g)*{s})]\")\n            candidates.append(f\"def p(g):return[r[::{s}]for r in g[::{s}]]\")\n            \n        return list(set(candidates))  # Remove duplicates\n        \n    def _evaluate_candidate(self, candidate, examples):\n        \"\"\"Evaluate candidate on examples\"\"\"\n        try:\n            namespace = {}\n            exec(candidate, namespace)\n            p = namespace.get('p')\n            \n            if p is None:\n                return 0.0\n                \n            correct = 0\n            for ex in examples:\n                try:\n                    # Deep copy input\n                    input_copy = [row[:] for row in ex['input']]\n                    result = p(input_copy)\n                    if result == ex['output']:\n                        correct += 1\n                except:\n                    pass\n                    \n            return correct / len(examples) if examples else 0.0\n        except:\n            return 0.0\n            \n    def generate_solution(self, task_data):\n        \"\"\"Generate optimal solution using all techniques\"\"\"\n        examples = task_data.get('train', [])\n        \n        if not examples:\n            return \"def p(g):return g\"\n        \n        # Extract comprehensive features\n        features = self.extract_comprehensive_features(examples)\n        \n        # Generate all candidates\n        candidates = self.generate_all_candidates(examples)\n        \n        # Evaluate all candidates\n        candidate_scores = []\n        for candidate in candidates:\n            score = self._evaluate_candidate(candidate, examples)\n            candidate_scores.append((candidate, score))\n            \n        # Sort by score and length (prefer shorter solutions)\n        candidate_scores.sort(key=lambda x: (x[1], -len(x[0])), reverse=True)\n        \n        # Return best valid candidate\n        for candidate, score in candidate_scores:\n            if score >= 1.0:  # Perfect score\n                return candidate\n                \n        # If no perfect solution, return best partial match\n        if candidate_scores and candidate_scores[0][1] > 0:\n            return candidate_scores[0][0]\n                        \n        # Fallback\n        return \"def p(g):return g\"\n\n\ndef create_ultimate_combinatorial_submission():\n    \"\"\"Create submission using ultimate combinatorial approach\"\"\"\n    solver = UltimateCombinationalARCSolver()\n    solutions = {}\n    \n    print(\"üåü Generating Ultimate Combinatorial ARC Solutions...\")\n    print(\"üî¨ Using EXHAUSTIVE feature extraction and ALL model types\")\n    print(\"üß¨ Genetic evolution with advanced strategies\")\n    print(\"üìä Continuous learning and adaptation\")\n    print(\"=\" * 70)\n    \n    successful = 0\n    total_bytes = 0\n    \n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        task_file = f\"/kaggle/input/google-code-golf-2025/task{task_id}.json\"\n        \n        try:\n            with open(task_file) as f:\n                task_data = json.load(f)\n                \n            # Generate solution\n            code = solver.generate_solution(task_data)\n            solutions[task_id] = code\n            \n            # Verify solution\n            try:\n                namespace = {}\n                exec(code, namespace)\n                \n                valid = True\n                for ex in task_data['train'][:3]:\n                    p = namespace['p']\n                    result = p([row[:] for row in ex['input']])\n                    if result != ex['output']:\n                        valid = False\n                        break\n                        \n                if valid:\n                    successful += 1\n                    status = \"‚úÖ\"\n                else:\n                    status = \"‚ùå\"\n                    \n                # Record result for continuous learning\n                features = solver.extract_comprehensive_features(task_data['train'])\n                solver.continuous_learner.record_task_result(task_id, features, code, valid)\n                \n            except:\n                status = \"‚ö†Ô∏è\"\n                valid = False\n                \n            bytes_count = len(code)\n            total_bytes += bytes_count\n            \n            # Progress update\n            if task_num % 25 == 0:\n                print(f\"Progress: {task_num}/400 | Success: {successful}/{task_num} ({successful/task_num:.1%}) | \"\n                      f\"Avg bytes: {total_bytes/task_num:.1f}\")\n                \n        except Exception as e:\n            code = \"def p(g):return g\"\n            solutions[task_id] = code\n            total_bytes += len(code)\n            print(f\"Error on task {task_id}: {e}\")\n            \n    print(f\"\\n{'='*70}\")\n    print(f\"‚úÖ Completed: {successful}/400 valid solutions ({successful/400:.1%})\")\n    print(f\"üìä Total bytes: {total_bytes:,}\")\n    print(f\"üìà Average bytes per solution: {total_bytes/400:.1f}\")\n    \n    # Final statistics\n    print(f\"\\nüìä Final Statistics:\")\n    print(f\"  Total models in ensemble: {len(solver.mega_ensemble.models)}\")\n    print(f\"  Genetic algorithm generations: {solver.genetic_evolver.generation}\")\n    print(f\"  Feature cache entries: {len(solver.feature_cache)}\")\n    print(f\"  Continuous learning history: {len(solver.continuous_learner.task_history)} tasks\")\n    \n    # Pattern success rates\n    print(f\"\\nüéØ Pattern Success Rates:\")\n    for pattern, stats in sorted(\n        solver.continuous_learner.pattern_success_rate.items(),\n        key=lambda x: x[1]['success'] / (x[1]['total'] + 1e-10),\n        reverse=True\n    )[:10]:\n        if stats['total'] > 0:\n            success_rate = stats['success'] / stats['total']\n            print(f\"  {pattern}: {success_rate:.1%} ({stats['success']}/{stats['total']})\")\n            \n    # Create submission\n    os.makedirs(\"submission\", exist_ok=True)\n    \n    for task_id, code in solutions.items():\n        with open(f\"submission/task{task_id}.py\", \"w\") as f:\n            f.write(code)\n            \n    with zipfile.ZipFile(\"submission.zip\", \"w\") as zipf:\n        for task_id in solutions:\n            zipf.write(f\"submission/task{task_id}.py\", f\"task{task_id}.py\")\n            \n    print(f\"\\n‚ú® Ultimate Combinatorial submission created: submission.zip\")\n    \n    return solutions\n\nif __name__ == \"__main__\":\n    create_ultimate_combinatorial_submission()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}