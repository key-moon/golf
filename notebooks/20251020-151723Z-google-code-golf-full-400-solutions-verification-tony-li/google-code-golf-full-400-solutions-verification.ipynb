{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13472782,"sourceType":"competition"},{"sourceId":13444947,"sourceType":"datasetVersion","datasetId":8252851},{"sourceId":269036929,"sourceType":"kernelVersion"},{"sourceId":269352271,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"jupytext":{"formats":"ipynb,py:percent"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n!pip install zopfli","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Auto-detect environment: local vs Kaggle\nimport os\n\nif os.path.exists(\"./tasks_others\"):\n    # Local environment\n    print(\"Detected LOCAL environment - using local tasks folder\")\n    ENVIRONMENT = \"local\"\n    sources = [\n        \"./tasks_others/tasks\",\n        \"./tasks_others/tasks_public\",\n    ]\nelse:\n    # Kaggle environment - treat input paths as direct dataset sources\n    print(\"Detected KAGGLE environment - using dataset sources\")\n    ENVIRONMENT = \"kaggle\"\n    sources = [\n        \"/kaggle/input/google-code-golf-2025-submit\",\n        \"/kaggle/input/dead-code\",\n        \"/kaggle/input/4-way-task-178-layercake-why-70b-is-hard\"\n    ]\n\nprint(f\"Environment: {ENVIRONMENT}\")\nprint(f\"Sources: {sources}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:47:53.383427Z","iopub.execute_input":"2025-10-19T14:47:53.384256Z","iopub.status.idle":"2025-10-19T14:47:53.396781Z","shell.execute_reply.started":"2025-10-19T14:47:53.384205Z","shell.execute_reply":"2025-10-19T14:47:53.39567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remove failing solutions","metadata":{}},{"cell_type":"code","source":"failing = []\n\n# Tasks that take too long to verify - skip verification for these\nslow_verification_tasks = [157]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The zipper:","metadata":{}},{"cell_type":"code","source":"from zipfile import ZipFile\nimport zipfile\nimport zopfli.zlib\nimport zlib\n\ndef zip_src(src_code):\n    candidates=[src_code]\n    for compress in[zopfli.zlib.compress,lambda d:zlib.compress(d,9)]:\n        for trailing in[b'',b'\\n']:\n            src=src_code+trailing\n            while(comp:=compress(src))[-1]==ord('\"'):src+=b'#'\n            for delim in[b\"'\",b'\"']:\n                esc_map={0:b'\\\\x00',ord('\\n'):b'\\\\n',ord('\\r'):b'\\\\r',ord('\\\\'):b'\\\\\\\\',delim[0]:b'\\\\'+delim}\n                sanitized=b''.join(esc_map.get(b,bytes([b]))for b in comp)\n                compressed=b'import zlib\\nexec(zlib.decompress(bytes('+delim+sanitized+delim+b',\"L1\")))'\n                if max(sanitized)>127:compressed=b'#coding:L1\\n'+compressed\n                else:print('no header needed!')\n                candidates.append(compressed)\n            esc_map={0:b'\\\\x00',ord('\\r'):b'\\\\r',ord('\\\\'):b'\\\\\\\\'}\n            sanitized=b''.join(esc_map.get(b,bytes([b]))for b in comp)\n            compressed=b'import zlib\\nexec(zlib.decompress(bytes(\"\"\"'+sanitized+b'\"\"\",\"L1\")))'\n            if max(sanitized)>127:compressed=b'#coding:L1\\n'+compressed\n            else:print('no header needed!')\n            candidates.append(compressed)\n    valid_options=[]\n    for code in candidates:\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=SyntaxWarning)\n                with open('tmp.py','wb')as f:f.write(code)\n                with open('tmp.py','rb')as f:x=f.read()\n                exec(x,{})\n                valid_options.append(code)\n        except:0\n    return min(valid_options,key=len)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And get zipping","metadata":{}},{"cell_type":"code","source":"! pip install python-minifier\n\nimport os\nfrom python_minifier import minify\nimport warnings\nimport traceback\nimport zipfile\nimport shutil\nfrom collections import defaultdict\nimport json\nimport copy\nimport numpy as np\nimport re\n\nwarnings.filterwarnings(\"ignore\")\n\ndef is_already_compressed(source_code):\n    \"\"\"Check if source code is already zlib compressed\"\"\"\n    return \"exec(zlib.decompress\" in source_code\n\ndef load_task_examples(task_num):\n    \"\"\"Load examples for a task (simplified version of verify1.py logic)\"\"\"\n    # Try to find task data file\n    task_data_paths = [\n        f\"/kaggle/input/google-code-golf-2025/task{task_num:03d}.json\",\n        f\"../google-code-golf-2025/task{task_num:03d}.json\"\n    ]\n    \n    for path in task_data_paths:\n        if os.path.exists(path):\n            with open(path, 'r') as f:\n                return json.load(f)\n    \n    print(f\"Warning: No test data found for task {task_num}\")\n    return None\n\ndef verify_solution_code(solution_code, task_num, examples=None):\n    \"\"\"Verify if a solution works correctly\"\"\"\n    if examples is None:\n        examples = load_task_examples(task_num)\n        if examples is None:\n            return False, \"No test data available\"\n    \n    try:\n        # Execute the solution code\n        namespace = {}\n        exec(solution_code, namespace)\n        \n        if 'p' not in namespace:\n            return False, \"No function 'p' found\"\n        \n        solution_func = namespace['p']\n        if not callable(solution_func):\n            return False, \"Function 'p' is not callable\"\n        \n        # Test on all available test cases\n        test_cases = []\n        for category in ['test', 'train', 'arc-gen']:\n            if category in examples:\n                test_cases.extend(examples[category])\n        \n        if not test_cases:\n            return False, \"No test cases available\"\n        \n        correct_count = 0\n        total_count = len(test_cases)\n        \n        for i, test_case in enumerate(test_cases):\n            try:\n                result = solution_func(copy.deepcopy(test_case['input']))\n                \n                # Convert result to comparable format\n                result_json = json.dumps(result)\n                result_json = result_json.replace(\"true\", \"1\").replace(\"false\", \"0\")\n                \n                # Check for unsafe characters\n                unsafe_chars = re.compile(r\"[^0-9,\\[\\]\\s\\.]\")\n                if unsafe_chars.search(result_json):\n                    return False, f\"Invalid output format: {result_json[:100]}\"\n                \n                result_array = np.array(json.loads(result_json))\n                expected_array = np.array(test_case['output'])\n                \n                if np.array_equal(result_array, expected_array):\n                    correct_count += 1\n                else:\n                    return False, f\"Wrong output on test case {i+1}/{total_count}\"\n                    \n            except Exception as e:\n                return False, f\"Exception on test case {i+1}: {str(e)[:100]}\"\n        \n        return True, f\"Passed all {correct_count}/{total_count} test cases\"\n        \n    except Exception as e:\n        return False, f\"Execution error: {str(e)[:100]}\"\n\ndef safe_minify(source_code, file_path=\"unknown\"):\n    \"\"\"Error-proof minification that reports bytes saved (latin-1 only)\"\"\"\n    original_size = len(source_code.encode('latin-1'))\n    try:\n        minified = minify(source_code)\n        minified_size = len(minified.encode('latin-1'))\n        bytes_saved = original_size - minified_size\n        \n        # If minified result is bigger than original, use original instead\n        if minified_size > original_size:\n            return source_code, 0, None\n        \n        return minified, bytes_saved, None\n    except Exception as e:\n        # Return original code if minification fails\n        error_msg = f\"File: {file_path} | Error: {str(e)}\"\n        return source_code, 0, error_msg\n\nprint(f\"\\nUsing sources: {sources}\")\n\ntotal_save = 0\nminify_save = 0\nminify_errors = 0\nminify_error_details = []\nalready_compressed_count = 0  # Track how many scripts were already compressed\nscore = 0\n\n# Track verification statistics\ntotal_verification_attempted = 0\ntotal_verification_failed = 0\ntasks_with_verification_issues = 0\ndefault_verification_attempted = 0\ndefault_verification_failed = 0\nfailed_default_tasks = []  # Track which default source tasks failed verification\n\n# Track source usage and savings\nsource_stats = defaultdict(lambda: {'tasks_used': 0, 'bytes_saved': 0, 'original_bytes': 0, 'final_bytes': 0, 'already_compressed_tasks': 0})\ntask_source_map = {}  # Track which source was used for each task\nnon_default_tasks = {}  # Track tasks that used non-default sources\ndefault_source = sources[0] if sources else None  # First source is default\n\nif ENVIRONMENT == \"kaggle\":\n    submission = \"/kaggle/working/submission\"\nelse:\n    submission = \"./submission\"\n\nos.makedirs(submission, exist_ok=True)\n\nprint(f\"\\nProcessing 400 tasks...\")\n\nfor task_num in range(1, 401):\n    path_out = f\"{submission}/task{task_num:03d}.py\" \n\n    # Collect all candidates with their optimized versions\n    candidates = []\n    \n    # Check all sources for candidate solutions\n    for source in sources:\n        path_in = f\"{source}/task{task_num:03d}.py\"\n        if not os.path.exists(path_in): continue\n        if path_in in failing: continue\n\n        # Read all files as latin-1 (binary hack)\n        try:\n            with open(path_in, \"r\", encoding='latin-1') as task_in:\n                new_src = task_in.read()\n        except Exception as e:\n            print(f\"Error reading {path_in}: {e}\")\n            continue\n        \n        # Remove BOM if present to save bytes\n        if new_src.startswith('\\ufeff'):\n            new_src = new_src[1:]\n        elif new_src.startswith('\\xff\\xfe') or new_src.startswith('\\xfe\\xff'):\n            new_src = new_src[2:]\n        \n        original_size = len(new_src.encode('latin-1'))\n        \n        # Check if the script is already zlib compressed\n        if is_already_compressed(new_src):\n            already_compressed_count += 1\n            final_size = original_size  # Use actual file size, no optimization needed\n            bytes_saved = 0  # No minification savings since we skipped it\n            compression_improvement = 0\n        else:\n            # Try safe minification (latin-1 only)\n            minified_src, bytes_saved, error = safe_minify(new_src, path_in)\n            if error:\n                minify_errors += 1\n                minify_error_details.append(error)\n                print(f\"Minification failed for task {task_num}: {error}\")\n            else:\n                minify_save += bytes_saved\n            \n            # Get the best of raw vs minified (latin-1 encoding)\n            raw_size = len(new_src.encode('latin-1'))\n            minified_size = len(minified_src.encode('latin-1'))\n            \n            # Use the smaller version for compression\n            candidate_src = minified_src.encode('latin-1') if minified_size < raw_size else new_src.encode('latin-1')\n            \n            # Apply compression to get final size\n            compressed_src = zip_src(candidate_src)\n            final_size = len(compressed_src) if len(compressed_src) < len(candidate_src) else len(candidate_src)\n            \n            # Track compression savings for this candidate\n            compression_improvement = len(candidate_src) - len(compressed_src) if len(compressed_src) < len(candidate_src) else 0\n        \n        # Store candidate info\n        candidates.append({\n            'source': source,\n            'original_src': new_src,  # Keep for verification only\n            'original_size': original_size,\n            'final_size': final_size,  # Estimated optimized size\n            'compression_improvement': compression_improvement,\n            'was_already_compressed': is_already_compressed(new_src),\n            'path': path_in  # Keep original file path for binary copying\n        })\n    \n    # Sort candidates by final size (smallest first)\n    candidates.sort(key=lambda x: x['final_size'])\n    \n    # Find the default candidate (first source is default)\n    default_candidate = None\n    for candidate in candidates:\n        if candidate['source'] == default_source:\n            default_candidate = candidate\n            break\n    \n    if not default_candidate:\n        print(f\"Task {task_num}: No default solution found, using smallest available\")\n        # Fallback to smallest if no default found\n        default_candidate = candidates[0] if candidates else None\n    \n    if not default_candidate:\n        print(f\"Task {task_num}: No candidates found at all\")\n        continue\n    \n    # Find smaller candidates than default\n    smaller_candidates = [c for c in candidates if c['final_size'] < default_candidate['final_size']]\n    \n    # First, verify the default solution works\n    default_verification_attempted += 1\n    total_verification_attempted += 1\n    \n    # Load test examples once for this task\n    examples = load_task_examples(task_num)\n    \n    # Verify default solution\n    if task_num in slow_verification_tasks:\n        print(f\"Task {task_num}: ⏭ Skipping default verification (known to be slow) - assuming it works\")\n        default_is_working = True\n        default_message = \"Verification skipped (slow task)\"\n    else:\n        print(f\"Task {task_num}: Verifying default solution from {default_candidate['source']}\")\n        default_is_working, default_message = verify_solution_code(default_candidate['original_src'], task_num, examples)\n    \n    if not default_is_working:\n        # Default solution failed verification\n        default_verification_failed += 1\n        total_verification_failed += 1\n        failed_default_tasks.append({\n            'task_num': task_num,\n            'source': default_candidate['source'],\n            'error': default_message\n        })\n        print(f\"  ✗ Default solution failed: {default_message}\")\n        \n        # Remove default from candidates and find next best working solution\n        working_candidates = [c for c in candidates if c != default_candidate]\n        working_candidates.sort(key=lambda x: x['final_size'])  # Sort by size\n        \n        best_candidate = None\n        for i, candidate in enumerate(working_candidates):\n            total_verification_attempted += 1\n            print(f\"  Testing fallback candidate {i+1}/{len(working_candidates)}: {candidate['source']} ({candidate['final_size']} bytes)\")\n            \n            if task_num in slow_verification_tasks:\n                print(f\"    ⏭ Skipping verification (known to be slow) - assuming it works\")\n                is_working = True\n                message = \"Verification skipped (slow task)\"\n            else:\n                is_working, message = verify_solution_code(candidate['original_src'], task_num, examples)\n            \n            if is_working:\n                best_candidate = candidate\n                print(f\"    ✓ Working! Using fallback {candidate['source']} solution ({candidate['final_size']} bytes)\")\n                break\n            else:\n                total_verification_failed += 1\n                print(f\"    ✗ Failed: {message}\")\n        \n        if not best_candidate:\n            print(f\"  ⚠ No working solution found for task {task_num}! Using default anyway (may fail)\")\n            best_candidate = default_candidate\n    else:\n        # Default solution works\n        print(f\"  ✓ Default solution verified successfully\")\n        best_candidate = default_candidate\n    \n    # Initialize best solution variables\n    best_source = best_candidate['source']\n    best_original_size = best_candidate['original_size']\n    best_compression_savings = best_candidate['compression_improvement']\n    best_was_already_compressed = best_candidate['was_already_compressed']\n    verification_attempted = 0\n    verification_failed = 0\n    \n    # Only test smaller candidates if default solution worked and there are smaller alternatives\n    if default_is_working and smaller_candidates:\n        print(f\"Task {task_num}: Default size {default_candidate['final_size']}, testing {len(smaller_candidates)} smaller alternatives\")\n        \n        # Test smaller candidates in order (smallest first) until we find a working one\n        for i, candidate in enumerate(smaller_candidates):\n            verification_attempted += 1\n            total_verification_attempted += 1\n            \n            print(f\"  Testing candidate {i+1}/{len(smaller_candidates)}: {candidate['source']} ({candidate['final_size']} bytes)\")\n            \n            # Skip verification for slow tasks\n            if task_num in slow_verification_tasks:\n                print(f\"  ⏭ Skipping verification for task {task_num} (known to be slow) - assuming it works\")\n                is_working = True\n                message = \"Verification skipped (slow task)\"\n            else:\n                # Verify the solution works\n                is_working, message = verify_solution_code(candidate['original_src'], task_num, examples)\n            \n            if is_working:\n                # This smaller candidate works, use it instead of default\n                best_candidate = candidate\n                best_source = candidate['source']\n                best_original_size = candidate['original_size']\n                best_compression_savings = candidate['compression_improvement']\n                best_was_already_compressed = candidate['was_already_compressed']\n                \n                bytes_saved = default_candidate['final_size'] - candidate['final_size']\n                print(f\"  ✓ Working! Using {candidate['source']} solution ({candidate['final_size']} bytes, saved {bytes_saved} bytes)\")\n                break\n            else:\n                verification_failed += 1\n                total_verification_failed += 1\n                print(f\"  ✗ Failed: {message}\")\n        \n        # Track tasks that had verification issues\n        if verification_failed > 0:\n            tasks_with_verification_issues += 1\n        \n        # If no smaller working solution found, we already have default set above\n        if verification_attempted > 0 and best_candidate == default_candidate:\n            print(f\"  Using default solution ({default_candidate['final_size']} bytes) - no smaller working solution found\")\n    \n    # Add compression savings for the chosen source\n    if best_compression_savings > 0:\n        total_save += best_compression_savings\n    \n    # Track which source was used for this task\n    if best_source:\n        task_source_map[task_num] = best_source\n        source_stats[best_source]['tasks_used'] += 1\n        source_stats[best_source]['original_bytes'] += best_original_size\n        \n        # Track final size and total bytes saved for the source that was used\n        source_stats[best_source]['final_bytes'] += best_candidate['final_size']\n        source_stats[best_source]['bytes_saved'] += best_original_size - best_candidate['final_size']\n        \n        # Track if this was an already-compressed script\n        if best_was_already_compressed:\n            source_stats[best_source]['already_compressed_tasks'] += 1\n        \n        # Track if non-default source was used\n        if best_source != default_source:\n            # Process default source through all optimization stages for comparison\n            default_file_path = f\"{default_source}/task{task_num:03d}.py\"\n            task_file_path = f\"{best_source}/task{task_num:03d}.py\"\n            \n            if os.path.exists(task_file_path):\n                try:\n                    # Read as latin-1 (binary hack)\n                    with open(task_file_path, \"r\", encoding='latin-1') as f:\n                        content = f.read()\n                    \n                    # Remove BOM if present to save bytes\n                    if content.startswith('\\ufeff'):\n                        content = content[1:]\n                    elif content.startswith('\\xff\\xfe') or content.startswith('\\xfe\\xff'):\n                        content = content[2:]\n                        \n                except Exception as e:\n                    print(f\"Error reading chosen source for task {task_num}: {e}\")\n                    continue\n                \n                # Process default source through all stages if it exists\n                default_stages = {\n                    'raw_size': 0,\n                    'minified_size': 0,\n                    'compressed_size': 0,\n                    'minify_error': None\n                }\n                \n                if os.path.exists(default_file_path):\n                    try:\n                        # Read as latin-1 (binary hack)\n                        with open(default_file_path, \"r\", encoding='latin-1') as f:\n                            default_content = f.read()\n                        \n                        # Remove BOM if present to save bytes\n                        if default_content.startswith('\\ufeff'):\n                            default_content = default_content[1:]\n                        elif default_content.startswith('\\xff\\xfe') or default_content.startswith('\\xfe\\xff'):\n                            default_content = default_content[2:]\n                            \n                    except Exception as e:\n                        print(f\"Error reading default source for task {task_num}: {e}\")\n                        continue\n                    \n                    default_stages['raw_size'] = len(default_content.encode('latin-1'))\n                    \n                    # Check if default source is already compressed\n                    if is_already_compressed(default_content):\n                        # Already compressed - skip optimization stages\n                        default_stages['minified_size'] = default_stages['raw_size']  # No minification applied\n                        default_stages['compressed_size'] = default_stages['raw_size']  # Already compressed\n                        default_minified = default_content  # Keep original\n                    else:\n                        # Try minifying default source\n                        default_minified, _, default_error = safe_minify(default_content, default_file_path)\n                        if default_error:\n                            default_stages['minify_error'] = default_error\n                            default_stages['minified_size'] = default_stages['raw_size']  # Use raw size if minify fails\n                        else:\n                            default_stages['minified_size'] = len(default_minified.encode('latin-1'))\n                        \n                        # Try compressing default source (use best of minified vs raw)\n                        default_best_src = min([default_minified.encode('latin-1'), default_content.encode('latin-1')], key=len)\n                        default_compressed = zip_src(default_best_src)\n                        default_compressed_improvement = len(default_best_src) - len(default_compressed)\n                        \n                        if default_compressed_improvement > 0:\n                            default_stages['compressed_size'] = len(default_compressed)\n                        else:\n                            default_stages['compressed_size'] = len(default_best_src)\n                \n                # Current source stages - calculate all optimization steps properly\n                current_stages = {\n                    'raw_size': len(content.encode('latin-1')),\n                    'minified_size': 0,\n                    'compressed_size': 0,\n                    'minify_error': None\n                }\n                \n                # Check if current source is already compressed\n                if is_already_compressed(content):\n                    # Already compressed - skip optimization stages\n                    current_stages['minified_size'] = current_stages['raw_size']  # No minification applied\n                    current_stages['compressed_size'] = current_stages['raw_size']  # Already compressed\n                    current_minified = content  # Keep original\n                else:\n                    # We already processed this through minification above, but let's get the exact sizes\n                    current_minified, _, current_error = safe_minify(content, task_file_path)\n                    if current_error:\n                        current_stages['minify_error'] = current_error\n                        current_stages['minified_size'] = current_stages['raw_size']\n                    else:\n                        current_stages['minified_size'] = len(current_minified.encode('latin-1'))\n                    \n                    # Calculate compressed size for current source\n                    current_best_src = min([current_minified.encode('latin-1'), content.encode('latin-1')], key=len)\n                    current_compressed = zip_src(current_best_src)\n                    current_compressed_improvement = len(current_best_src) - len(current_compressed)\n                    \n                    if current_compressed_improvement > 0:\n                        current_stages['compressed_size'] = len(current_compressed)\n                    else:\n                        current_stages['compressed_size'] = len(current_best_src)\n                \n                non_default_tasks[task_num] = {\n                    'source': best_source,\n                    'content': content,\n                    'file_path': task_file_path,\n                    'default_stages': default_stages,\n                    'current_stages': current_stages,\n                    'final_bytes_saved': default_stages['compressed_size'] - current_stages['compressed_size']\n                }\n    \n    # Compression was already applied during source selection\n    # Source statistics tracking is now handled in the source selection loop above\n    \n    score += max(1, 2500-best_candidate['final_size'])\n    \n    # Write the final optimized content (or original if compressed)\n    if best_candidate['was_already_compressed']:\n        # Copy compressed file as-is (binary)\n        with open(best_candidate['path'], \"rb\") as src_file:\n            with open(path_out, \"wb\") as task_out:\n                task_out.write(src_file.read())\n    else:\n        # Write optimized content for non-compressed files\n        # This requires re-doing the optimization to get the actual optimized bytes\n        src_content = best_candidate['original_src']\n        \n        # Re-do minification\n        minified_src, _, error = safe_minify(src_content, best_candidate['path'])\n        if not error and len(minified_src.encode('latin-1')) < len(src_content.encode('latin-1')):\n            best_text = minified_src\n        else:\n            best_text = src_content\n        \n        # Try compression\n        candidate_bytes = best_text.encode('latin-1')\n        compressed_bytes = zip_src(candidate_bytes)\n        \n        # Use the smaller version\n        if len(compressed_bytes) < len(candidate_bytes):\n            final_content = compressed_bytes\n        else:\n            final_content = candidate_bytes\n        \n        with open(path_out, \"wb\") as task_out:\n            task_out.write(final_content)\n\nprint(f\"\\nResults:\")\nprint(f\"Scripts already compressed (skipped optimization): {already_compressed_count}\")\nprint(f\"Minification saved: {minify_save}b across all tasks\")\nprint(f\"Minification errors: {minify_errors}\")\n\n# Show detailed minification error information\nif minify_error_details:\n    print(f\"\\nDetailed Minification Errors:\")\n    for i, error_detail in enumerate(minify_error_details, 1):\n        print(f\"  {i}. {error_detail}\")\n\nprint(f\"Zlib compression saved: {total_save}b\")\nprint(f\"Projected score: {score}\")\n\n# Report default source verification results\nprint(f\"\\nDefault Source Verification Summary:\")\nprint(f\"Default source: {default_source}\")\nprint(f\"Default verifications attempted: {default_verification_attempted}\")\nprint(f\"Default verifications failed: {default_verification_failed}\")\nif default_verification_attempted > 0:\n    default_success_rate = ((default_verification_attempted - default_verification_failed) / default_verification_attempted) * 100\n    print(f\"Default verification success rate: {default_success_rate:.1f}%\")\n\nif failed_default_tasks:\n    print(f\"\\nFailed Default Source Tasks ({len(failed_default_tasks)} tasks):\")\n    for failed_task in failed_default_tasks:\n        print(f\"  Task {failed_task['task_num']:03d}: {failed_task['source']}\")\n        print(f\"    Error: {failed_task['error'][:100]}...\")  # Truncate long error messages\n\n# Report non-default source usage\nprint(f\"\\nNon-Default Source Usage:\")\nprint(f\"Tasks using non-default sources: {len(non_default_tasks)}\")\n\n# Clarify what sizes are being measured\nprint(f\"\\nSize Measurement Clarification:\")\nprint(f\"  - Non-default sources were chosen because their FINAL OPTIMIZED size was smaller\")\nprint(f\"  - CSV shows all optimization stages: raw → minified → compressed\")\nprint(f\"  - Empty columns mean that optimization stage wasn't beneficial\")\nprint(f\"  - 'final_bytes_saved' = default_final_size - chosen_final_size\")\nprint(f\"  - 'Total bytes saved' shows additional optimization savings (0 for already-compressed scripts)\")\nprint(f\"  - Already-compressed scripts contribute by providing better solutions, not additional savings\")\n\nif non_default_tasks:\n    print(f\"\\nNon-default source details:\")\n    for task_num, task_info in sorted(non_default_tasks.items()):\n        print(f\"  Task {task_num}: {task_info['source']}\")\n\n# Show source usage statistics\nprint(f\"\\nSource Usage Statistics:\")\nfor source, stats in source_stats.items():\n    if stats['tasks_used'] > 0:\n        avg_original = stats['original_bytes'] / stats['tasks_used']\n        avg_final = stats['final_bytes'] / stats['tasks_used']\n        avg_savings = stats['bytes_saved'] / stats['tasks_used']\n        is_default = \" (DEFAULT)\" if source == default_source else \"\"\n        \n        # Get list of tasks that used this source\n        tasks_from_source = [task_num for task_num, task_source in task_source_map.items() if task_source == source]\n        tasks_from_source.sort()\n        \n        print(f\"Source: {source}{is_default}\")\n        print(f\"  Tasks used: {stats['tasks_used']}\")\n        print(f\"  Task IDs: {tasks_from_source}\")\n        if stats['already_compressed_tasks'] > 0:\n            print(f\"  Already compressed tasks: {stats['already_compressed_tasks']}\")\n        print(f\"  Total bytes saved: {stats['bytes_saved']:,}\")\n        print(f\"  Average original size: {avg_original:.1f} bytes\")\n        print(f\"  Average final size: {avg_final:.1f} bytes\")\n        print(f\"  Average savings per task: {avg_savings:.1f} bytes\")\n        print()","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Additional analysis and final zip creation","metadata":{}},{"cell_type":"code","source":"def analyze_compression_effectiveness():\n    \"\"\"Analyze which tasks benefit most from compression\"\"\"\n    compression_stats = []\n    \n    for task_num in range(1, 401):\n        task_path = f\"{submission}/task{task_num:03d}.py\"\n        if os.path.exists(task_path):\n            with open(task_path, \"rb\") as f:\n                content = f.read()\n                \n            # Check if it's a compressed solution\n            if content.startswith(b\"#coding:L1\"):\n                compression_stats.append({\n                    'task': task_num,\n                    'size': len(content),\n                    'compressed': True\n                })\n            else:\n                compression_stats.append({\n                    'task': task_num,\n                    'size': len(content),\n                    'compressed': False\n                })\n    \n    compressed_count = sum(1 for s in compression_stats if s['compressed'])\n    total_tasks = len(compression_stats)\n    \n    print(f\"\\nCompression Analysis:\")\n    print(f\"Tasks using zlib compression: {compressed_count}/{total_tasks}\")\n    print(f\"Average size of compressed solutions: {sum(s['size'] for s in compression_stats if s['compressed']) / max(1, compressed_count):.1f} bytes\")\n    print(f\"Average size of uncompressed solutions: {sum(s['size'] for s in compression_stats if not s['compressed']) / max(1, total_tasks - compressed_count):.1f} bytes\")\n    \n    return compression_stats\n\ndef create_others_zip():\n    \"\"\"Create others.zip containing tasks that used non-default sources and CSV metadata\"\"\"\n    if not non_default_tasks:\n        print(\"No non-default sources used - skipping others.zip creation\")\n        return None\n    \n    if ENVIRONMENT == \"kaggle\":\n        others_zip_path = \"/kaggle/working/others.zip\"\n        csv_path = \"/kaggle/working/others_metadata.csv\"\n    else:\n        others_zip_path = \"./others.zip\"\n        csv_path = \"./others_metadata.csv\"\n    \n    # Create CSV file with all optimization stages\n    import csv\n    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = [\n            'task_number', 'source',\n            'default_raw_size', 'default_minified_size', 'default_compressed_size', 'default_minify_error',\n            'chosen_raw_size', 'chosen_minified_size', 'chosen_compressed_size', 'chosen_minify_error',\n            'final_bytes_saved'\n        ]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for task_num, task_info in sorted(non_default_tasks.items()):\n            default = task_info['default_stages']\n            current = task_info['current_stages']\n            \n            writer.writerow({\n                'task_number': f\"task{task_num:03d}\",\n                'source': task_info['source'],\n                'default_raw_size': default['raw_size'] if default['raw_size'] > 0 else '',\n                'default_minified_size': default['minified_size'] if default['minified_size'] > 0 else '',\n                'default_compressed_size': default['compressed_size'] if default['compressed_size'] > 0 else '',\n                'default_minify_error': default['minify_error'] if default['minify_error'] else '',\n                'chosen_raw_size': current['raw_size'],\n                'chosen_minified_size': current['minified_size'] if current['minified_size'] > 0 else '',\n                'chosen_compressed_size': current['compressed_size'],\n                'chosen_minify_error': current['minify_error'] if current['minify_error'] else '',\n                'final_bytes_saved': task_info['final_bytes_saved']\n            })\n    \n    # Create zip file with just the original source files\n    with zipfile.ZipFile(others_zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n        for task_num, task_info in sorted(non_default_tasks.items()):\n            # Add only the original source file (no metadata txt files)\n            task_filename = f\"task{task_num:03d}.py\"\n            zipf.writestr(task_filename, task_info['content'])\n    \n    zip_size = os.path.getsize(others_zip_path)\n    csv_size = os.path.getsize(csv_path)\n    \n    print(f\"\\nCreated others.zip and metadata:\")\n    print(f\"  Others.zip path: {others_zip_path}\")\n    print(f\"  Others.zip size: {zip_size:,} bytes ({zip_size/1024:.1f} KB)\")\n    print(f\"  CSV metadata path: {csv_path}\")\n    print(f\"  CSV size: {csv_size:,} bytes\")\n    print(f\"  Tasks from non-default sources: {len(non_default_tasks)}\")\n    \n    # Show some statistics from the CSV data\n    total_bytes_saved = sum(task_info['final_bytes_saved'] for task_info in non_default_tasks.values())\n    avg_bytes_saved = total_bytes_saved / len(non_default_tasks) if non_default_tasks else 0\n    print(f\"  Total final bytes saved by using non-default sources: {total_bytes_saved:,}\")\n    print(f\"  Average final bytes saved per task: {avg_bytes_saved:.1f}\")\n    \n    return others_zip_path\n\ndef create_final_submission():\n    \"\"\"Create final submission with detailed reporting\"\"\"\n    submission_zip = f\"{submission}.zip\"\n    \n    with zipfile.ZipFile(submission_zip, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n        task_count = 0\n        total_size = 0\n        \n        for task_num in range(1, 401):\n            task_id = f\"{task_num:03d}\"\n            src_path = f\"{submission}/task{task_id}.py\"\n            \n            if os.path.exists(src_path):\n                file_size = os.path.getsize(src_path)\n                zipf.write(src_path, arcname=f\"task{task_id}.py\")\n                task_count += 1\n                total_size += file_size\n    \n    zip_size = os.path.getsize(submission_zip)\n    \n    print(f\"\\nFinal Submission:\")\n    print(f\"Created submission zip with {task_count} tasks\")\n    print(f\"Total uncompressed size: {total_size:,} bytes\")\n    print(f\"Submission zip size: {zip_size:,} bytes ({zip_size/1024:.1f} KB)\")\n    print(f\"Zip compression ratio: {zip_size/total_size:.3f}\")\n    \n    return submission_zip\n\n# Analyze compression effectiveness\ncompression_stats = analyze_compression_effectiveness()\n\n# Create others.zip for non-default source tasks\nothers_zip = create_others_zip()\n\n# Create final submission\nfinal_zip = create_final_submission()","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Summary Report","metadata":{}},{"cell_type":"code","source":"def generate_summary_report():\n    \"\"\"Generate comprehensive summary of the optimization process\"\"\"\n    print(\"=\"*60)\n    print(\"=\"*60)\n    \n    print(f\"Environment: {ENVIRONMENT.upper()}\")\n    print(f\"Sources: {len(sources)} datasets/directories\")\n    print(f\"Total Tasks: 400\")\n    \n    print(f\"\\nOptimization Results:\")\n    print(f\"Scripts already compressed (skipped): {already_compressed_count}\")\n    print(f\"Minification saved: {minify_save:,} bytes\")\n    print(f\"Minification errors: {minify_errors}\")\n    if minify_error_details:\n        print(f\"Error details: {len(minify_error_details)} files had minification issues\")\n    print(f\"Zlib compression saved: {total_save:,} bytes\")\n    print(f\"Total optimization: {minify_save + total_save:,} bytes\")\n    \n    print(f\"\\nSolution Verification Results:\")\n    print(f\"Total verifications attempted: {total_verification_attempted:,}\")\n    print(f\"Verifications failed: {total_verification_failed:,}\")\n    print(f\"Tasks with verification issues: {tasks_with_verification_issues}\")\n    if total_verification_attempted > 0:\n        success_rate = ((total_verification_attempted - total_verification_failed) / total_verification_attempted) * 100\n        print(f\"Verification success rate: {success_rate:.1f}%\")\n    \n    print(f\"\\nDefault Source Verification:\")\n    print(f\"Default verifications attempted: {default_verification_attempted:,}\")\n    print(f\"Default verifications failed: {default_verification_failed:,}\")\n    if default_verification_attempted > 0:\n        default_success_rate = ((default_verification_attempted - default_verification_failed) / default_verification_attempted) * 100\n        print(f\"Default verification success rate: {default_success_rate:.1f}%\")\n    \n    if failed_default_tasks:\n        print(f\"\\nFailed Default Source Tasks ({len(failed_default_tasks)} tasks):\")\n        for failed_task in failed_default_tasks:\n            print(f\"  Task {failed_task['task_num']:03d}: {failed_task['source']}\")\n            print(f\"    Error: {failed_task['error']}\")\n    else:\n        print(f\"\\nAll default source tasks passed verification! ✓\")\n    \n    print(f\"Projected score: {score}\")\n    \n    print(f\"\\nSource Performance Summary:\")\n    print(f\"Default Source: {default_source}\")\n    for i, source in enumerate(sources):\n        stats = source_stats.get(source, {'tasks_used': 0, 'bytes_saved': 0})\n        is_default = \" (DEFAULT)\" if source == default_source else \"\"\n        print(f\"  {i}: {source}{is_default}\")\n        print(f\"     Tasks contributed: {stats['tasks_used']}\")\n        print(f\"     Total bytes saved: {stats['bytes_saved']:,}\")\n    \n    # Show which source was most effective\n    if source_stats:\n        best_source = max(source_stats.keys(), key=lambda x: source_stats[x]['tasks_used'])\n        best_stats = source_stats[best_source]\n        print(f\"\\nMost Used Source: {best_source}\")\n        print(f\"  Contributed to {best_stats['tasks_used']} tasks\")\n        print(f\"  Total savings: {best_stats['bytes_saved']:,} bytes\")\n    \n    # Report non-default source usage\n    print(f\"\\nNon-Default Source Usage:\")\n    print(f\"  Tasks using non-default sources: {len(non_default_tasks)}\")\n    if non_default_tasks:\n        print(f\"  Others.zip created with {len(non_default_tasks)} tasks\")\n        if others_zip:\n            print(f\"  Others.zip path: {others_zip}\")\n    else:\n        print(f\"  All tasks used the default source\")\n    \n    print(\"=\"*60)\n\ngenerate_summary_report()\nprint(f\"Projected score: {score}\")","metadata":{},"outputs":[],"execution_count":null}]}