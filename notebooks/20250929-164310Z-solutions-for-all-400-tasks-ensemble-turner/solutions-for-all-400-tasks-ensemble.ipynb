{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95282,"databundleVersionId":13472782,"sourceType":"competition"},{"sourceId":13099530,"sourceType":"datasetVersion","datasetId":8295919},{"sourceId":13118341,"sourceType":"datasetVersion","datasetId":8295089},{"sourceId":13133271,"sourceType":"datasetVersion","datasetId":8304144},{"sourceId":258687029,"sourceType":"kernelVersion"},{"sourceId":260051873,"sourceType":"kernelVersion"},{"sourceId":261721366,"sourceType":"kernelVersion"},{"sourceId":262666916,"sourceType":"kernelVersion"},{"sourceId":262819064,"sourceType":"kernelVersion"},{"sourceId":263058147,"sourceType":"kernelVersion"},{"sourceId":263062960,"sourceType":"kernelVersion"},{"sourceId":263225016,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Cristiano Calcagno](https://www.kaggle.com/cristianocalcagno) has the solutions for all 400 tasks here: https://www.kaggle.com/datasets/cristianocalcagno/arc-code-golf-starter-solutions\n\nThis notebook combines the top notebooks with his work.\n\nReferences:\n\n* https://www.kaggle.com/code/tonylica/road-to-400-collaboration\n* https://www.kaggle.com/code/cheeseexports/big-zippa\n* https://www.kaggle.com/code/vladislavlassa/python-minifier-applied\n* https://www.kaggle.com/code/taylorsamarel/top-scores-remix-with-visualizations-and-insight\n* https://www.kaggle.com/code/jazivxt/oh-barnacles\n* https://www.kaggle.com/code/seshurajup/code-golf-ensemble-local-score-391-400-dsl\n* https://www.kaggle.com/code/mbilichenko/road-to-400-collaboration\n* https://www.kaggle.com/code/mcwema/neuroips-4-of-some-lessons-learned","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/google-code-golf-2025/code_golf_utils\")\nfrom code_golf_utils import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T06:23:49.584089Z","iopub.execute_input":"2025-09-22T06:23:49.584641Z","iopub.status.idle":"2025-09-22T06:23:49.602885Z","shell.execute_reply.started":"2025-09-22T06:23:49.584598Z","shell.execute_reply":"2025-09-22T06:23:49.601915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nimport importlib.util\nimport json\nimport re\nimport sys\nimport traceback\nimport numpy as np\nimport os\nimport shutil\nfrom tqdm import tqdm \n\ndef simple_verify_program(task_num, examples):\n    task_name, task_path = \"task_with_imports\", \"/kaggle/working/task.py\"\n    spec = importlib.util.spec_from_file_location(task_name, task_path)\n    if spec is None:\n        print(\"Error: Unable to import task.py.\")\n        return\n    module = sys.modules[task_name] = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    if not hasattr(module, \"p\"):\n        print(\"Error: Unable to locate function p() in task.py.\")\n        return\n    program = getattr(module, \"p\")\n    if not callable(program):\n        print(\"Error: Function p() in task.py is not callable.\")\n        return\n\n    def verify(example_subset):\n        right, wrong, expected, error = 0, 0, None, \"\"\n        for example in example_subset:\n            example_copy = copy.deepcopy(example)\n            try:\n                result = program(example_copy[\"input\"])\n                result = json.dumps(result)\n                result = result.replace(\"true\", \"1\").replace(\"false\", \"0\")\n                unsafe_chars = re.compile(r\"[^0-9,\\[\\]\\s\\.]\")\n                if unsafe_chars.search(result):\n                    raise ValueError(f\"Invalid output from user code: {result[:500]}\")\n                result = json.loads(result)\n                user_output = np.array(result)\n                label_output = np.array(example_copy[\"output\"])\n                if numpy.array_equal(user_output, label_output):\n                    right += 1\n                else:\n                    expected = copy.deepcopy(example)\n                    wrong += 1\n            except:\n                error = traceback.format_exc()\n                wrong += 1\n                #if error: print(f\"Error: {error}\")\n        return right, wrong, expected\n    \n    arc_agi_right, arc_agi_wrong, arc_agi_expected = verify(examples[\"train\"] + examples[\"test\"])\n    arc_gen_right, arc_gen_wrong, arc_gen_expected = verify(examples[\"arc-gen\"])\n    #print(f\"Results on ARC-AGI examples: {arc_agi_right} pass, {arc_agi_wrong} fail\")\n    #print(f\"Results on ARC-GEN examples: {arc_gen_right} pass, {arc_gen_wrong} fail\")\n    return arc_agi_right, arc_agi_wrong, arc_gen_right, arc_gen_wrong","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T06:23:49.605081Z","iopub.execute_input":"2025-09-22T06:23:49.606092Z","iopub.status.idle":"2025-09-22T06:23:49.623724Z","shell.execute_reply.started":"2025-09-22T06:23:49.606059Z","shell.execute_reply":"2025-09-22T06:23:49.622696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract solutions","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nfrom io import BytesIO\nimport warnings\nfrom itertools import combinations, permutations\n\n# Ignore SyntaxWarning globally\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n# Paths containing submission.zip\npaths = [\"/kaggle/input/python-minifier-applied\",\n         \"/kaggle/input/road-to-400-collaboration-39-unsolved-tasks\", \n         \"/kaggle/input/road-to-400-collaboration\", \n         \"/kaggle/input/code-golf-ensemble-local-score-391-400-dsl\",\n         \"/kaggle/input/oh-barnacles\",\n         \"/kaggle/input/big-zippa\",\n         \"/kaggle/input/top-scores-remix-with-visualizations-and-insight\",\n         \"/kaggle/input/neuroips-4-of-some-lessons-learned\"\n        ]\n\nfor idx, path in enumerate(paths):\n    dest_folder = f\"/kaggle/working/submission_{idx}\"\n    os.makedirs(dest_folder, exist_ok=True)\n    \n    zip_path = os.path.join(path, \"submission.zip\")\n    if not os.path.exists(zip_path):\n        print(f\"Zip not found: {zip_path}\")\n        continue\n\n    with zipfile.ZipFile(zip_path, \"r\") as z:\n        z.extractall(dest_folder)\n\n    print(f\"Extracted all files from {zip_path} â†’ {dest_folder}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T06:23:49.624753Z","iopub.execute_input":"2025-09-22T06:23:49.62512Z","iopub.status.idle":"2025-09-22T06:23:50.196351Z","shell.execute_reply.started":"2025-09-22T06:23:49.625083Z","shell.execute_reply":"2025-09-22T06:23:50.195247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src = \"/kaggle/input/gcgc-data-solutions\"\ndst = f\"/kaggle/working/submission_{len(paths)}\"\nos.makedirs(dst, exist_ok=True)\nfor f in os.listdir(src):\n    if f.endswith(\".py\"):\n        shutil.copy(os.path.join(src, f), dst)\npaths.append(src)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T06:23:50.197439Z","iopub.execute_input":"2025-09-22T06:23:50.197703Z","iopub.status.idle":"2025-09-22T06:23:50.212742Z","shell.execute_reply.started":"2025-09-22T06:23:50.197681Z","shell.execute_reply":"2025-09-22T06:23:50.211528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src = \"/kaggle/input/arc-code-golf-starter-solutions\"\ndst = f\"/kaggle/working/submission_{len(paths)}\"\nos.makedirs(dst, exist_ok=True)\nfor f in os.listdir(src):\n    if f.endswith(\".py\"):\n        shutil.copy(os.path.join(src, f), dst)\npaths.append(src)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T06:23:50.215341Z","iopub.execute_input":"2025-09-22T06:23:50.215632Z","iopub.status.idle":"2025-09-22T06:23:50.834349Z","shell.execute_reply.started":"2025-09-22T06:23:50.215609Z","shell.execute_reply":"2025-09-22T06:23:50.83292Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Search the smallest-size solution","metadata":{}},{"cell_type":"code","source":"import os, json, hashlib, shutil\nfrom tqdm import tqdm\n\n# ====== Fixed paths (no flags, no input writes) ======\nINPUT_CACHE_PATH = \"/kaggle/input/verify-cache/verify_cache.json\"\nWORKING_CACHE_PATH = \"/kaggle/working/verify_cache.json\"\n\n# Derive cache dataset info for clear messaging at the end\nINPUT_DATASET_DIR = os.path.dirname(INPUT_CACHE_PATH)               # e.g., /kaggle/input/verify-cache\nINPUT_DATASET_NAME = os.path.basename(INPUT_DATASET_DIR)            # e.g., verify-cache\n\ndef _log(msg: str):\n    try:\n        tqdm.write(str(msg))\n    except Exception:\n        print(str(msg))\n\ndef _read_json(path: str):\n    try:\n        if os.path.exists(path):\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                return json.load(f)\n    except Exception as e:\n        _log(f\"[CACHE LOAD WARNING] Could not read {path}: {e}\")\n    return {}\n\ndef _save_json_atomic(path: str, data: dict):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    tmp = path + \".tmp\"\n    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False)\n    os.replace(tmp, path)\n\ndef _load_cache_union() -> dict:\n    \"\"\"\n    Load cache from INPUT and WORKING; return their union.\n    - WORKING entries override INPUT for the same key.\n    - We never delete keys; later we only add new ones (monotonic growth).\n    \"\"\"\n    base = _read_json(INPUT_CACHE_PATH)\n    work = _read_json(WORKING_CACHE_PATH)\n    if work:\n        base.update(work)\n    return base\n\ndef _hash_bytes(b: bytes) -> str:\n    h = hashlib.sha256()\n    h.update(b)\n    return h.hexdigest()\n\ndef _hash_file(path: str) -> str:\n    h = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n            h.update(chunk)\n    return h.hexdigest()\n\ndef _examples_signature(examples) -> str:\n    \"\"\"\n    Make a stable-ish signature of the examples used by verification.\n    Falls back to str(...) if not JSON-serializable.\n    \"\"\"\n    try:\n        data = json.dumps(examples, sort_keys=True, ensure_ascii=False)\n        return _hash_bytes(data.encode(\"utf-8\"))\n    except Exception:\n        return _hash_bytes(str(examples).encode(\"utf-8\"))\n\ndef _dataset_slug_from_input_path(p: str) -> str:\n    \"\"\"\n    Return the top-level dataset folder under /kaggle/input from a path.\n    Robust to extra subfolders or files.\n    \"\"\"\n    parts = os.path.normpath(p).split(os.sep)\n    try:\n        i = parts.index(\"input\")\n        if i + 1 < len(parts):\n            return parts[i + 1]\n    except ValueError:\n        pass\n    return os.path.basename(os.path.normpath(p))\n\n# ====== Main loop with mandatory miss reporting ======\nsubmission = \"/kaggle/working/submission\"\nos.makedirs(submission, exist_ok=True)\n\ncache = _load_cache_union()\ninitial_cache_size = len(cache)\n\ncache_hits = 0\ncache_misses = 0\nhad_miss = False  # tracks if *any* miss occurred this run\n\nlb_score = 0\nunsolved_tasks = []\nnum_tasks = 400\n\nfor task_num in tqdm(range(1, num_tasks + 1), desc=\"Tasks\"):\n    task_name = f\"task{task_num:03d}.py\"\n    smallest_size = None\n    smallest_idx = None  # use None to indicate 'unset'\n\n    # Load examples once per task\n    examples = load_examples(task_num)\n    ex_sig = _examples_signature(examples)\n\n    # Check all submission paths\n    for idx in range(len(paths)):\n        folder = f\"submission_{idx}\"\n        task_path = os.path.join(folder, task_name)\n\n        if not os.path.exists(task_path):\n            continue\n\n        # Prepare cache key\n        file_hash = _hash_file(task_path)\n        cache_key = f\"{task_num}::{idx}::{file_hash}::{ex_sig}\"\n\n        # Use cache or compute and add (report every miss)\n        if cache_key in cache:\n            result = cache[cache_key]\n            cache_hits += 1\n            agi_right = result[\"agi_right\"]\n            agi_wrong = result[\"agi_wrong\"]\n            agen_right = result[\"agen_right\"]\n            gen_wrong = result[\"gen_wrong\"]\n            is_valid = (agi_wrong + gen_wrong == 0)\n        else:\n            cache_misses += 1\n            had_miss = True\n\n            # NEW: print the exact source dataset path for this task\n            source_path = paths[idx]  # e.g., /kaggle/input/road-to-400-collaboration-39-unsolved-tasks\n            _log(\n                f\"[CACHE MISS] task={task_num} idx={idx} \"\n                f\"src={source_path} \"\n                f\"file={os.path.basename(task_path)} \"\n                f\"hash={file_hash[:12]} exsig={ex_sig[:12]}\"\n            )\n\n            # Verify and then append to cache (monotonic growth)\n            shutil.copy(task_path, \"task.py\")\n            agi_right, agi_wrong, agen_right, gen_wrong = simple_verify_program(task_num, examples)\n            is_valid = (agi_wrong + gen_wrong == 0)\n\n            cache[cache_key] = {\n                \"agi_right\": agi_right,\n                \"agi_wrong\": agi_wrong,\n                \"agen_right\": agen_right,\n                \"gen_wrong\": gen_wrong,\n                \"task_num\": task_num,\n                \"idx\": idx,\n                \"file_hash\": file_hash,\n                \"examples_sig\": ex_sig,\n            }\n\n        if is_valid:\n            size = os.path.getsize(task_path)\n            if smallest_size is None or size < smallest_size:\n                smallest_size = size\n                smallest_idx = idx\n\n    # Decide best file (if any)\n    if smallest_idx is not None:\n        smallest_file = os.path.join(f\"submission_{smallest_idx}\", task_name)\n        if os.path.exists(smallest_file):\n            # Determine source dataset path and slug for display\n            source_path = paths[smallest_idx]\n            source_slug = _dataset_slug_from_input_path(source_path)\n\n            # Logging (now includes dataset slug)\n            if smallest_size is not None:\n                mark = \"\"\n            else:\n                shutil.copy(smallest_file, \"task.py\")\n                agi_right, agi_wrong, agen_right, gen_wrong = simple_verify_program(task_num, examples)\n                mark = f\" >>> agi_wrong: {agi_wrong}, gen_wrong: {gen_wrong}\"\n\n            print(\n                f'Task_id = {task_num}: the best solution ({smallest_size} bytes) = '\n                f'{source_path} \"{source_slug}\"' + mark\n            )\n\n            shutil.copy(smallest_file, os.path.join(submission, task_name))\n\n            # Score\n            if smallest_size is None:\n                unsolved_tasks.append(task_num)\n                lb_score += 0.001\n            else:\n                lb_score += max(1, 2500 - smallest_size)\n        else:\n            unsolved_tasks.append(task_num)\n            print(f\"Task_id = {task_num}: Selected index had no file present\")\n    else:\n        print(f\"Task_id = {task_num}: No solution found\")\n        unsolved_tasks.append(task_num)\n\n# ====== Wrap-up ======\nfinal_cache_size = len(cache)\nadded_keys = final_cache_size - initial_cache_size\n\nprint(\"Unsolved tasks:\", unsolved_tasks)\nprint(\"LB score (approx):\", lb_score)\nprint(f\"Cache hits: {cache_hits}, misses: {cache_misses}\")\nprint(f\"Cache size: {initial_cache_size} -> {final_cache_size} \"\n      f\"({'grew' if final_cache_size>initial_cache_size else 'unchanged'})\")\n\n# If any miss happened, write the full, updated cache to WORKING and print dataset-aware instructions.\nif had_miss:\n    _save_json_atomic(WORKING_CACHE_PATH, cache)\n    print(\n        f\"\\n>>> Updated cache written to {WORKING_CACHE_PATH}\\n\"\n        f\"Cache dataset: '{INPUT_DATASET_NAME}' (mounted at {INPUT_DATASET_DIR})\\n\"\n        \"To update your input dataset:\\n\"\n        f\"  1) Open the 'Output' tab and download verify_cache.json from {WORKING_CACHE_PATH}.\\n\"\n        f\"  2) Go to your Kaggle dataset named '{INPUT_DATASET_NAME}' (the one mounted at {INPUT_DATASET_DIR}).\\n\"\n        \"  3) Click 'New Version', upload the file as verify_cache.json (overwrite the old one), and save.\\n\"\n        \"Future runs will pick it up automatically from /kaggle/input.\\n\"\n        f\"(Added {added_keys} new cache entr{'y' if added_keys==1 else 'ies'} this run.)\\n\"\n    )\nelse:\n    print(\n        f\"\\n>>> No cache misses this run; input dataset update not needed.\\n\"\n        f\"Cache dataset: '{INPUT_DATASET_NAME}' (mounted at {INPUT_DATASET_DIR}).\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T07:16:43.867583Z","iopub.execute_input":"2025-09-22T07:16:43.868004Z","iopub.status.idle":"2025-09-22T07:16:55.138686Z","shell.execute_reply.started":"2025-09-22T07:16:43.867972Z","shell.execute_reply":"2025-09-22T07:16:55.136874Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission.zip","metadata":{}},{"cell_type":"code","source":"import zipfile\n\nsubmission_zip = f\"{submission}.zip\"\n\nwith zipfile.ZipFile(submission_zip, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n    task_count = 0\n    for task_num in range(1, 401):\n        task_id = f\"{task_num:03d}\"\n        src_path = f\"{submission}/task{task_id}.py\"\n        \n        if os.path.exists(src_path):\n            zipf.write(src_path, arcname=f\"task{task_id}.py\")\n            task_count += 1\n\nprint(f\"Created submission zip with {task_count} tasks: {submission_zip}\")\n\n# Display zip file size\nzip_size = os.path.getsize(submission_zip)\nprint(f\"Submission zip size: {zip_size:,} bytes ({zip_size/1024:.1f} KB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T07:09:00.956037Z","iopub.execute_input":"2025-09-22T07:09:00.956377Z","iopub.status.idle":"2025-09-22T07:09:01.014151Z","shell.execute_reply.started":"2025-09-22T07:09:00.956346Z","shell.execute_reply":"2025-09-22T07:09:01.013086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"LB Score: \", lb_score)\nprint(f\"{len(unsolved_tasks)} Unsolved Tasks: {unsolved_tasks}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T07:09:01.01504Z","iopub.execute_input":"2025-09-22T07:09:01.015322Z","iopub.status.idle":"2025-09-22T07:09:01.020253Z","shell.execute_reply.started":"2025-09-22T07:09:01.0153Z","shell.execute_reply":"2025-09-22T07:09:01.019201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization for Unsolved Tasks","metadata":{}},{"cell_type":"code","source":"# Task visualization: https://www.kaggle.com/code/jacekwl/a-bit-more-of-code-golf-255-400-visualization\n\nfor idx in range(1,len(unsolved_tasks)+1): # only display 100 at once to not cause any memory issues\n    task_num = unsolved_tasks[idx-1]\n    task_id = f\"{task_num:03d}\"\n    examples = load_examples(task_num)\n    show_examples(examples['train'][:1] + examples['test'][:1] )\n    plt.figure(idx).suptitle(\n        f\"task{task_id}\" + [' : solved', ' : unsolved'][task_num in unsolved_tasks],\n        fontsize=16,color=['green', 'red'][task_num in unsolved_tasks])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T07:09:01.021538Z","iopub.execute_input":"2025-09-22T07:09:01.022015Z","iopub.status.idle":"2025-09-22T07:09:01.045344Z","shell.execute_reply.started":"2025-09-22T07:09:01.021979Z","shell.execute_reply":"2025-09-22T07:09:01.044286Z"}},"outputs":[],"execution_count":null}]}